{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepDrugDiscoveryExpt1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aced125/Coulomb_matrix_for_Drug_discovery/blob/master/Coulomb_matrix_for_DD_latest_5aug2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNBH2DK8HVvc",
        "colab_type": "text"
      },
      "source": [
        "# CoulombNet v1\n",
        "\n",
        "## Laksh Aithani, Exscientia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WskrCDqpVsNz",
        "colab_type": "text"
      },
      "source": [
        "## The Coulomb matrix is defined as:\n",
        "\n",
        "## $ C_{ij} = \\begin{cases}\n",
        " \\frac{Z_i Z_j}{r_{ij}}, & \\text{if } i \\neq j \\\\\n",
        " \\frac{1}{2}Z_{i}^{2.4}, & \\text{if } i = j\n",
        "\\end{cases}$\n",
        "\n",
        "where $Z_i$ is the nuclear charge of atom $i$ and $r_{ij}$ is the scalar distance between the 3D coordinaters of atom $i$ and atom $j$.\n",
        "\n",
        "By encoding a molecule in this sort of representation, we encode **all the information necessary to solve the Schrodinger Equation** for this molecule, from which we can determine **all** properties of the molecule: atomisation energy, dipole moment, you name it...\n",
        "\n",
        "### In this notebook we attempt to construct a map from the Coulomb matrix onto the chemical binding of a drug to a particular target.\n",
        "\n",
        "### We find that the model predicts binding well, about as well as current state of the art models like the Random Forest classifier. \n",
        "\n",
        "### More testing shall elucidate whether this type of regressor/classifier can predict binding affinity well of very dissimilar molecules (out-of-domain applicability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAAoUDP7cyJN",
        "colab_type": "text"
      },
      "source": [
        "![Coulomb matrix](https://i.ibb.co/ZLyDvFd/Screenshot-2019-08-05-at-12-04-25.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45rUo6xwe3Yd",
        "colab_type": "text"
      },
      "source": [
        "Source:  ['Montavon, G 2012, 'Learning Invariant Representations of Molecules for\n",
        "Atomization Energy Prediction', NIPS](https://pdfs.semanticscholar.org/5761/d22bb67798167a832879a473e0ece867df04.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX_B0AeXWmvP",
        "colab_type": "text"
      },
      "source": [
        "## Download RDKit into Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvSucSqsMXs9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "32a0cc4c-70e7-4527-b069-ca77f5da0861"
      },
      "source": [
        "# Download RDKit\n",
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!time conda install -q -y -c conda-forge rdkit"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-05 08:32:18--  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.201.79, 104.18.200.79, 2606:4700::6812:c94f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.201.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75257002 (72M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-latest-L 100%[===================>]  71.77M   196MB/s    in 0.4s    \n",
            "\n",
            "2019-08-05 08:32:19 (196 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [75257002/75257002]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==0.24.0=py37_0\n",
            "    - bzip2==1.0.8=h7b6447c_0\n",
            "    - ca-certificates==2019.5.15=0\n",
            "    - certifi==2019.6.16=py37_0\n",
            "    - cffi==1.12.3=py37h2e261b9_0\n",
            "    - chardet==3.0.4=py37_1\n",
            "    - conda-package-handling==1.3.11=py37_0\n",
            "    - conda==4.7.10=py37_0\n",
            "    - cryptography==2.7=py37h1ba5d50_0\n",
            "    - idna==2.8=py37_0\n",
            "    - libarchive==3.3.3=h5d8350f_5\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - libxml2==2.9.9=hea5a465_1\n",
            "    - lz4-c==1.8.1.2=h14c3975_0\n",
            "    - lzo==2.10=h49e0be7_2\n",
            "    - ncurses==6.1=he6710b0_1\n",
            "    - openssl==1.1.1c=h7b6447c_1\n",
            "    - pip==19.1.1=py37_0\n",
            "    - pycosat==0.6.3=py37h14c3975_0\n",
            "    - pycparser==2.19=py37_0\n",
            "    - pyopenssl==19.0.0=py37_0\n",
            "    - pysocks==1.7.0=py37_0\n",
            "    - python-libarchive-c==2.8=py37_11\n",
            "    - python==3.7.3=h0371630_0\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py37_0\n",
            "    - ruamel_yaml==0.15.46=py37h14c3975_0\n",
            "    - setuptools==41.0.1=py37_0\n",
            "    - six==1.12.0=py37_0\n",
            "    - sqlite==3.29.0=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.32.1=py_0\n",
            "    - urllib3==1.24.2=py37_0\n",
            "    - wheel==0.33.4=py37_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "    - zstd==1.3.7=h0b5b093_0\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-0.24.0-py37_0\n",
            "  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h7b6447c_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2019.5.15-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.6.16-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.12.3-py37h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1\n",
            "  conda              pkgs/main/linux-64::conda-4.7.10-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.3.11-py37_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.7-py37h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py37_0\n",
            "  libarchive         pkgs/main/linux-64::libarchive-3.3.3-h5d8350f_5\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  libxml2            pkgs/main/linux-64::libxml2-2.9.9-hea5a465_1\n",
            "  lz4-c              pkgs/main/linux-64::lz4-c-1.8.1.2-h14c3975_0\n",
            "  lzo                pkgs/main/linux-64::lzo-2.10-h49e0be7_2\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.1-he6710b0_1\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1c-h7b6447c_1\n",
            "  pip                pkgs/main/linux-64::pip-19.1.1-py37_0\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h14c3975_0\n",
            "  pycparser          pkgs/main/linux-64::pycparser-2.19-py37_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.0.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.0-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.3-h0371630_0\n",
            "  python-libarchive~ pkgs/main/linux-64::python-libarchive-c-2.8-py37_11\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py37_0\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.46-py37h14c3975_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-41.0.1-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.12.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.29.0-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.32.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.24.2-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.33.4-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "  zstd               pkgs/main/linux-64::zstd-1.3.7-h0b5b093_0\n",
            "\n",
            "\n",
            "Preparing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ WARNING conda.core.envs_manager:register_env(46): Unable to register environment. Path not writable or missing.\n",
            "  environment location: /usr/local\n",
            "  registry file: /root/.conda/environments.txt\n",
            "\b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "\n",
            "real\t0m18.462s\n",
            "user\t0m10.768s\n",
            "sys\t0m5.558s\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - rdkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    boost-1.70.0               |   py37h9de70de_1         337 KB  conda-forge\n",
            "    boost-cpp-1.70.0           |       h8e57a91_2        21.1 MB  conda-forge\n",
            "    ca-certificates-2019.6.16  |       hecc5488_0         145 KB  conda-forge\n",
            "    cairo-1.16.0               |    hfb77d84_1002         1.5 MB  conda-forge\n",
            "    certifi-2019.6.16          |           py37_1         149 KB  conda-forge\n",
            "    conda-4.7.10               |           py37_0         3.0 MB  conda-forge\n",
            "    fontconfig-2.13.1          |    h86ecdb6_1001         340 KB  conda-forge\n",
            "    freetype-2.10.0            |       he983fc9_0         885 KB  conda-forge\n",
            "    gettext-0.19.8.1           |    hc5be6a0_1002         3.6 MB  conda-forge\n",
            "    glib-2.58.3                |    h6f030ca_1002         3.3 MB  conda-forge\n",
            "    icu-64.2                   |       he1b5a44_0        12.6 MB  conda-forge\n",
            "    jpeg-9c                    |    h14c3975_1001         251 KB  conda-forge\n",
            "    libblas-3.8.0              |      11_openblas          10 KB  conda-forge\n",
            "    libcblas-3.8.0             |      11_openblas          10 KB  conda-forge\n",
            "    libgfortran-ng-7.3.0       |       hdf63c60_0        1006 KB\n",
            "    libiconv-1.15              |    h516909a_1005         2.0 MB  conda-forge\n",
            "    liblapack-3.8.0            |      11_openblas          10 KB  conda-forge\n",
            "    libopenblas-0.3.6          |       h6e990d7_6         7.7 MB  conda-forge\n",
            "    libpng-1.6.37              |       hed695b0_0         343 KB  conda-forge\n",
            "    libtiff-4.0.10             |       h2733197_2         435 KB\n",
            "    libuuid-2.32.1             |    h14c3975_1000          26 KB  conda-forge\n",
            "    libxcb-1.13                |    h14c3975_1002         396 KB  conda-forge\n",
            "    lz4-c-1.8.3                |    he1b5a44_1001         187 KB  conda-forge\n",
            "    numpy-1.17.0               |   py37h95a1406_0         5.2 MB  conda-forge\n",
            "    olefile-0.46               |             py_0          31 KB  conda-forge\n",
            "    openssl-1.1.1c             |       h516909a_0         2.1 MB  conda-forge\n",
            "    pandas-0.25.0              |   py37hb3f55d8_0        11.4 MB  conda-forge\n",
            "    pcre-8.41                  |    hf484d3e_1003         249 KB  conda-forge\n",
            "    pillow-6.1.0               |   py37h34e0f95_0         635 KB\n",
            "    pixman-0.38.0              |    h516909a_1003         594 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h14c3975_1001           5 KB  conda-forge\n",
            "    pycairo-1.18.1             |   py37h438ddbb_0          77 KB  conda-forge\n",
            "    python-dateutil-2.8.0      |             py_0         219 KB  conda-forge\n",
            "    pytz-2019.2                |             py_0         228 KB  conda-forge\n",
            "    rdkit-2019.03.2            |   py37hb31dc5d_1        23.3 MB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h14c3975_1002          26 KB  conda-forge\n",
            "    xorg-libice-1.0.10         |       h516909a_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.3           |    h84519dc_1000          25 KB  conda-forge\n",
            "    xorg-libx11-1.6.8          |       h516909a_0         907 KB  conda-forge\n",
            "    xorg-libxau-1.0.9          |       h14c3975_0          13 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h516909a_0          18 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h516909a_0          51 KB  conda-forge\n",
            "    xorg-libxrender-0.9.10     |    h516909a_1002          31 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h14c3975_1002           8 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h14c3975_1002          27 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h14c3975_1007          72 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       104.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  boost              conda-forge/linux-64::boost-1.70.0-py37h9de70de_1\n",
            "  boost-cpp          conda-forge/linux-64::boost-cpp-1.70.0-h8e57a91_2\n",
            "  cairo              conda-forge/linux-64::cairo-1.16.0-hfb77d84_1002\n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.13.1-h86ecdb6_1001\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.0-he983fc9_0\n",
            "  gettext            conda-forge/linux-64::gettext-0.19.8.1-hc5be6a0_1002\n",
            "  glib               conda-forge/linux-64::glib-2.58.3-h6f030ca_1002\n",
            "  icu                conda-forge/linux-64::icu-64.2-he1b5a44_0\n",
            "  jpeg               conda-forge/linux-64::jpeg-9c-h14c3975_1001\n",
            "  libblas            conda-forge/linux-64::libblas-3.8.0-11_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.8.0-11_openblas\n",
            "  libgfortran-ng     pkgs/main/linux-64::libgfortran-ng-7.3.0-hdf63c60_0\n",
            "  libiconv           conda-forge/linux-64::libiconv-1.15-h516909a_1005\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.8.0-11_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.6-h6e990d7_6\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-hed695b0_0\n",
            "  libtiff            pkgs/main/linux-64::libtiff-4.0.10-h2733197_2\n",
            "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000\n",
            "  libxcb             conda-forge/linux-64::libxcb-1.13-h14c3975_1002\n",
            "  numpy              conda-forge/linux-64::numpy-1.17.0-py37h95a1406_0\n",
            "  olefile            conda-forge/noarch::olefile-0.46-py_0\n",
            "  pandas             conda-forge/linux-64::pandas-0.25.0-py37hb3f55d8_0\n",
            "  pcre               conda-forge/linux-64::pcre-8.41-hf484d3e_1003\n",
            "  pillow             pkgs/main/linux-64::pillow-6.1.0-py37h34e0f95_0\n",
            "  pixman             conda-forge/linux-64::pixman-0.38.0-h516909a_1003\n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h14c3975_1001\n",
            "  pycairo            conda-forge/linux-64::pycairo-1.18.1-py37h438ddbb_0\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.0-py_0\n",
            "  pytz               conda-forge/noarch::pytz-2019.2-py_0\n",
            "  rdkit              conda-forge/linux-64::rdkit-2019.03.2-py37hb31dc5d_1\n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.0.10-h516909a_0\n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.3-h84519dc_1000\n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.6.8-h516909a_0\n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h14c3975_0\n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h516909a_0\n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h516909a_0\n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h516909a_1002\n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h14c3975_1007\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2019.5.15-0 --> conda-forge::ca-certificates-2019.6.16-hecc5488_0\n",
            "  certifi               pkgs/main::certifi-2019.6.16-py37_0 --> conda-forge::certifi-2019.6.16-py37_1\n",
            "  lz4-c                 pkgs/main::lz4-c-1.8.1.2-h14c3975_0 --> conda-forge::lz4-c-1.8.3-he1b5a44_1001\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  conda                                           pkgs/main --> conda-forge\n",
            "  openssl              pkgs/main::openssl-1.1.1c-h7b6447c_1 --> conda-forge::openssl-1.1.1c-h516909a_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "\n",
            "real\t0m45.324s\n",
            "user\t0m39.494s\n",
            "sys\t0m6.041s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKjivByX72bC",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmMQogveNEfW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "efcebb8a-b466-43c5-a85d-f9de3ea0faea"
      },
      "source": [
        "# Append RDKit to system variables\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "\n",
        "# Clone this repo to get the data (ADR1B strong and weak ligands)\n",
        "!git clone https://github.com/aced125/RandomMatrixDiscriminant\n",
        "  \n",
        "# Imports\n",
        "import os\n",
        "\n",
        "# RDkit, a chemoinformatics library\n",
        "import rdkit\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem as Chem\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import PandasTools\n",
        "\n",
        "# Numpy, a quintessential library\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "\n",
        "# Pandas\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import functools\n",
        "\n",
        "# Sklearn imports \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'RandomMatrixDiscriminant'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (5/5), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTcuH0oPuujj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(file_name, drop_non_binders = True):\n",
        "    '''\n",
        "    Function to load in the csv dataset from ChEMBL\n",
        "    '''\n",
        "    df = pd.read_csv(file_name, encoding = \"ISO-8859-1\")\n",
        "    \n",
        "    # Converting strings to floats, also set all non-numbers to NaN\n",
        "    df['Standard Vaue'] = pd.to_numeric(df['Standard Value'],errors = 'coerce')\n",
        "\n",
        "    # Drop Nans in affinity column\n",
        "    df.dropna(subset = ['Standard Value'], inplace = True)\n",
        "    df.reset_index(inplace = True)\n",
        "    df = df.drop('index',axis = 1)\n",
        "    \n",
        "    # Filtering for only activities recorded in nanomolar affinity\n",
        "    df = df[df['Standard Units'] == 'nM']    \n",
        "\n",
        "    # Dropping any molecules that don't have a SMILES\n",
        "    df = df.dropna(subset = ['Canonical Smiles'])\n",
        "    \n",
        "    # Considering only the binders (compounds with affinities of less than 1000nM)\n",
        "    if drop_non_binders:\n",
        "        df = df[df['Standard Value'] < 1000]\n",
        "    \n",
        "    # Dropping duplicate molecules\n",
        "    df = df.drop_duplicates(subset = 'Canonical Smiles', keep = 'first')\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK62T7ubs00G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train_test = load_dataset('/content/RandomMatrixDiscriminant/adr1b_chembl.csv', False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBB_AVHbXcMN",
        "colab_type": "text"
      },
      "source": [
        "## The dataframe has the standard value (binding affinity) of the drug, as well as its SMILES string, which we will use to get a view of the molecule, and eventually encode it into its Coulomb matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CDN-WC5QD5-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "a1e59b16-ddfd-41eb-8058-aecc05334750"
      },
      "source": [
        "df_train_test.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Molecule</th>\n",
              "      <th>Standard Type</th>\n",
              "      <th>Standard Relation</th>\n",
              "      <th>Standard Value</th>\n",
              "      <th>Standard Units</th>\n",
              "      <th>pChEMBL Value</th>\n",
              "      <th>Comment</th>\n",
              "      <th>Compound Key</th>\n",
              "      <th>Assay</th>\n",
              "      <th>Assay Description</th>\n",
              "      <th>BAO Format</th>\n",
              "      <th>Target</th>\n",
              "      <th>Target Pref. Name</th>\n",
              "      <th>Target Organism</th>\n",
              "      <th>Target Type</th>\n",
              "      <th>Document</th>\n",
              "      <th>Source</th>\n",
              "      <th>Max Phase</th>\n",
              "      <th>#Ro5 Violations</th>\n",
              "      <th>Molecular Weight</th>\n",
              "      <th>Ligand Efficiency BEI</th>\n",
              "      <th>Ligand Efficiency LE</th>\n",
              "      <th>Ligand Efficiency LLE</th>\n",
              "      <th>Ligand Efficiency SEI</th>\n",
              "      <th>ALogP</th>\n",
              "      <th>Assay Organism</th>\n",
              "      <th>Assay Tissue ChEMBL ID</th>\n",
              "      <th>Assay Tissue Name</th>\n",
              "      <th>Assay Cell Type</th>\n",
              "      <th>Assay Subcellular Fraction</th>\n",
              "      <th>Organism Tax ID</th>\n",
              "      <th>BAO Format ID</th>\n",
              "      <th>Published Type</th>\n",
              "      <th>Published Relation</th>\n",
              "      <th>Published Value</th>\n",
              "      <th>Published Units</th>\n",
              "      <th>Canonical Smiles</th>\n",
              "      <th>Data Validity Comment</th>\n",
              "      <th>Document Journal</th>\n",
              "      <th>Document Year</th>\n",
              "      <th>SRC ID</th>\n",
              "      <th>UO Units</th>\n",
              "      <th>Potential Duplicate</th>\n",
              "      <th>Unnamed: 43</th>\n",
              "      <th>Standard Vaue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CHEMBL57430</td>\n",
              "      <td>IC50</td>\n",
              "      <td>=</td>\n",
              "      <td>1000.00</td>\n",
              "      <td>nM</td>\n",
              "      <td>6.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5c</td>\n",
              "      <td>CHEMBL883288</td>\n",
              "      <td>In vitro binding affinity towards cloned human...</td>\n",
              "      <td>cell-based format</td>\n",
              "      <td>CHEMBL213</td>\n",
              "      <td>Beta-1 adrenergic receptor</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>SINGLE PROTEIN</td>\n",
              "      <td>CHEMBL1131776</td>\n",
              "      <td>Scientific Literature</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>429.59</td>\n",
              "      <td>13.97</td>\n",
              "      <td>0.27</td>\n",
              "      <td>1.08</td>\n",
              "      <td>7.64</td>\n",
              "      <td>4.92</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CHO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9606.0</td>\n",
              "      <td>BAO_0000219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CCCCCCCCNC(=O)N1CCc2cc(ccc12)S(=O)(=O)Nc3ccccc3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bioorg. Med. Chem. Lett.</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>1</td>\n",
              "      <td>UO_0000065</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CHEMBL248548</td>\n",
              "      <td>Ki</td>\n",
              "      <td>&gt;</td>\n",
              "      <td>10000.00</td>\n",
              "      <td>nM</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15e</td>\n",
              "      <td>CHEMBL888304</td>\n",
              "      <td>Displacement of [125I]iodocyanopindolol from h...</td>\n",
              "      <td>cell-based format</td>\n",
              "      <td>CHEMBL213</td>\n",
              "      <td>Beta-1 adrenergic receptor</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>SINGLE PROTEIN</td>\n",
              "      <td>CHEMBL1140016</td>\n",
              "      <td>Scientific Literature</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>500.62</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.26</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CHO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9606.0</td>\n",
              "      <td>BAO_0000219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>COc1ccc(cc1OC)[C@@H](Cc2ccccc2)NC[C@H](O)Cc3cc...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bioorg. Med. Chem. Lett.</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>1</td>\n",
              "      <td>UO_0000065</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10000.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CHEMBL251581</td>\n",
              "      <td>Ki</td>\n",
              "      <td>=</td>\n",
              "      <td>6000.00</td>\n",
              "      <td>nM</td>\n",
              "      <td>5.22</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15d</td>\n",
              "      <td>CHEMBL888304</td>\n",
              "      <td>Displacement of [125I]iodocyanopindolol from h...</td>\n",
              "      <td>cell-based format</td>\n",
              "      <td>CHEMBL213</td>\n",
              "      <td>Beta-1 adrenergic receptor</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>SINGLE PROTEIN</td>\n",
              "      <td>CHEMBL1140016</td>\n",
              "      <td>Scientific Literature</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>470.59</td>\n",
              "      <td>11.10</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.97</td>\n",
              "      <td>4.84</td>\n",
              "      <td>3.25</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CHO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9606.0</td>\n",
              "      <td>BAO_0000219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>COc1ccc(cc1)[C@@H](Cc2ccccc2)NC[C@@H](O)Cc3ccc...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bioorg. Med. Chem. Lett.</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>1</td>\n",
              "      <td>UO_0000065</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6000.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CHEMBL251347</td>\n",
              "      <td>Ki</td>\n",
              "      <td>=</td>\n",
              "      <td>730.00</td>\n",
              "      <td>nM</td>\n",
              "      <td>6.14</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16b</td>\n",
              "      <td>CHEMBL888304</td>\n",
              "      <td>Displacement of [125I]iodocyanopindolol from h...</td>\n",
              "      <td>cell-based format</td>\n",
              "      <td>CHEMBL213</td>\n",
              "      <td>Beta-1 adrenergic receptor</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>SINGLE PROTEIN</td>\n",
              "      <td>CHEMBL1140016</td>\n",
              "      <td>Scientific Literature</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>484.62</td>\n",
              "      <td>12.66</td>\n",
              "      <td>0.25</td>\n",
              "      <td>2.50</td>\n",
              "      <td>5.69</td>\n",
              "      <td>3.64</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CHO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9606.0</td>\n",
              "      <td>BAO_0000219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>COc1ccc(cc1)[C@H](CCc2ccccc2)NC[C@H](O)Cc3ccc(...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bioorg. Med. Chem. Lett.</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>1</td>\n",
              "      <td>UO_0000065</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>730.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CHEMBL245875</td>\n",
              "      <td>EC50</td>\n",
              "      <td>=</td>\n",
              "      <td>3981.07</td>\n",
              "      <td>nM</td>\n",
              "      <td>5.40</td>\n",
              "      <td>NaN</td>\n",
              "      <td>50</td>\n",
              "      <td>CHEMBL887286</td>\n",
              "      <td>Agonist activity at human beta-1 adrenergic re...</td>\n",
              "      <td>cell-based format</td>\n",
              "      <td>CHEMBL213</td>\n",
              "      <td>Beta-1 adrenergic receptor</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>SINGLE PROTEIN</td>\n",
              "      <td>CHEMBL1140647</td>\n",
              "      <td>Scientific Literature</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>416.93</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.50</td>\n",
              "      <td>Homo sapiens</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CHO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9606.0</td>\n",
              "      <td>BAO_0000219</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>O[C@@H](CNCCNc1cccc(c1)c2sccc2C(=O)O)c3cccc(Cl)c3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bioorg. Med. Chem. Lett.</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>1</td>\n",
              "      <td>UO_0000065</td>\n",
              "      <td>FALSE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3981.07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Molecule Standard Type  ... Unnamed: 43  Standard Vaue\n",
              "0   CHEMBL57430          IC50  ...         NaN        1000.00\n",
              "1  CHEMBL248548            Ki  ...         NaN       10000.00\n",
              "2  CHEMBL251581            Ki  ...         NaN        6000.00\n",
              "3  CHEMBL251347            Ki  ...         NaN         730.00\n",
              "4  CHEMBL245875          EC50  ...         NaN        3981.07\n",
              "\n",
              "[5 rows x 45 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3kVC_1SX6Yp",
        "colab_type": "text"
      },
      "source": [
        "## Select the relevant part of the dataframe, to generate our training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mkItiJBJW4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the SMILES strings\n",
        "smiles_train_test = df_train_test['Canonical Smiles']\n",
        "smiles_train_test = smiles_train_test.reset_index()['Canonical Smiles']\n",
        "\n",
        "# Get their respective binding affinities, the quantity we are\n",
        "# trying to predict.\n",
        "affinity_train_test = df_train_test['Standard Value']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-9UlqnIYtOS",
        "colab_type": "text"
      },
      "source": [
        "## Define functions to generate the Coulomb matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSl896mSxbd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "def get_coulomb_matrix(numbers, coords):\n",
        "    \"\"\"\n",
        "    Generates the unsorted Coulomb-matrix, given an array of\n",
        "    atom atomic numbers, and atom coordinates in space (these\n",
        "    coordinates must be found through minimization of the\n",
        "    atomic structure)\n",
        "    \"\"\"\n",
        "    top = np.outer(numbers, numbers).astype(np.float64)\n",
        "    r = cdist(coords, coords)\n",
        "\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        np.divide(top, r, top)\n",
        "    np.fill_diagonal(top, 0.5 * np.array(numbers) ** 2.4)\n",
        "    top[top == np.Infinity] = 0\n",
        "    top[np.isnan(top)] = 0\n",
        "    return top"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV7ZzIqdhZ67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def coulomb_generator(smiles):\n",
        "    '''\n",
        "    Given a list of SMILES strings, this function\n",
        "    will return the Coulomb matrices,\n",
        "    as well as the indices of the SMILES list of\n",
        "    molecules that failed to be minimized by MM94 force field\n",
        "    implemented in RDKit.\n",
        "    '''\n",
        "    matrices = []\n",
        "    failed_indices = []\n",
        "    max_el = 0\n",
        "    for idx,smile in enumerate(smiles):\n",
        "        print(idx)\n",
        "        mol = Chem.MolFromSmiles(smile)\n",
        "        mol = Chem.AddHs(mol)\n",
        "        try:\n",
        "          Chem.EmbedMolecule(mol)\n",
        "          AllChem.MMFFOptimizeMolecule(mol)\n",
        "          conf = mol.GetConformer()\n",
        "          n_atoms = mol.GetNumAtoms()\n",
        "          z = np.array([atom.GetAtomicNum() for atom in mol.GetAtoms()])\n",
        "          xyz = conf.GetPositions()\n",
        "          m = get_coulomb_matrix(z,xyz)\n",
        "          \n",
        "          matrices.append(m)\n",
        "          max_el = max(max_el, m.max())\n",
        "          print(max_el)\n",
        "        except:\n",
        "          print('failed_idx:',idx)\n",
        "          failed_indices.append(idx)\n",
        "    \n",
        "    max_atoms = max([m[0].shape[0] for m in matrices])\n",
        "    for index, matrix in enumerate(matrices):\n",
        "        n_atoms = matrix[0].shape[0]\n",
        "        m = np.zeros((max_atoms, max_atoms))\n",
        "        m[:n_atoms, :n_atoms] = matrix\n",
        "        matrices[index] = m\n",
        "        \n",
        "    return matrices, failed_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMDlkDehWSNx",
        "colab_type": "text"
      },
      "source": [
        "## Generate the Coulomb matrices from the SMILES strings of molecules - one Coulomb matrix per molecule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CYDhIUHkWfO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4dd5ad38-978d-4ae7-90c8-3a8dfacfcdc3"
      },
      "source": [
        "cmatrices, failed_indices = coulomb_generator(np.array(smiles_train_test))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "388.0234410266618\n",
            "1\n",
            "388.0234410266618\n",
            "2\n",
            "388.0234410266618\n",
            "3\n",
            "388.0234410266618\n",
            "4\n",
            "448.79438598087216\n",
            "5\n",
            "448.79438598087216\n",
            "6\n",
            "448.79438598087216\n",
            "7\n",
            "448.79438598087216\n",
            "8\n",
            "448.79438598087216\n",
            "9\n",
            "448.79438598087216\n",
            "10\n",
            "448.79438598087216\n",
            "11\n",
            "448.79438598087216\n",
            "12\n",
            "448.79438598087216\n",
            "13\n",
            "448.79438598087216\n",
            "14\n",
            "448.79438598087216\n",
            "15\n",
            "448.79438598087216\n",
            "16\n",
            "448.79438598087216\n",
            "17\n",
            "448.79438598087216\n",
            "18\n",
            "448.79438598087216\n",
            "19\n",
            "448.79438598087216\n",
            "20\n",
            "448.79438598087216\n",
            "21\n",
            "448.79438598087216\n",
            "22\n",
            "448.79438598087216\n",
            "23\n",
            "448.79438598087216\n",
            "24\n",
            "448.79438598087216\n",
            "25\n",
            "448.79438598087216\n",
            "26\n",
            "448.79438598087216\n",
            "27\n",
            "448.79438598087216\n",
            "28\n",
            "448.79438598087216\n",
            "29\n",
            "448.79438598087216\n",
            "30\n",
            "448.79438598087216\n",
            "31\n",
            "448.79438598087216\n",
            "32\n",
            "448.79438598087216\n",
            "33\n",
            "448.79438598087216\n",
            "34\n",
            "448.79438598087216\n",
            "35\n",
            "448.79438598087216\n",
            "36\n",
            "448.79438598087216\n",
            "37\n",
            "448.79438598087216\n",
            "38\n",
            "448.79438598087216\n",
            "39\n",
            "448.79438598087216\n",
            "40\n",
            "448.79438598087216\n",
            "41\n",
            "448.79438598087216\n",
            "42\n",
            "448.79438598087216\n",
            "43\n",
            "448.79438598087216\n",
            "44\n",
            "448.79438598087216\n",
            "45\n",
            "448.79438598087216\n",
            "46\n",
            "448.79438598087216\n",
            "47\n",
            "448.79438598087216\n",
            "48\n",
            "448.79438598087216\n",
            "49\n",
            "448.79438598087216\n",
            "50\n",
            "448.79438598087216\n",
            "51\n",
            "448.79438598087216\n",
            "52\n",
            "448.79438598087216\n",
            "53\n",
            "448.79438598087216\n",
            "54\n",
            "448.79438598087216\n",
            "55\n",
            "448.79438598087216\n",
            "56\n",
            "448.79438598087216\n",
            "57\n",
            "448.79438598087216\n",
            "58\n",
            "448.79438598087216\n",
            "59\n",
            "448.79438598087216\n",
            "60\n",
            "448.79438598087216\n",
            "61\n",
            "448.79438598087216\n",
            "62\n",
            "448.79438598087216\n",
            "63\n",
            "448.79438598087216\n",
            "64\n",
            "448.79438598087216\n",
            "65\n",
            "448.79438598087216\n",
            "66\n",
            "448.79438598087216\n",
            "67\n",
            "448.79438598087216\n",
            "68\n",
            "448.79438598087216\n",
            "69\n",
            "448.79438598087216\n",
            "70\n",
            "448.79438598087216\n",
            "71\n",
            "448.79438598087216\n",
            "72\n",
            "448.79438598087216\n",
            "73\n",
            "448.79438598087216\n",
            "74\n",
            "448.79438598087216\n",
            "75\n",
            "448.79438598087216\n",
            "76\n",
            "448.79438598087216\n",
            "77\n",
            "448.79438598087216\n",
            "78\n",
            "448.79438598087216\n",
            "79\n",
            "448.79438598087216\n",
            "80\n",
            "6874.357144891168\n",
            "81\n",
            "6874.357144891168\n",
            "82\n",
            "6874.357144891168\n",
            "83\n",
            "6874.357144891168\n",
            "84\n",
            "6874.357144891168\n",
            "85\n",
            "6874.357144891168\n",
            "86\n",
            "6874.357144891168\n",
            "87\n",
            "6874.357144891168\n",
            "88\n",
            "6874.357144891168\n",
            "89\n",
            "6874.357144891168\n",
            "90\n",
            "6874.357144891168\n",
            "91\n",
            "6874.357144891168\n",
            "92\n",
            "6874.357144891168\n",
            "93\n",
            "6874.357144891168\n",
            "94\n",
            "6874.357144891168\n",
            "95\n",
            "6874.357144891168\n",
            "96\n",
            "6874.357144891168\n",
            "97\n",
            "6874.357144891168\n",
            "98\n",
            "6874.357144891168\n",
            "99\n",
            "6874.357144891168\n",
            "100\n",
            "6874.357144891168\n",
            "101\n",
            "6874.357144891168\n",
            "102\n",
            "6874.357144891168\n",
            "103\n",
            "6874.357144891168\n",
            "104\n",
            "6874.357144891168\n",
            "105\n",
            "6874.357144891168\n",
            "106\n",
            "6874.357144891168\n",
            "107\n",
            "6874.357144891168\n",
            "108\n",
            "6874.357144891168\n",
            "109\n",
            "6874.357144891168\n",
            "110\n",
            "6874.357144891168\n",
            "111\n",
            "6874.357144891168\n",
            "112\n",
            "6874.357144891168\n",
            "113\n",
            "6874.357144891168\n",
            "114\n",
            "6874.357144891168\n",
            "115\n",
            "6874.357144891168\n",
            "116\n",
            "6874.357144891168\n",
            "117\n",
            "6874.357144891168\n",
            "118\n",
            "6874.357144891168\n",
            "119\n",
            "6874.357144891168\n",
            "120\n",
            "6874.357144891168\n",
            "121\n",
            "6874.357144891168\n",
            "122\n",
            "6874.357144891168\n",
            "123\n",
            "6874.357144891168\n",
            "124\n",
            "6874.357144891168\n",
            "125\n",
            "6874.357144891168\n",
            "126\n",
            "6874.357144891168\n",
            "127\n",
            "6874.357144891168\n",
            "128\n",
            "6874.357144891168\n",
            "129\n",
            "6874.357144891168\n",
            "130\n",
            "6874.357144891168\n",
            "131\n",
            "6874.357144891168\n",
            "132\n",
            "6874.357144891168\n",
            "133\n",
            "6874.357144891168\n",
            "134\n",
            "6874.357144891168\n",
            "135\n",
            "6874.357144891168\n",
            "136\n",
            "6874.357144891168\n",
            "137\n",
            "6874.357144891168\n",
            "138\n",
            "6874.357144891168\n",
            "139\n",
            "6874.357144891168\n",
            "140\n",
            "6874.357144891168\n",
            "141\n",
            "6874.357144891168\n",
            "142\n",
            "6874.357144891168\n",
            "143\n",
            "6874.357144891168\n",
            "144\n",
            "6874.357144891168\n",
            "145\n",
            "6874.357144891168\n",
            "146\n",
            "6874.357144891168\n",
            "147\n",
            "70261.52961720768\n",
            "148\n",
            "70261.52961720768\n",
            "149\n",
            "70261.52961720768\n",
            "150\n",
            "70261.52961720768\n",
            "151\n",
            "487541.4200130721\n",
            "152\n",
            "487541.4200130721\n",
            "153\n",
            "487541.4200130721\n",
            "154\n",
            "487541.4200130721\n",
            "155\n",
            "487541.4200130721\n",
            "156\n",
            "487541.4200130721\n",
            "157\n",
            "487541.4200130721\n",
            "158\n",
            "487541.4200130721\n",
            "159\n",
            "487541.4200130721\n",
            "160\n",
            "487541.4200130721\n",
            "161\n",
            "487541.4200130721\n",
            "162\n",
            "487541.4200130721\n",
            "163\n",
            "487541.4200130721\n",
            "164\n",
            "487541.4200130721\n",
            "165\n",
            "487541.4200130721\n",
            "166\n",
            "487541.4200130721\n",
            "167\n",
            "487541.4200130721\n",
            "168\n",
            "487541.4200130721\n",
            "169\n",
            "487541.4200130721\n",
            "170\n",
            "487541.4200130721\n",
            "171\n",
            "487541.4200130721\n",
            "172\n",
            "487541.4200130721\n",
            "173\n",
            "487541.4200130721\n",
            "174\n",
            "487541.4200130721\n",
            "175\n",
            "487541.4200130721\n",
            "176\n",
            "487541.4200130721\n",
            "177\n",
            "487541.4200130721\n",
            "178\n",
            "487541.4200130721\n",
            "179\n",
            "487541.4200130721\n",
            "180\n",
            "487541.4200130721\n",
            "181\n",
            "487541.4200130721\n",
            "182\n",
            "487541.4200130721\n",
            "183\n",
            "487541.4200130721\n",
            "184\n",
            "487541.4200130721\n",
            "185\n",
            "487541.4200130721\n",
            "186\n",
            "487541.4200130721\n",
            "187\n",
            "487541.4200130721\n",
            "188\n",
            "487541.4200130721\n",
            "189\n",
            "487541.4200130721\n",
            "190\n",
            "487541.4200130721\n",
            "191\n",
            "487541.4200130721\n",
            "192\n",
            "487541.4200130721\n",
            "193\n",
            "487541.4200130721\n",
            "194\n",
            "487541.4200130721\n",
            "195\n",
            "487541.4200130721\n",
            "196\n",
            "487541.4200130721\n",
            "197\n",
            "487541.4200130721\n",
            "198\n",
            "487541.4200130721\n",
            "199\n",
            "487541.4200130721\n",
            "200\n",
            "487541.4200130721\n",
            "201\n",
            "487541.4200130721\n",
            "202\n",
            "487541.4200130721\n",
            "203\n",
            "487541.4200130721\n",
            "204\n",
            "487541.4200130721\n",
            "205\n",
            "487541.4200130721\n",
            "206\n",
            "487541.4200130721\n",
            "207\n",
            "487541.4200130721\n",
            "208\n",
            "487541.4200130721\n",
            "209\n",
            "487541.4200130721\n",
            "210\n",
            "487541.4200130721\n",
            "211\n",
            "487541.4200130721\n",
            "212\n",
            "487541.4200130721\n",
            "213\n",
            "487541.4200130721\n",
            "214\n",
            "487541.4200130721\n",
            "215\n",
            "487541.4200130721\n",
            "216\n",
            "487541.4200130721\n",
            "217\n",
            "487541.4200130721\n",
            "218\n",
            "487541.4200130721\n",
            "219\n",
            "487541.4200130721\n",
            "220\n",
            "487541.4200130721\n",
            "221\n",
            "487541.4200130721\n",
            "222\n",
            "487541.4200130721\n",
            "223\n",
            "487541.4200130721\n",
            "224\n",
            "487541.4200130721\n",
            "225\n",
            "487541.4200130721\n",
            "226\n",
            "487541.4200130721\n",
            "227\n",
            "487541.4200130721\n",
            "228\n",
            "487541.4200130721\n",
            "229\n",
            "487541.4200130721\n",
            "230\n",
            "487541.4200130721\n",
            "231\n",
            "487541.4200130721\n",
            "232\n",
            "487541.4200130721\n",
            "233\n",
            "487541.4200130721\n",
            "234\n",
            "487541.4200130721\n",
            "235\n",
            "487541.4200130721\n",
            "236\n",
            "487541.4200130721\n",
            "237\n",
            "487541.4200130721\n",
            "238\n",
            "487541.4200130721\n",
            "239\n",
            "487541.4200130721\n",
            "240\n",
            "487541.4200130721\n",
            "241\n",
            "487541.4200130721\n",
            "242\n",
            "487541.4200130721\n",
            "243\n",
            "487541.4200130721\n",
            "244\n",
            "487541.4200130721\n",
            "245\n",
            "487541.4200130721\n",
            "246\n",
            "487541.4200130721\n",
            "247\n",
            "487541.4200130721\n",
            "248\n",
            "487541.4200130721\n",
            "249\n",
            "487541.4200130721\n",
            "250\n",
            "487541.4200130721\n",
            "251\n",
            "487541.4200130721\n",
            "252\n",
            "487541.4200130721\n",
            "253\n",
            "487541.4200130721\n",
            "254\n",
            "487541.4200130721\n",
            "255\n",
            "487541.4200130721\n",
            "256\n",
            "487541.4200130721\n",
            "257\n",
            "487541.4200130721\n",
            "258\n",
            "487541.4200130721\n",
            "259\n",
            "487541.4200130721\n",
            "260\n",
            "487541.4200130721\n",
            "261\n",
            "487541.4200130721\n",
            "262\n",
            "487541.4200130721\n",
            "263\n",
            "487541.4200130721\n",
            "264\n",
            "487541.4200130721\n",
            "265\n",
            "487541.4200130721\n",
            "266\n",
            "487541.4200130721\n",
            "267\n",
            "487541.4200130721\n",
            "268\n",
            "487541.4200130721\n",
            "269\n",
            "487541.4200130721\n",
            "270\n",
            "487541.4200130721\n",
            "271\n",
            "487541.4200130721\n",
            "272\n",
            "487541.4200130721\n",
            "273\n",
            "487541.4200130721\n",
            "274\n",
            "487541.4200130721\n",
            "275\n",
            "487541.4200130721\n",
            "276\n",
            "487541.4200130721\n",
            "277\n",
            "487541.4200130721\n",
            "278\n",
            "487541.4200130721\n",
            "279\n",
            "487541.4200130721\n",
            "280\n",
            "487541.4200130721\n",
            "281\n",
            "487541.4200130721\n",
            "282\n",
            "487541.4200130721\n",
            "283\n",
            "487541.4200130721\n",
            "284\n",
            "487541.4200130721\n",
            "285\n",
            "487541.4200130721\n",
            "286\n",
            "487541.4200130721\n",
            "287\n",
            "487541.4200130721\n",
            "288\n",
            "487541.4200130721\n",
            "289\n",
            "487541.4200130721\n",
            "290\n",
            "487541.4200130721\n",
            "291\n",
            "487541.4200130721\n",
            "292\n",
            "487541.4200130721\n",
            "293\n",
            "487541.4200130721\n",
            "294\n",
            "487541.4200130721\n",
            "295\n",
            "487541.4200130721\n",
            "296\n",
            "487541.4200130721\n",
            "297\n",
            "487541.4200130721\n",
            "298\n",
            "487541.4200130721\n",
            "299\n",
            "487541.4200130721\n",
            "300\n",
            "487541.4200130721\n",
            "301\n",
            "487541.4200130721\n",
            "302\n",
            "487541.4200130721\n",
            "303\n",
            "487541.4200130721\n",
            "304\n",
            "487541.4200130721\n",
            "305\n",
            "487541.4200130721\n",
            "306\n",
            "487541.4200130721\n",
            "307\n",
            "487541.4200130721\n",
            "308\n",
            "487541.4200130721\n",
            "309\n",
            "487541.4200130721\n",
            "310\n",
            "487541.4200130721\n",
            "311\n",
            "487541.4200130721\n",
            "312\n",
            "487541.4200130721\n",
            "313\n",
            "487541.4200130721\n",
            "314\n",
            "487541.4200130721\n",
            "315\n",
            "487541.4200130721\n",
            "316\n",
            "487541.4200130721\n",
            "317\n",
            "487541.4200130721\n",
            "318\n",
            "487541.4200130721\n",
            "319\n",
            "487541.4200130721\n",
            "320\n",
            "487541.4200130721\n",
            "321\n",
            "487541.4200130721\n",
            "322\n",
            "487541.4200130721\n",
            "323\n",
            "487541.4200130721\n",
            "324\n",
            "487541.4200130721\n",
            "325\n",
            "487541.4200130721\n",
            "326\n",
            "487541.4200130721\n",
            "327\n",
            "487541.4200130721\n",
            "328\n",
            "487541.4200130721\n",
            "329\n",
            "487541.4200130721\n",
            "330\n",
            "487541.4200130721\n",
            "331\n",
            "487541.4200130721\n",
            "332\n",
            "487541.4200130721\n",
            "333\n",
            "487541.4200130721\n",
            "334\n",
            "487541.4200130721\n",
            "335\n",
            "487541.4200130721\n",
            "336\n",
            "487541.4200130721\n",
            "337\n",
            "487541.4200130721\n",
            "338\n",
            "487541.4200130721\n",
            "339\n",
            "487541.4200130721\n",
            "340\n",
            "487541.4200130721\n",
            "341\n",
            "487541.4200130721\n",
            "342\n",
            "487541.4200130721\n",
            "343\n",
            "487541.4200130721\n",
            "344\n",
            "487541.4200130721\n",
            "345\n",
            "487541.4200130721\n",
            "346\n",
            "487541.4200130721\n",
            "347\n",
            "487541.4200130721\n",
            "348\n",
            "487541.4200130721\n",
            "349\n",
            "487541.4200130721\n",
            "350\n",
            "487541.4200130721\n",
            "351\n",
            "487541.4200130721\n",
            "352\n",
            "487541.4200130721\n",
            "353\n",
            "487541.4200130721\n",
            "354\n",
            "487541.4200130721\n",
            "355\n",
            "487541.4200130721\n",
            "356\n",
            "487541.4200130721\n",
            "357\n",
            "487541.4200130721\n",
            "358\n",
            "487541.4200130721\n",
            "359\n",
            "487541.4200130721\n",
            "360\n",
            "487541.4200130721\n",
            "361\n",
            "487541.4200130721\n",
            "362\n",
            "487541.4200130721\n",
            "363\n",
            "487541.4200130721\n",
            "364\n",
            "487541.4200130721\n",
            "365\n",
            "487541.4200130721\n",
            "366\n",
            "487541.4200130721\n",
            "367\n",
            "487541.4200130721\n",
            "368\n",
            "487541.4200130721\n",
            "369\n",
            "487541.4200130721\n",
            "370\n",
            "487541.4200130721\n",
            "371\n",
            "487541.4200130721\n",
            "372\n",
            "487541.4200130721\n",
            "373\n",
            "487541.4200130721\n",
            "374\n",
            "487541.4200130721\n",
            "375\n",
            "487541.4200130721\n",
            "376\n",
            "487541.4200130721\n",
            "377\n",
            "487541.4200130721\n",
            "378\n",
            "487541.4200130721\n",
            "379\n",
            "487541.4200130721\n",
            "380\n",
            "487541.4200130721\n",
            "381\n",
            "487541.4200130721\n",
            "382\n",
            "487541.4200130721\n",
            "383\n",
            "487541.4200130721\n",
            "384\n",
            "487541.4200130721\n",
            "385\n",
            "487541.4200130721\n",
            "386\n",
            "487541.4200130721\n",
            "387\n",
            "487541.4200130721\n",
            "388\n",
            "487541.4200130721\n",
            "389\n",
            "487541.4200130721\n",
            "390\n",
            "487541.4200130721\n",
            "391\n",
            "487541.4200130721\n",
            "392\n",
            "487541.4200130721\n",
            "393\n",
            "487541.4200130721\n",
            "394\n",
            "487541.4200130721\n",
            "395\n",
            "487541.4200130721\n",
            "396\n",
            "487541.4200130721\n",
            "397\n",
            "487541.4200130721\n",
            "398\n",
            "487541.4200130721\n",
            "399\n",
            "487541.4200130721\n",
            "400\n",
            "487541.4200130721\n",
            "401\n",
            "487541.4200130721\n",
            "402\n",
            "487541.4200130721\n",
            "403\n",
            "487541.4200130721\n",
            "404\n",
            "487541.4200130721\n",
            "405\n",
            "487541.4200130721\n",
            "406\n",
            "487541.4200130721\n",
            "407\n",
            "487541.4200130721\n",
            "408\n",
            "487541.4200130721\n",
            "409\n",
            "487541.4200130721\n",
            "410\n",
            "487541.4200130721\n",
            "411\n",
            "487541.4200130721\n",
            "412\n",
            "487541.4200130721\n",
            "413\n",
            "487541.4200130721\n",
            "414\n",
            "487541.4200130721\n",
            "415\n",
            "487541.4200130721\n",
            "416\n",
            "487541.4200130721\n",
            "417\n",
            "487541.4200130721\n",
            "418\n",
            "487541.4200130721\n",
            "419\n",
            "487541.4200130721\n",
            "420\n",
            "487541.4200130721\n",
            "421\n",
            "487541.4200130721\n",
            "422\n",
            "487541.4200130721\n",
            "423\n",
            "487541.4200130721\n",
            "424\n",
            "487541.4200130721\n",
            "425\n",
            "487541.4200130721\n",
            "426\n",
            "487541.4200130721\n",
            "427\n",
            "487541.4200130721\n",
            "428\n",
            "487541.4200130721\n",
            "429\n",
            "487541.4200130721\n",
            "430\n",
            "487541.4200130721\n",
            "431\n",
            "487541.4200130721\n",
            "432\n",
            "487541.4200130721\n",
            "433\n",
            "487541.4200130721\n",
            "434\n",
            "487541.4200130721\n",
            "435\n",
            "487541.4200130721\n",
            "436\n",
            "487541.4200130721\n",
            "437\n",
            "487541.4200130721\n",
            "438\n",
            "487541.4200130721\n",
            "439\n",
            "487541.4200130721\n",
            "440\n",
            "487541.4200130721\n",
            "441\n",
            "487541.4200130721\n",
            "442\n",
            "487541.4200130721\n",
            "443\n",
            "487541.4200130721\n",
            "444\n",
            "487541.4200130721\n",
            "445\n",
            "487541.4200130721\n",
            "446\n",
            "487541.4200130721\n",
            "447\n",
            "487541.4200130721\n",
            "448\n",
            "487541.4200130721\n",
            "449\n",
            "487541.4200130721\n",
            "450\n",
            "487541.4200130721\n",
            "451\n",
            "487541.4200130721\n",
            "452\n",
            "487541.4200130721\n",
            "453\n",
            "487541.4200130721\n",
            "454\n",
            "487541.4200130721\n",
            "455\n",
            "487541.4200130721\n",
            "456\n",
            "487541.4200130721\n",
            "457\n",
            "487541.4200130721\n",
            "458\n",
            "487541.4200130721\n",
            "459\n",
            "487541.4200130721\n",
            "460\n",
            "487541.4200130721\n",
            "461\n",
            "487541.4200130721\n",
            "462\n",
            "487541.4200130721\n",
            "463\n",
            "487541.4200130721\n",
            "464\n",
            "487541.4200130721\n",
            "465\n",
            "487541.4200130721\n",
            "466\n",
            "487541.4200130721\n",
            "467\n",
            "487541.4200130721\n",
            "468\n",
            "487541.4200130721\n",
            "469\n",
            "487541.4200130721\n",
            "470\n",
            "487541.4200130721\n",
            "471\n",
            "487541.4200130721\n",
            "472\n",
            "487541.4200130721\n",
            "473\n",
            "487541.4200130721\n",
            "474\n",
            "487541.4200130721\n",
            "475\n",
            "487541.4200130721\n",
            "476\n",
            "487541.4200130721\n",
            "477\n",
            "487541.4200130721\n",
            "478\n",
            "487541.4200130721\n",
            "479\n",
            "487541.4200130721\n",
            "480\n",
            "487541.4200130721\n",
            "481\n",
            "487541.4200130721\n",
            "482\n",
            "487541.4200130721\n",
            "483\n",
            "487541.4200130721\n",
            "484\n",
            "487541.4200130721\n",
            "485\n",
            "487541.4200130721\n",
            "486\n",
            "487541.4200130721\n",
            "487\n",
            "487541.4200130721\n",
            "488\n",
            "487541.4200130721\n",
            "489\n",
            "487541.4200130721\n",
            "490\n",
            "487541.4200130721\n",
            "491\n",
            "487541.4200130721\n",
            "492\n",
            "487541.4200130721\n",
            "493\n",
            "487541.4200130721\n",
            "494\n",
            "487541.4200130721\n",
            "495\n",
            "487541.4200130721\n",
            "496\n",
            "487541.4200130721\n",
            "497\n",
            "487541.4200130721\n",
            "498\n",
            "487541.4200130721\n",
            "499\n",
            "487541.4200130721\n",
            "500\n",
            "487541.4200130721\n",
            "501\n",
            "487541.4200130721\n",
            "502\n",
            "487541.4200130721\n",
            "503\n",
            "487541.4200130721\n",
            "504\n",
            "487541.4200130721\n",
            "505\n",
            "487541.4200130721\n",
            "506\n",
            "487541.4200130721\n",
            "507\n",
            "487541.4200130721\n",
            "508\n",
            "487541.4200130721\n",
            "509\n",
            "487541.4200130721\n",
            "510\n",
            "487541.4200130721\n",
            "511\n",
            "487541.4200130721\n",
            "512\n",
            "487541.4200130721\n",
            "513\n",
            "487541.4200130721\n",
            "514\n",
            "487541.4200130721\n",
            "515\n",
            "487541.4200130721\n",
            "516\n",
            "487541.4200130721\n",
            "517\n",
            "487541.4200130721\n",
            "518\n",
            "487541.4200130721\n",
            "519\n",
            "487541.4200130721\n",
            "520\n",
            "487541.4200130721\n",
            "521\n",
            "487541.4200130721\n",
            "522\n",
            "487541.4200130721\n",
            "523\n",
            "487541.4200130721\n",
            "524\n",
            "487541.4200130721\n",
            "525\n",
            "487541.4200130721\n",
            "526\n",
            "487541.4200130721\n",
            "527\n",
            "487541.4200130721\n",
            "528\n",
            "487541.4200130721\n",
            "529\n",
            "487541.4200130721\n",
            "530\n",
            "487541.4200130721\n",
            "531\n",
            "487541.4200130721\n",
            "532\n",
            "487541.4200130721\n",
            "533\n",
            "487541.4200130721\n",
            "534\n",
            "487541.4200130721\n",
            "535\n",
            "487541.4200130721\n",
            "536\n",
            "487541.4200130721\n",
            "537\n",
            "487541.4200130721\n",
            "538\n",
            "487541.4200130721\n",
            "539\n",
            "487541.4200130721\n",
            "540\n",
            "487541.4200130721\n",
            "541\n",
            "487541.4200130721\n",
            "542\n",
            "487541.4200130721\n",
            "543\n",
            "487541.4200130721\n",
            "544\n",
            "487541.4200130721\n",
            "545\n",
            "487541.4200130721\n",
            "546\n",
            "487541.4200130721\n",
            "547\n",
            "487541.4200130721\n",
            "548\n",
            "677315.7402575882\n",
            "549\n",
            "677315.7402575882\n",
            "550\n",
            "677315.7402575882\n",
            "551\n",
            "677315.7402575882\n",
            "552\n",
            "677315.7402575882\n",
            "553\n",
            "677315.7402575882\n",
            "554\n",
            "677315.7402575882\n",
            "555\n",
            "677315.7402575882\n",
            "556\n",
            "677315.7402575882\n",
            "557\n",
            "677315.7402575882\n",
            "558\n",
            "677315.7402575882\n",
            "559\n",
            "failed_idx: 559\n",
            "560\n",
            "677315.7402575882\n",
            "561\n",
            "677315.7402575882\n",
            "562\n",
            "677315.7402575882\n",
            "563\n",
            "677315.7402575882\n",
            "564\n",
            "677315.7402575882\n",
            "565\n",
            "677315.7402575882\n",
            "566\n",
            "677315.7402575882\n",
            "567\n",
            "677315.7402575882\n",
            "568\n",
            "677315.7402575882\n",
            "569\n",
            "677315.7402575882\n",
            "570\n",
            "677315.7402575882\n",
            "571\n",
            "677315.7402575882\n",
            "572\n",
            "677315.7402575882\n",
            "573\n",
            "677315.7402575882\n",
            "574\n",
            "677315.7402575882\n",
            "575\n",
            "677315.7402575882\n",
            "576\n",
            "677315.7402575882\n",
            "577\n",
            "677315.7402575882\n",
            "578\n",
            "677315.7402575882\n",
            "579\n",
            "677315.7402575882\n",
            "580\n",
            "677315.7402575882\n",
            "581\n",
            "677315.7402575882\n",
            "582\n",
            "677315.7402575882\n",
            "583\n",
            "677315.7402575882\n",
            "584\n",
            "677315.7402575882\n",
            "585\n",
            "677315.7402575882\n",
            "586\n",
            "677315.7402575882\n",
            "587\n",
            "677315.7402575882\n",
            "588\n",
            "677315.7402575882\n",
            "589\n",
            "677315.7402575882\n",
            "590\n",
            "677315.7402575882\n",
            "591\n",
            "677315.7402575882\n",
            "592\n",
            "677315.7402575882\n",
            "593\n",
            "677315.7402575882\n",
            "594\n",
            "677315.7402575882\n",
            "595\n",
            "677315.7402575882\n",
            "596\n",
            "677315.7402575882\n",
            "597\n",
            "677315.7402575882\n",
            "598\n",
            "677315.7402575882\n",
            "599\n",
            "677315.7402575882\n",
            "600\n",
            "677315.7402575882\n",
            "601\n",
            "677315.7402575882\n",
            "602\n",
            "677315.7402575882\n",
            "603\n",
            "677315.7402575882\n",
            "604\n",
            "677315.7402575882\n",
            "605\n",
            "677315.7402575882\n",
            "606\n",
            "677315.7402575882\n",
            "607\n",
            "677315.7402575882\n",
            "608\n",
            "677315.7402575882\n",
            "609\n",
            "677315.7402575882\n",
            "610\n",
            "677315.7402575882\n",
            "611\n",
            "677315.7402575882\n",
            "612\n",
            "677315.7402575882\n",
            "613\n",
            "677315.7402575882\n",
            "614\n",
            "677315.7402575882\n",
            "615\n",
            "677315.7402575882\n",
            "616\n",
            "677315.7402575882\n",
            "617\n",
            "677315.7402575882\n",
            "618\n",
            "677315.7402575882\n",
            "619\n",
            "677315.7402575882\n",
            "620\n",
            "677315.7402575882\n",
            "621\n",
            "677315.7402575882\n",
            "622\n",
            "677315.7402575882\n",
            "623\n",
            "677315.7402575882\n",
            "624\n",
            "677315.7402575882\n",
            "625\n",
            "677315.7402575882\n",
            "626\n",
            "677315.7402575882\n",
            "627\n",
            "677315.7402575882\n",
            "628\n",
            "677315.7402575882\n",
            "629\n",
            "677315.7402575882\n",
            "630\n",
            "677315.7402575882\n",
            "631\n",
            "677315.7402575882\n",
            "632\n",
            "677315.7402575882\n",
            "633\n",
            "677315.7402575882\n",
            "634\n",
            "677315.7402575882\n",
            "635\n",
            "677315.7402575882\n",
            "636\n",
            "677315.7402575882\n",
            "637\n",
            "677315.7402575882\n",
            "638\n",
            "677315.7402575882\n",
            "639\n",
            "677315.7402575882\n",
            "640\n",
            "677315.7402575882\n",
            "641\n",
            "677315.7402575882\n",
            "642\n",
            "677315.7402575882\n",
            "643\n",
            "677315.7402575882\n",
            "644\n",
            "677315.7402575882\n",
            "645\n",
            "677315.7402575882\n",
            "646\n",
            "677315.7402575882\n",
            "647\n",
            "677315.7402575882\n",
            "648\n",
            "677315.7402575882\n",
            "649\n",
            "677315.7402575882\n",
            "650\n",
            "677315.7402575882\n",
            "651\n",
            "677315.7402575882\n",
            "652\n",
            "677315.7402575882\n",
            "653\n",
            "677315.7402575882\n",
            "654\n",
            "677315.7402575882\n",
            "655\n",
            "677315.7402575882\n",
            "656\n",
            "677315.7402575882\n",
            "657\n",
            "677315.7402575882\n",
            "658\n",
            "677315.7402575882\n",
            "659\n",
            "677315.7402575882\n",
            "660\n",
            "677315.7402575882\n",
            "661\n",
            "677315.7402575882\n",
            "662\n",
            "677315.7402575882\n",
            "663\n",
            "677315.7402575882\n",
            "664\n",
            "677315.7402575882\n",
            "665\n",
            "677315.7402575882\n",
            "666\n",
            "677315.7402575882\n",
            "667\n",
            "677315.7402575882\n",
            "668\n",
            "677315.7402575882\n",
            "669\n",
            "677315.7402575882\n",
            "670\n",
            "677315.7402575882\n",
            "671\n",
            "677315.7402575882\n",
            "672\n",
            "677315.7402575882\n",
            "673\n",
            "677315.7402575882\n",
            "674\n",
            "677315.7402575882\n",
            "675\n",
            "677315.7402575882\n",
            "676\n",
            "677315.7402575882\n",
            "677\n",
            "677315.7402575882\n",
            "678\n",
            "677315.7402575882\n",
            "679\n",
            "677315.7402575882\n",
            "680\n",
            "677315.7402575882\n",
            "681\n",
            "677315.7402575882\n",
            "682\n",
            "677315.7402575882\n",
            "683\n",
            "677315.7402575882\n",
            "684\n",
            "677315.7402575882\n",
            "685\n",
            "677315.7402575882\n",
            "686\n",
            "677315.7402575882\n",
            "687\n",
            "677315.7402575882\n",
            "688\n",
            "677315.7402575882\n",
            "689\n",
            "677315.7402575882\n",
            "690\n",
            "677315.7402575882\n",
            "691\n",
            "677315.7402575882\n",
            "692\n",
            "677315.7402575882\n",
            "693\n",
            "677315.7402575882\n",
            "694\n",
            "677315.7402575882\n",
            "695\n",
            "677315.7402575882\n",
            "696\n",
            "677315.7402575882\n",
            "697\n",
            "677315.7402575882\n",
            "698\n",
            "677315.7402575882\n",
            "699\n",
            "677315.7402575882\n",
            "700\n",
            "677315.7402575882\n",
            "701\n",
            "677315.7402575882\n",
            "702\n",
            "677315.7402575882\n",
            "703\n",
            "677315.7402575882\n",
            "704\n",
            "677315.7402575882\n",
            "705\n",
            "677315.7402575882\n",
            "706\n",
            "677315.7402575882\n",
            "707\n",
            "677315.7402575882\n",
            "708\n",
            "677315.7402575882\n",
            "709\n",
            "677315.7402575882\n",
            "710\n",
            "677315.7402575882\n",
            "711\n",
            "677315.7402575882\n",
            "712\n",
            "677315.7402575882\n",
            "713\n",
            "677315.7402575882\n",
            "714\n",
            "677315.7402575882\n",
            "715\n",
            "677315.7402575882\n",
            "716\n",
            "677315.7402575882\n",
            "717\n",
            "677315.7402575882\n",
            "718\n",
            "677315.7402575882\n",
            "719\n",
            "677315.7402575882\n",
            "720\n",
            "677315.7402575882\n",
            "721\n",
            "677315.7402575882\n",
            "722\n",
            "677315.7402575882\n",
            "723\n",
            "677315.7402575882\n",
            "724\n",
            "677315.7402575882\n",
            "725\n",
            "677315.7402575882\n",
            "726\n",
            "677315.7402575882\n",
            "727\n",
            "677315.7402575882\n",
            "728\n",
            "677315.7402575882\n",
            "729\n",
            "677315.7402575882\n",
            "730\n",
            "677315.7402575882\n",
            "731\n",
            "677315.7402575882\n",
            "732\n",
            "677315.7402575882\n",
            "733\n",
            "677315.7402575882\n",
            "734\n",
            "677315.7402575882\n",
            "735\n",
            "677315.7402575882\n",
            "736\n",
            "677315.7402575882\n",
            "737\n",
            "677315.7402575882\n",
            "738\n",
            "677315.7402575882\n",
            "739\n",
            "677315.7402575882\n",
            "740\n",
            "677315.7402575882\n",
            "741\n",
            "677315.7402575882\n",
            "742\n",
            "677315.7402575882\n",
            "743\n",
            "677315.7402575882\n",
            "744\n",
            "677315.7402575882\n",
            "745\n",
            "677315.7402575882\n",
            "746\n",
            "677315.7402575882\n",
            "747\n",
            "677315.7402575882\n",
            "748\n",
            "677315.7402575882\n",
            "749\n",
            "677315.7402575882\n",
            "750\n",
            "677315.7402575882\n",
            "751\n",
            "677315.7402575882\n",
            "752\n",
            "677315.7402575882\n",
            "753\n",
            "677315.7402575882\n",
            "754\n",
            "677315.7402575882\n",
            "755\n",
            "677315.7402575882\n",
            "756\n",
            "677315.7402575882\n",
            "757\n",
            "677315.7402575882\n",
            "758\n",
            "677315.7402575882\n",
            "759\n",
            "677315.7402575882\n",
            "760\n",
            "677315.7402575882\n",
            "761\n",
            "677315.7402575882\n",
            "762\n",
            "677315.7402575882\n",
            "763\n",
            "677315.7402575882\n",
            "764\n",
            "677315.7402575882\n",
            "765\n",
            "677315.7402575882\n",
            "766\n",
            "677315.7402575882\n",
            "767\n",
            "677315.7402575882\n",
            "768\n",
            "677315.7402575882\n",
            "769\n",
            "677315.7402575882\n",
            "770\n",
            "677315.7402575882\n",
            "771\n",
            "677315.7402575882\n",
            "772\n",
            "677315.7402575882\n",
            "773\n",
            "677315.7402575882\n",
            "774\n",
            "677315.7402575882\n",
            "775\n",
            "677315.7402575882\n",
            "776\n",
            "677315.7402575882\n",
            "777\n",
            "677315.7402575882\n",
            "778\n",
            "677315.7402575882\n",
            "779\n",
            "677315.7402575882\n",
            "780\n",
            "677315.7402575882\n",
            "781\n",
            "677315.7402575882\n",
            "782\n",
            "677315.7402575882\n",
            "783\n",
            "677315.7402575882\n",
            "784\n",
            "677315.7402575882\n",
            "785\n",
            "677315.7402575882\n",
            "786\n",
            "677315.7402575882\n",
            "787\n",
            "677315.7402575882\n",
            "788\n",
            "677315.7402575882\n",
            "789\n",
            "677315.7402575882\n",
            "790\n",
            "677315.7402575882\n",
            "791\n",
            "677315.7402575882\n",
            "792\n",
            "677315.7402575882\n",
            "793\n",
            "677315.7402575882\n",
            "794\n",
            "677315.7402575882\n",
            "795\n",
            "677315.7402575882\n",
            "796\n",
            "677315.7402575882\n",
            "797\n",
            "677315.7402575882\n",
            "798\n",
            "677315.7402575882\n",
            "799\n",
            "677315.7402575882\n",
            "800\n",
            "677315.7402575882\n",
            "801\n",
            "677315.7402575882\n",
            "802\n",
            "677315.7402575882\n",
            "803\n",
            "677315.7402575882\n",
            "804\n",
            "677315.7402575882\n",
            "805\n",
            "677315.7402575882\n",
            "806\n",
            "677315.7402575882\n",
            "807\n",
            "677315.7402575882\n",
            "808\n",
            "677315.7402575882\n",
            "809\n",
            "677315.7402575882\n",
            "810\n",
            "677315.7402575882\n",
            "811\n",
            "677315.7402575882\n",
            "812\n",
            "677315.7402575882\n",
            "813\n",
            "677315.7402575882\n",
            "814\n",
            "677315.7402575882\n",
            "815\n",
            "677315.7402575882\n",
            "816\n",
            "677315.7402575882\n",
            "817\n",
            "677315.7402575882\n",
            "818\n",
            "677315.7402575882\n",
            "819\n",
            "677315.7402575882\n",
            "820\n",
            "677315.7402575882\n",
            "821\n",
            "677315.7402575882\n",
            "822\n",
            "677315.7402575882\n",
            "823\n",
            "677315.7402575882\n",
            "824\n",
            "677315.7402575882\n",
            "825\n",
            "677315.7402575882\n",
            "826\n",
            "677315.7402575882\n",
            "827\n",
            "677315.7402575882\n",
            "828\n",
            "677315.7402575882\n",
            "829\n",
            "677315.7402575882\n",
            "830\n",
            "677315.7402575882\n",
            "831\n",
            "677315.7402575882\n",
            "832\n",
            "677315.7402575882\n",
            "833\n",
            "677315.7402575882\n",
            "834\n",
            "677315.7402575882\n",
            "835\n",
            "677315.7402575882\n",
            "836\n",
            "677315.7402575882\n",
            "837\n",
            "677315.7402575882\n",
            "838\n",
            "677315.7402575882\n",
            "839\n",
            "677315.7402575882\n",
            "840\n",
            "677315.7402575882\n",
            "841\n",
            "677315.7402575882\n",
            "842\n",
            "677315.7402575882\n",
            "843\n",
            "677315.7402575882\n",
            "844\n",
            "677315.7402575882\n",
            "845\n",
            "677315.7402575882\n",
            "846\n",
            "677315.7402575882\n",
            "847\n",
            "677315.7402575882\n",
            "848\n",
            "677315.7402575882\n",
            "849\n",
            "677315.7402575882\n",
            "850\n",
            "677315.7402575882\n",
            "851\n",
            "677315.7402575882\n",
            "852\n",
            "677315.7402575882\n",
            "853\n",
            "677315.7402575882\n",
            "854\n",
            "677315.7402575882\n",
            "855\n",
            "677315.7402575882\n",
            "856\n",
            "677315.7402575882\n",
            "857\n",
            "677315.7402575882\n",
            "858\n",
            "677315.7402575882\n",
            "859\n",
            "677315.7402575882\n",
            "860\n",
            "677315.7402575882\n",
            "861\n",
            "677315.7402575882\n",
            "862\n",
            "677315.7402575882\n",
            "863\n",
            "677315.7402575882\n",
            "864\n",
            "677315.7402575882\n",
            "865\n",
            "677315.7402575882\n",
            "866\n",
            "677315.7402575882\n",
            "867\n",
            "677315.7402575882\n",
            "868\n",
            "677315.7402575882\n",
            "869\n",
            "677315.7402575882\n",
            "870\n",
            "677315.7402575882\n",
            "871\n",
            "677315.7402575882\n",
            "872\n",
            "677315.7402575882\n",
            "873\n",
            "677315.7402575882\n",
            "874\n",
            "677315.7402575882\n",
            "875\n",
            "677315.7402575882\n",
            "876\n",
            "677315.7402575882\n",
            "877\n",
            "677315.7402575882\n",
            "878\n",
            "677315.7402575882\n",
            "879\n",
            "677315.7402575882\n",
            "880\n",
            "677315.7402575882\n",
            "881\n",
            "677315.7402575882\n",
            "882\n",
            "677315.7402575882\n",
            "883\n",
            "677315.7402575882\n",
            "884\n",
            "677315.7402575882\n",
            "885\n",
            "677315.7402575882\n",
            "886\n",
            "677315.7402575882\n",
            "887\n",
            "677315.7402575882\n",
            "888\n",
            "677315.7402575882\n",
            "889\n",
            "677315.7402575882\n",
            "890\n",
            "677315.7402575882\n",
            "891\n",
            "677315.7402575882\n",
            "892\n",
            "677315.7402575882\n",
            "893\n",
            "677315.7402575882\n",
            "894\n",
            "677315.7402575882\n",
            "895\n",
            "4031762.1408289038\n",
            "896\n",
            "4031762.1408289038\n",
            "897\n",
            "4031762.1408289038\n",
            "898\n",
            "4031762.1408289038\n",
            "899\n",
            "4031762.1408289038\n",
            "900\n",
            "4031762.1408289038\n",
            "901\n",
            "4031762.1408289038\n",
            "902\n",
            "4031762.1408289038\n",
            "903\n",
            "4031762.1408289038\n",
            "904\n",
            "4031762.1408289038\n",
            "905\n",
            "4031762.1408289038\n",
            "906\n",
            "4031762.1408289038\n",
            "907\n",
            "4031762.1408289038\n",
            "908\n",
            "4031762.1408289038\n",
            "909\n",
            "4031762.1408289038\n",
            "910\n",
            "4031762.1408289038\n",
            "911\n",
            "4031762.1408289038\n",
            "912\n",
            "4031762.1408289038\n",
            "913\n",
            "4031762.1408289038\n",
            "914\n",
            "4031762.1408289038\n",
            "915\n",
            "4031762.1408289038\n",
            "916\n",
            "4031762.1408289038\n",
            "917\n",
            "4031762.1408289038\n",
            "918\n",
            "4031762.1408289038\n",
            "919\n",
            "4031762.1408289038\n",
            "920\n",
            "4031762.1408289038\n",
            "921\n",
            "4031762.1408289038\n",
            "922\n",
            "4031762.1408289038\n",
            "923\n",
            "4031762.1408289038\n",
            "924\n",
            "4031762.1408289038\n",
            "925\n",
            "4031762.1408289038\n",
            "926\n",
            "4031762.1408289038\n",
            "927\n",
            "4031762.1408289038\n",
            "928\n",
            "4031762.1408289038\n",
            "929\n",
            "4031762.1408289038\n",
            "930\n",
            "4031762.1408289038\n",
            "931\n",
            "4031762.1408289038\n",
            "932\n",
            "4031762.1408289038\n",
            "933\n",
            "4031762.1408289038\n",
            "934\n",
            "4031762.1408289038\n",
            "935\n",
            "4031762.1408289038\n",
            "936\n",
            "4031762.1408289038\n",
            "937\n",
            "4031762.1408289038\n",
            "938\n",
            "4031762.1408289038\n",
            "939\n",
            "4031762.1408289038\n",
            "940\n",
            "4031762.1408289038\n",
            "941\n",
            "4031762.1408289038\n",
            "942\n",
            "4031762.1408289038\n",
            "943\n",
            "4031762.1408289038\n",
            "944\n",
            "4031762.1408289038\n",
            "945\n",
            "4031762.1408289038\n",
            "946\n",
            "4031762.1408289038\n",
            "947\n",
            "4031762.1408289038\n",
            "948\n",
            "4031762.1408289038\n",
            "949\n",
            "4031762.1408289038\n",
            "950\n",
            "4031762.1408289038\n",
            "951\n",
            "4031762.1408289038\n",
            "952\n",
            "4031762.1408289038\n",
            "953\n",
            "4031762.1408289038\n",
            "954\n",
            "4031762.1408289038\n",
            "955\n",
            "4031762.1408289038\n",
            "956\n",
            "4031762.1408289038\n",
            "957\n",
            "4031762.1408289038\n",
            "958\n",
            "4031762.1408289038\n",
            "959\n",
            "4031762.1408289038\n",
            "960\n",
            "4031762.1408289038\n",
            "961\n",
            "4031762.1408289038\n",
            "962\n",
            "4031762.1408289038\n",
            "963\n",
            "4031762.1408289038\n",
            "964\n",
            "4031762.1408289038\n",
            "965\n",
            "4031762.1408289038\n",
            "966\n",
            "4031762.1408289038\n",
            "967\n",
            "4031762.1408289038\n",
            "968\n",
            "4031762.1408289038\n",
            "969\n",
            "4031762.1408289038\n",
            "970\n",
            "4031762.1408289038\n",
            "971\n",
            "4031762.1408289038\n",
            "972\n",
            "4031762.1408289038\n",
            "973\n",
            "4031762.1408289038\n",
            "974\n",
            "4031762.1408289038\n",
            "975\n",
            "4031762.1408289038\n",
            "976\n",
            "4031762.1408289038\n",
            "977\n",
            "4031762.1408289038\n",
            "978\n",
            "4031762.1408289038\n",
            "979\n",
            "4031762.1408289038\n",
            "980\n",
            "4031762.1408289038\n",
            "981\n",
            "4031762.1408289038\n",
            "982\n",
            "4031762.1408289038\n",
            "983\n",
            "4031762.1408289038\n",
            "984\n",
            "4031762.1408289038\n",
            "985\n",
            "4031762.1408289038\n",
            "986\n",
            "4031762.1408289038\n",
            "987\n",
            "4031762.1408289038\n",
            "988\n",
            "4031762.1408289038\n",
            "989\n",
            "4031762.1408289038\n",
            "990\n",
            "4031762.1408289038\n",
            "991\n",
            "4031762.1408289038\n",
            "992\n",
            "4031762.1408289038\n",
            "993\n",
            "4031762.1408289038\n",
            "994\n",
            "4031762.1408289038\n",
            "995\n",
            "4031762.1408289038\n",
            "996\n",
            "4031762.1408289038\n",
            "997\n",
            "4031762.1408289038\n",
            "998\n",
            "4031762.1408289038\n",
            "999\n",
            "4031762.1408289038\n",
            "1000\n",
            "4031762.1408289038\n",
            "1001\n",
            "4031762.1408289038\n",
            "1002\n",
            "4031762.1408289038\n",
            "1003\n",
            "4031762.1408289038\n",
            "1004\n",
            "4031762.1408289038\n",
            "1005\n",
            "4031762.1408289038\n",
            "1006\n",
            "4031762.1408289038\n",
            "1007\n",
            "4031762.1408289038\n",
            "1008\n",
            "4031762.1408289038\n",
            "1009\n",
            "4031762.1408289038\n",
            "1010\n",
            "4031762.1408289038\n",
            "1011\n",
            "4031762.1408289038\n",
            "1012\n",
            "4031762.1408289038\n",
            "1013\n",
            "4031762.1408289038\n",
            "1014\n",
            "4031762.1408289038\n",
            "1015\n",
            "4031762.1408289038\n",
            "1016\n",
            "4031762.1408289038\n",
            "1017\n",
            "4031762.1408289038\n",
            "1018\n",
            "4031762.1408289038\n",
            "1019\n",
            "4031762.1408289038\n",
            "1020\n",
            "4031762.1408289038\n",
            "1021\n",
            "4031762.1408289038\n",
            "1022\n",
            "4031762.1408289038\n",
            "1023\n",
            "4031762.1408289038\n",
            "1024\n",
            "4031762.1408289038\n",
            "1025\n",
            "4031762.1408289038\n",
            "1026\n",
            "4031762.1408289038\n",
            "1027\n",
            "4031762.1408289038\n",
            "1028\n",
            "4031762.1408289038\n",
            "1029\n",
            "4031762.1408289038\n",
            "1030\n",
            "4031762.1408289038\n",
            "1031\n",
            "4031762.1408289038\n",
            "1032\n",
            "4031762.1408289038\n",
            "1033\n",
            "4031762.1408289038\n",
            "1034\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "RDKit ERROR: [09:18:10] UFFTYPER: Unrecognized charge state for atom: 6\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4031762.1408289038\n",
            "1035\n",
            "4031762.1408289038\n",
            "1036\n",
            "4031762.1408289038\n",
            "1037\n",
            "4031762.1408289038\n",
            "1038\n",
            "4031762.1408289038\n",
            "1039\n",
            "4031762.1408289038\n",
            "1040\n",
            "4031762.1408289038\n",
            "1041\n",
            "4031762.1408289038\n",
            "1042\n",
            "4031762.1408289038\n",
            "1043\n",
            "4031762.1408289038\n",
            "1044\n",
            "4031762.1408289038\n",
            "1045\n",
            "4031762.1408289038\n",
            "1046\n",
            "4031762.1408289038\n",
            "1047\n",
            "4031762.1408289038\n",
            "1048\n",
            "4031762.1408289038\n",
            "1049\n",
            "4031762.1408289038\n",
            "1050\n",
            "4031762.1408289038\n",
            "1051\n",
            "4031762.1408289038\n",
            "1052\n",
            "4031762.1408289038\n",
            "1053\n",
            "4031762.1408289038\n",
            "1054\n",
            "4031762.1408289038\n",
            "1055\n",
            "4031762.1408289038\n",
            "1056\n",
            "4031762.1408289038\n",
            "1057\n",
            "4031762.1408289038\n",
            "1058\n",
            "4031762.1408289038\n",
            "1059\n",
            "4031762.1408289038\n",
            "1060\n",
            "4031762.1408289038\n",
            "1061\n",
            "4031762.1408289038\n",
            "1062\n",
            "4031762.1408289038\n",
            "1063\n",
            "4031762.1408289038\n",
            "1064\n",
            "4031762.1408289038\n",
            "1065\n",
            "4031762.1408289038\n",
            "1066\n",
            "4031762.1408289038\n",
            "1067\n",
            "4031762.1408289038\n",
            "1068\n",
            "4031762.1408289038\n",
            "1069\n",
            "4031762.1408289038\n",
            "1070\n",
            "4031762.1408289038\n",
            "1071\n",
            "4031762.1408289038\n",
            "1072\n",
            "4031762.1408289038\n",
            "1073\n",
            "4031762.1408289038\n",
            "1074\n",
            "4031762.1408289038\n",
            "1075\n",
            "4031762.1408289038\n",
            "1076\n",
            "4031762.1408289038\n",
            "1077\n",
            "4031762.1408289038\n",
            "1078\n",
            "4031762.1408289038\n",
            "1079\n",
            "4031762.1408289038\n",
            "1080\n",
            "4031762.1408289038\n",
            "1081\n",
            "4031762.1408289038\n",
            "1082\n",
            "4031762.1408289038\n",
            "1083\n",
            "4031762.1408289038\n",
            "1084\n",
            "4031762.1408289038\n",
            "1085\n",
            "4031762.1408289038\n",
            "1086\n",
            "4031762.1408289038\n",
            "1087\n",
            "4031762.1408289038\n",
            "1088\n",
            "4031762.1408289038\n",
            "1089\n",
            "4031762.1408289038\n",
            "1090\n",
            "4031762.1408289038\n",
            "1091\n",
            "4031762.1408289038\n",
            "1092\n",
            "4031762.1408289038\n",
            "1093\n",
            "4031762.1408289038\n",
            "1094\n",
            "4031762.1408289038\n",
            "1095\n",
            "4031762.1408289038\n",
            "1096\n",
            "4031762.1408289038\n",
            "1097\n",
            "4031762.1408289038\n",
            "1098\n",
            "4031762.1408289038\n",
            "1099\n",
            "4031762.1408289038\n",
            "1100\n",
            "4031762.1408289038\n",
            "1101\n",
            "4031762.1408289038\n",
            "1102\n",
            "4031762.1408289038\n",
            "1103\n",
            "4031762.1408289038\n",
            "1104\n",
            "4031762.1408289038\n",
            "1105\n",
            "4031762.1408289038\n",
            "1106\n",
            "4031762.1408289038\n",
            "1107\n",
            "4031762.1408289038\n",
            "1108\n",
            "4031762.1408289038\n",
            "1109\n",
            "4031762.1408289038\n",
            "1110\n",
            "4031762.1408289038\n",
            "1111\n",
            "4031762.1408289038\n",
            "1112\n",
            "4031762.1408289038\n",
            "1113\n",
            "4031762.1408289038\n",
            "1114\n",
            "4031762.1408289038\n",
            "1115\n",
            "4031762.1408289038\n",
            "1116\n",
            "4031762.1408289038\n",
            "1117\n",
            "4031762.1408289038\n",
            "1118\n",
            "4031762.1408289038\n",
            "1119\n",
            "4031762.1408289038\n",
            "1120\n",
            "4031762.1408289038\n",
            "1121\n",
            "4031762.1408289038\n",
            "1122\n",
            "4031762.1408289038\n",
            "1123\n",
            "4031762.1408289038\n",
            "1124\n",
            "4031762.1408289038\n",
            "1125\n",
            "4031762.1408289038\n",
            "1126\n",
            "4031762.1408289038\n",
            "1127\n",
            "4031762.1408289038\n",
            "1128\n",
            "4031762.1408289038\n",
            "1129\n",
            "4031762.1408289038\n",
            "1130\n",
            "4031762.1408289038\n",
            "1131\n",
            "4031762.1408289038\n",
            "1132\n",
            "4031762.1408289038\n",
            "1133\n",
            "4031762.1408289038\n",
            "1134\n",
            "4031762.1408289038\n",
            "1135\n",
            "4031762.1408289038\n",
            "1136\n",
            "4031762.1408289038\n",
            "1137\n",
            "4031762.1408289038\n",
            "1138\n",
            "4031762.1408289038\n",
            "1139\n",
            "4031762.1408289038\n",
            "1140\n",
            "4031762.1408289038\n",
            "1141\n",
            "4031762.1408289038\n",
            "1142\n",
            "4031762.1408289038\n",
            "1143\n",
            "4031762.1408289038\n",
            "1144\n",
            "4031762.1408289038\n",
            "1145\n",
            "4031762.1408289038\n",
            "1146\n",
            "4031762.1408289038\n",
            "1147\n",
            "4031762.1408289038\n",
            "1148\n",
            "4031762.1408289038\n",
            "1149\n",
            "4031762.1408289038\n",
            "1150\n",
            "4031762.1408289038\n",
            "1151\n",
            "4031762.1408289038\n",
            "1152\n",
            "4031762.1408289038\n",
            "1153\n",
            "4031762.1408289038\n",
            "1154\n",
            "4031762.1408289038\n",
            "1155\n",
            "4031762.1408289038\n",
            "1156\n",
            "4031762.1408289038\n",
            "1157\n",
            "4031762.1408289038\n",
            "1158\n",
            "4031762.1408289038\n",
            "1159\n",
            "4031762.1408289038\n",
            "1160\n",
            "4031762.1408289038\n",
            "1161\n",
            "4031762.1408289038\n",
            "1162\n",
            "4031762.1408289038\n",
            "1163\n",
            "4031762.1408289038\n",
            "1164\n",
            "4031762.1408289038\n",
            "1165\n",
            "4031762.1408289038\n",
            "1166\n",
            "4031762.1408289038\n",
            "1167\n",
            "4031762.1408289038\n",
            "1168\n",
            "4031762.1408289038\n",
            "1169\n",
            "4031762.1408289038\n",
            "1170\n",
            "4031762.1408289038\n",
            "1171\n",
            "4031762.1408289038\n",
            "1172\n",
            "4031762.1408289038\n",
            "1173\n",
            "4031762.1408289038\n",
            "1174\n",
            "4031762.1408289038\n",
            "1175\n",
            "4031762.1408289038\n",
            "1176\n",
            "4031762.1408289038\n",
            "1177\n",
            "4031762.1408289038\n",
            "1178\n",
            "4031762.1408289038\n",
            "1179\n",
            "4031762.1408289038\n",
            "1180\n",
            "4031762.1408289038\n",
            "1181\n",
            "4031762.1408289038\n",
            "1182\n",
            "4031762.1408289038\n",
            "1183\n",
            "4031762.1408289038\n",
            "1184\n",
            "4031762.1408289038\n",
            "1185\n",
            "4031762.1408289038\n",
            "1186\n",
            "4031762.1408289038\n",
            "1187\n",
            "4031762.1408289038\n",
            "1188\n",
            "4031762.1408289038\n",
            "1189\n",
            "4031762.1408289038\n",
            "1190\n",
            "4031762.1408289038\n",
            "1191\n",
            "4031762.1408289038\n",
            "1192\n",
            "4031762.1408289038\n",
            "1193\n",
            "4031762.1408289038\n",
            "1194\n",
            "4031762.1408289038\n",
            "1195\n",
            "4031762.1408289038\n",
            "1196\n",
            "4031762.1408289038\n",
            "1197\n",
            "4031762.1408289038\n",
            "1198\n",
            "4031762.1408289038\n",
            "1199\n",
            "4031762.1408289038\n",
            "1200\n",
            "4031762.1408289038\n",
            "1201\n",
            "4031762.1408289038\n",
            "1202\n",
            "4031762.1408289038\n",
            "1203\n",
            "4031762.1408289038\n",
            "1204\n",
            "4031762.1408289038\n",
            "1205\n",
            "4031762.1408289038\n",
            "1206\n",
            "4031762.1408289038\n",
            "1207\n",
            "4031762.1408289038\n",
            "1208\n",
            "4031762.1408289038\n",
            "1209\n",
            "4031762.1408289038\n",
            "1210\n",
            "4031762.1408289038\n",
            "1211\n",
            "4031762.1408289038\n",
            "1212\n",
            "4031762.1408289038\n",
            "1213\n",
            "4031762.1408289038\n",
            "1214\n",
            "4031762.1408289038\n",
            "1215\n",
            "4031762.1408289038\n",
            "1216\n",
            "4031762.1408289038\n",
            "1217\n",
            "4031762.1408289038\n",
            "1218\n",
            "4031762.1408289038\n",
            "1219\n",
            "4031762.1408289038\n",
            "1220\n",
            "4031762.1408289038\n",
            "1221\n",
            "4031762.1408289038\n",
            "1222\n",
            "4031762.1408289038\n",
            "1223\n",
            "4031762.1408289038\n",
            "1224\n",
            "4031762.1408289038\n",
            "1225\n",
            "4031762.1408289038\n",
            "1226\n",
            "4031762.1408289038\n",
            "1227\n",
            "4031762.1408289038\n",
            "1228\n",
            "4031762.1408289038\n",
            "1229\n",
            "4031762.1408289038\n",
            "1230\n",
            "4031762.1408289038\n",
            "1231\n",
            "4031762.1408289038\n",
            "1232\n",
            "4031762.1408289038\n",
            "1233\n",
            "4031762.1408289038\n",
            "1234\n",
            "4031762.1408289038\n",
            "1235\n",
            "4031762.1408289038\n",
            "1236\n",
            "4031762.1408289038\n",
            "1237\n",
            "4031762.1408289038\n",
            "1238\n",
            "4031762.1408289038\n",
            "1239\n",
            "4031762.1408289038\n",
            "1240\n",
            "4031762.1408289038\n",
            "1241\n",
            "4031762.1408289038\n",
            "1242\n",
            "4031762.1408289038\n",
            "1243\n",
            "4031762.1408289038\n",
            "1244\n",
            "4031762.1408289038\n",
            "1245\n",
            "4031762.1408289038\n",
            "1246\n",
            "4031762.1408289038\n",
            "1247\n",
            "4031762.1408289038\n",
            "1248\n",
            "4031762.1408289038\n",
            "1249\n",
            "4031762.1408289038\n",
            "1250\n",
            "4031762.1408289038\n",
            "1251\n",
            "4031762.1408289038\n",
            "1252\n",
            "4031762.1408289038\n",
            "1253\n",
            "4031762.1408289038\n",
            "1254\n",
            "4031762.1408289038\n",
            "1255\n",
            "4031762.1408289038\n",
            "1256\n",
            "4031762.1408289038\n",
            "1257\n",
            "4031762.1408289038\n",
            "1258\n",
            "4031762.1408289038\n",
            "1259\n",
            "4031762.1408289038\n",
            "1260\n",
            "4031762.1408289038\n",
            "1261\n",
            "4031762.1408289038\n",
            "1262\n",
            "4031762.1408289038\n",
            "1263\n",
            "4031762.1408289038\n",
            "1264\n",
            "4031762.1408289038\n",
            "1265\n",
            "4031762.1408289038\n",
            "1266\n",
            "4031762.1408289038\n",
            "1267\n",
            "4031762.1408289038\n",
            "1268\n",
            "4031762.1408289038\n",
            "1269\n",
            "4031762.1408289038\n",
            "1270\n",
            "4031762.1408289038\n",
            "1271\n",
            "4031762.1408289038\n",
            "1272\n",
            "4031762.1408289038\n",
            "1273\n",
            "4031762.1408289038\n",
            "1274\n",
            "4031762.1408289038\n",
            "1275\n",
            "4031762.1408289038\n",
            "1276\n",
            "4031762.1408289038\n",
            "1277\n",
            "4031762.1408289038\n",
            "1278\n",
            "4031762.1408289038\n",
            "1279\n",
            "4031762.1408289038\n",
            "1280\n",
            "4031762.1408289038\n",
            "1281\n",
            "4031762.1408289038\n",
            "1282\n",
            "4031762.1408289038\n",
            "1283\n",
            "4031762.1408289038\n",
            "1284\n",
            "4031762.1408289038\n",
            "1285\n",
            "4031762.1408289038\n",
            "1286\n",
            "4031762.1408289038\n",
            "1287\n",
            "4031762.1408289038\n",
            "1288\n",
            "4031762.1408289038\n",
            "1289\n",
            "4031762.1408289038\n",
            "1290\n",
            "4031762.1408289038\n",
            "1291\n",
            "4031762.1408289038\n",
            "1292\n",
            "4031762.1408289038\n",
            "1293\n",
            "4031762.1408289038\n",
            "1294\n",
            "4031762.1408289038\n",
            "1295\n",
            "4031762.1408289038\n",
            "1296\n",
            "4031762.1408289038\n",
            "1297\n",
            "4031762.1408289038\n",
            "1298\n",
            "4031762.1408289038\n",
            "1299\n",
            "4031762.1408289038\n",
            "1300\n",
            "4031762.1408289038\n",
            "1301\n",
            "4031762.1408289038\n",
            "1302\n",
            "4031762.1408289038\n",
            "1303\n",
            "4031762.1408289038\n",
            "1304\n",
            "4031762.1408289038\n",
            "1305\n",
            "4031762.1408289038\n",
            "1306\n",
            "4031762.1408289038\n",
            "1307\n",
            "4031762.1408289038\n",
            "1308\n",
            "4031762.1408289038\n",
            "1309\n",
            "4031762.1408289038\n",
            "1310\n",
            "4031762.1408289038\n",
            "1311\n",
            "4031762.1408289038\n",
            "1312\n",
            "4031762.1408289038\n",
            "1313\n",
            "4031762.1408289038\n",
            "1314\n",
            "4031762.1408289038\n",
            "1315\n",
            "4031762.1408289038\n",
            "1316\n",
            "4031762.1408289038\n",
            "1317\n",
            "4031762.1408289038\n",
            "1318\n",
            "4031762.1408289038\n",
            "1319\n",
            "4031762.1408289038\n",
            "1320\n",
            "4031762.1408289038\n",
            "1321\n",
            "4031762.1408289038\n",
            "1322\n",
            "4031762.1408289038\n",
            "1323\n",
            "4031762.1408289038\n",
            "1324\n",
            "4031762.1408289038\n",
            "1325\n",
            "4031762.1408289038\n",
            "1326\n",
            "4031762.1408289038\n",
            "1327\n",
            "4031762.1408289038\n",
            "1328\n",
            "4031762.1408289038\n",
            "1329\n",
            "4031762.1408289038\n",
            "1330\n",
            "4031762.1408289038\n",
            "1331\n",
            "4031762.1408289038\n",
            "1332\n",
            "4031762.1408289038\n",
            "1333\n",
            "4031762.1408289038\n",
            "1334\n",
            "4031762.1408289038\n",
            "1335\n",
            "4031762.1408289038\n",
            "1336\n",
            "4031762.1408289038\n",
            "1337\n",
            "4031762.1408289038\n",
            "1338\n",
            "4031762.1408289038\n",
            "1339\n",
            "4031762.1408289038\n",
            "1340\n",
            "4031762.1408289038\n",
            "1341\n",
            "4031762.1408289038\n",
            "1342\n",
            "4031762.1408289038\n",
            "1343\n",
            "4031762.1408289038\n",
            "1344\n",
            "4031762.1408289038\n",
            "1345\n",
            "4031762.1408289038\n",
            "1346\n",
            "4031762.1408289038\n",
            "1347\n",
            "4031762.1408289038\n",
            "1348\n",
            "4031762.1408289038\n",
            "1349\n",
            "4031762.1408289038\n",
            "1350\n",
            "4031762.1408289038\n",
            "1351\n",
            "4031762.1408289038\n",
            "1352\n",
            "4031762.1408289038\n",
            "1353\n",
            "4031762.1408289038\n",
            "1354\n",
            "4031762.1408289038\n",
            "1355\n",
            "4031762.1408289038\n",
            "1356\n",
            "4031762.1408289038\n",
            "1357\n",
            "4031762.1408289038\n",
            "1358\n",
            "4031762.1408289038\n",
            "1359\n",
            "4031762.1408289038\n",
            "1360\n",
            "4031762.1408289038\n",
            "1361\n",
            "4031762.1408289038\n",
            "1362\n",
            "4031762.1408289038\n",
            "1363\n",
            "4031762.1408289038\n",
            "1364\n",
            "4031762.1408289038\n",
            "1365\n",
            "4031762.1408289038\n",
            "1366\n",
            "4031762.1408289038\n",
            "1367\n",
            "4031762.1408289038\n",
            "1368\n",
            "4031762.1408289038\n",
            "1369\n",
            "4031762.1408289038\n",
            "1370\n",
            "4031762.1408289038\n",
            "1371\n",
            "4031762.1408289038\n",
            "1372\n",
            "4031762.1408289038\n",
            "1373\n",
            "4031762.1408289038\n",
            "1374\n",
            "4031762.1408289038\n",
            "1375\n",
            "4031762.1408289038\n",
            "1376\n",
            "4031762.1408289038\n",
            "1377\n",
            "4031762.1408289038\n",
            "1378\n",
            "4031762.1408289038\n",
            "1379\n",
            "4031762.1408289038\n",
            "1380\n",
            "4031762.1408289038\n",
            "1381\n",
            "4031762.1408289038\n",
            "1382\n",
            "4031762.1408289038\n",
            "1383\n",
            "4031762.1408289038\n",
            "1384\n",
            "4031762.1408289038\n",
            "1385\n",
            "4031762.1408289038\n",
            "1386\n",
            "4031762.1408289038\n",
            "1387\n",
            "4031762.1408289038\n",
            "1388\n",
            "4031762.1408289038\n",
            "1389\n",
            "4031762.1408289038\n",
            "1390\n",
            "4031762.1408289038\n",
            "1391\n",
            "4031762.1408289038\n",
            "1392\n",
            "4031762.1408289038\n",
            "1393\n",
            "4031762.1408289038\n",
            "1394\n",
            "4031762.1408289038\n",
            "1395\n",
            "4031762.1408289038\n",
            "1396\n",
            "4031762.1408289038\n",
            "1397\n",
            "4031762.1408289038\n",
            "1398\n",
            "4031762.1408289038\n",
            "1399\n",
            "4031762.1408289038\n",
            "1400\n",
            "4031762.1408289038\n",
            "1401\n",
            "4031762.1408289038\n",
            "1402\n",
            "4031762.1408289038\n",
            "1403\n",
            "4031762.1408289038\n",
            "1404\n",
            "4031762.1408289038\n",
            "1405\n",
            "4031762.1408289038\n",
            "1406\n",
            "4031762.1408289038\n",
            "1407\n",
            "4031762.1408289038\n",
            "1408\n",
            "4031762.1408289038\n",
            "1409\n",
            "4031762.1408289038\n",
            "1410\n",
            "4031762.1408289038\n",
            "1411\n",
            "4031762.1408289038\n",
            "1412\n",
            "4031762.1408289038\n",
            "1413\n",
            "4031762.1408289038\n",
            "1414\n",
            "4031762.1408289038\n",
            "1415\n",
            "4031762.1408289038\n",
            "1416\n",
            "4031762.1408289038\n",
            "1417\n",
            "4031762.1408289038\n",
            "1418\n",
            "4031762.1408289038\n",
            "1419\n",
            "4031762.1408289038\n",
            "1420\n",
            "4031762.1408289038\n",
            "1421\n",
            "4031762.1408289038\n",
            "1422\n",
            "4031762.1408289038\n",
            "1423\n",
            "4031762.1408289038\n",
            "1424\n",
            "4031762.1408289038\n",
            "1425\n",
            "4031762.1408289038\n",
            "1426\n",
            "4031762.1408289038\n",
            "1427\n",
            "4031762.1408289038\n",
            "1428\n",
            "4031762.1408289038\n",
            "1429\n",
            "4031762.1408289038\n",
            "1430\n",
            "4031762.1408289038\n",
            "1431\n",
            "4031762.1408289038\n",
            "1432\n",
            "4031762.1408289038\n",
            "1433\n",
            "4031762.1408289038\n",
            "1434\n",
            "4031762.1408289038\n",
            "1435\n",
            "4031762.1408289038\n",
            "1436\n",
            "4031762.1408289038\n",
            "1437\n",
            "4031762.1408289038\n",
            "1438\n",
            "4031762.1408289038\n",
            "1439\n",
            "4031762.1408289038\n",
            "1440\n",
            "4031762.1408289038\n",
            "1441\n",
            "4031762.1408289038\n",
            "1442\n",
            "4031762.1408289038\n",
            "1443\n",
            "4031762.1408289038\n",
            "1444\n",
            "4031762.1408289038\n",
            "1445\n",
            "4031762.1408289038\n",
            "1446\n",
            "4031762.1408289038\n",
            "1447\n",
            "4031762.1408289038\n",
            "1448\n",
            "4031762.1408289038\n",
            "1449\n",
            "4031762.1408289038\n",
            "1450\n",
            "4031762.1408289038\n",
            "1451\n",
            "4031762.1408289038\n",
            "1452\n",
            "4031762.1408289038\n",
            "1453\n",
            "4031762.1408289038\n",
            "1454\n",
            "4031762.1408289038\n",
            "1455\n",
            "4031762.1408289038\n",
            "1456\n",
            "4031762.1408289038\n",
            "1457\n",
            "4031762.1408289038\n",
            "1458\n",
            "4031762.1408289038\n",
            "1459\n",
            "4031762.1408289038\n",
            "1460\n",
            "4031762.1408289038\n",
            "1461\n",
            "4031762.1408289038\n",
            "1462\n",
            "4031762.1408289038\n",
            "1463\n",
            "4031762.1408289038\n",
            "1464\n",
            "4031762.1408289038\n",
            "1465\n",
            "4031762.1408289038\n",
            "1466\n",
            "4031762.1408289038\n",
            "1467\n",
            "4031762.1408289038\n",
            "1468\n",
            "4031762.1408289038\n",
            "1469\n",
            "4031762.1408289038\n",
            "1470\n",
            "4031762.1408289038\n",
            "1471\n",
            "4031762.1408289038\n",
            "1472\n",
            "4031762.1408289038\n",
            "1473\n",
            "4031762.1408289038\n",
            "1474\n",
            "4031762.1408289038\n",
            "1475\n",
            "4031762.1408289038\n",
            "1476\n",
            "4031762.1408289038\n",
            "1477\n",
            "4031762.1408289038\n",
            "1478\n",
            "4031762.1408289038\n",
            "1479\n",
            "4031762.1408289038\n",
            "1480\n",
            "4031762.1408289038\n",
            "1481\n",
            "4031762.1408289038\n",
            "1482\n",
            "4031762.1408289038\n",
            "1483\n",
            "4031762.1408289038\n",
            "1484\n",
            "4031762.1408289038\n",
            "1485\n",
            "4031762.1408289038\n",
            "1486\n",
            "4031762.1408289038\n",
            "1487\n",
            "4031762.1408289038\n",
            "1488\n",
            "4031762.1408289038\n",
            "1489\n",
            "4031762.1408289038\n",
            "1490\n",
            "4031762.1408289038\n",
            "1491\n",
            "4031762.1408289038\n",
            "1492\n",
            "4031762.1408289038\n",
            "1493\n",
            "4031762.1408289038\n",
            "1494\n",
            "4031762.1408289038\n",
            "1495\n",
            "4031762.1408289038\n",
            "1496\n",
            "4031762.1408289038\n",
            "1497\n",
            "4031762.1408289038\n",
            "1498\n",
            "4031762.1408289038\n",
            "1499\n",
            "4031762.1408289038\n",
            "1500\n",
            "4031762.1408289038\n",
            "1501\n",
            "4031762.1408289038\n",
            "1502\n",
            "4031762.1408289038\n",
            "1503\n",
            "4031762.1408289038\n",
            "1504\n",
            "4031762.1408289038\n",
            "1505\n",
            "4031762.1408289038\n",
            "1506\n",
            "4031762.1408289038\n",
            "1507\n",
            "4031762.1408289038\n",
            "1508\n",
            "4031762.1408289038\n",
            "1509\n",
            "4031762.1408289038\n",
            "1510\n",
            "4031762.1408289038\n",
            "1511\n",
            "4031762.1408289038\n",
            "1512\n",
            "4031762.1408289038\n",
            "1513\n",
            "4031762.1408289038\n",
            "1514\n",
            "4031762.1408289038\n",
            "1515\n",
            "4031762.1408289038\n",
            "1516\n",
            "4031762.1408289038\n",
            "1517\n",
            "4031762.1408289038\n",
            "1518\n",
            "4031762.1408289038\n",
            "1519\n",
            "4031762.1408289038\n",
            "1520\n",
            "4031762.1408289038\n",
            "1521\n",
            "4031762.1408289038\n",
            "1522\n",
            "4031762.1408289038\n",
            "1523\n",
            "4031762.1408289038\n",
            "1524\n",
            "4031762.1408289038\n",
            "1525\n",
            "4031762.1408289038\n",
            "1526\n",
            "4031762.1408289038\n",
            "1527\n",
            "4031762.1408289038\n",
            "1528\n",
            "4031762.1408289038\n",
            "1529\n",
            "4031762.1408289038\n",
            "1530\n",
            "4031762.1408289038\n",
            "1531\n",
            "4031762.1408289038\n",
            "1532\n",
            "4031762.1408289038\n",
            "1533\n",
            "4031762.1408289038\n",
            "1534\n",
            "4031762.1408289038\n",
            "1535\n",
            "4031762.1408289038\n",
            "1536\n",
            "4031762.1408289038\n",
            "1537\n",
            "4031762.1408289038\n",
            "1538\n",
            "4031762.1408289038\n",
            "1539\n",
            "4031762.1408289038\n",
            "1540\n",
            "4031762.1408289038\n",
            "1541\n",
            "4031762.1408289038\n",
            "1542\n",
            "4031762.1408289038\n",
            "1543\n",
            "4031762.1408289038\n",
            "1544\n",
            "4031762.1408289038\n",
            "1545\n",
            "4031762.1408289038\n",
            "1546\n",
            "4031762.1408289038\n",
            "1547\n",
            "4031762.1408289038\n",
            "1548\n",
            "4031762.1408289038\n",
            "1549\n",
            "4031762.1408289038\n",
            "1550\n",
            "4031762.1408289038\n",
            "1551\n",
            "4031762.1408289038\n",
            "1552\n",
            "4031762.1408289038\n",
            "1553\n",
            "4031762.1408289038\n",
            "1554\n",
            "4031762.1408289038\n",
            "1555\n",
            "4031762.1408289038\n",
            "1556\n",
            "4031762.1408289038\n",
            "1557\n",
            "4031762.1408289038\n",
            "1558\n",
            "4031762.1408289038\n",
            "1559\n",
            "4031762.1408289038\n",
            "1560\n",
            "4031762.1408289038\n",
            "1561\n",
            "4031762.1408289038\n",
            "1562\n",
            "4031762.1408289038\n",
            "1563\n",
            "4031762.1408289038\n",
            "1564\n",
            "4031762.1408289038\n",
            "1565\n",
            "4031762.1408289038\n",
            "1566\n",
            "4031762.1408289038\n",
            "1567\n",
            "4031762.1408289038\n",
            "1568\n",
            "4031762.1408289038\n",
            "1569\n",
            "4031762.1408289038\n",
            "1570\n",
            "4031762.1408289038\n",
            "1571\n",
            "4031762.1408289038\n",
            "1572\n",
            "4031762.1408289038\n",
            "1573\n",
            "4031762.1408289038\n",
            "1574\n",
            "4031762.1408289038\n",
            "1575\n",
            "4031762.1408289038\n",
            "1576\n",
            "4031762.1408289038\n",
            "1577\n",
            "4031762.1408289038\n",
            "1578\n",
            "4031762.1408289038\n",
            "1579\n",
            "4031762.1408289038\n",
            "1580\n",
            "4031762.1408289038\n",
            "1581\n",
            "4031762.1408289038\n",
            "1582\n",
            "4031762.1408289038\n",
            "1583\n",
            "4031762.1408289038\n",
            "1584\n",
            "4031762.1408289038\n",
            "1585\n",
            "4031762.1408289038\n",
            "1586\n",
            "4031762.1408289038\n",
            "1587\n",
            "4031762.1408289038\n",
            "1588\n",
            "4031762.1408289038\n",
            "1589\n",
            "4031762.1408289038\n",
            "1590\n",
            "4031762.1408289038\n",
            "1591\n",
            "4031762.1408289038\n",
            "1592\n",
            "4031762.1408289038\n",
            "1593\n",
            "4031762.1408289038\n",
            "1594\n",
            "4031762.1408289038\n",
            "1595\n",
            "4031762.1408289038\n",
            "1596\n",
            "4031762.1408289038\n",
            "1597\n",
            "4031762.1408289038\n",
            "1598\n",
            "4031762.1408289038\n",
            "1599\n",
            "4031762.1408289038\n",
            "1600\n",
            "4031762.1408289038\n",
            "1601\n",
            "4031762.1408289038\n",
            "1602\n",
            "4031762.1408289038\n",
            "1603\n",
            "4031762.1408289038\n",
            "1604\n",
            "4031762.1408289038\n",
            "1605\n",
            "4031762.1408289038\n",
            "1606\n",
            "4031762.1408289038\n",
            "1607\n",
            "4031762.1408289038\n",
            "1608\n",
            "4031762.1408289038\n",
            "1609\n",
            "4031762.1408289038\n",
            "1610\n",
            "4031762.1408289038\n",
            "1611\n",
            "4031762.1408289038\n",
            "1612\n",
            "4031762.1408289038\n",
            "1613\n",
            "4031762.1408289038\n",
            "1614\n",
            "4031762.1408289038\n",
            "1615\n",
            "4031762.1408289038\n",
            "1616\n",
            "4031762.1408289038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JivWKbnZHY1D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "df993c74-6b0b-4a15-c321-23ce75a9327a"
      },
      "source": [
        "# Make a copy of the matrix to use in this notebook\n",
        "cmatrices = np.array(cmatrices)\n",
        "cmatrices_copy = cmatrices.copy()"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-150-281ec0aceaec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcmatrices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmatrices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcmatrices_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmatrices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cmatrices' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pai4-hPKH_9i",
        "colab_type": "text"
      },
      "source": [
        "## Find the indices of datapoints which have a Coulomb element of more than 1000 (probably due to a glitch in the MM94 optimization) and remove these from our dataset. Also, remove datapoints which have binding affinity of 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0Dq01VkF1iC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_to_remove = np.where(cmatrices_copy > 1000)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHwvmf7kF1mx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1ac885dd-a233-40a9-8e23-aef793f0f0cd"
      },
      "source": [
        "idx_to_remove"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  80,  133,  133,  147,  147,  148,  148,  151,  151,  151,  151,\n",
              "        156,  188,  252,  265,  265,  284,  300,  393,  393,  401,  401,\n",
              "        402,  402,  403,  425,  438,  484,  548,  548,  548,  548,  562,\n",
              "        562,  562,  562,  562,  562,  563,  563,  613,  643,  643,  679,\n",
              "        679,  680,  680,  707,  716,  789,  861,  862,  894,  894,  894,\n",
              "        894,  917,  975,  994,  994,  994,  994,  994,  994,  994,  994,\n",
              "       1074, 1209, 1224, 1257, 1292, 1321, 1323, 1324, 1346, 1347, 1352,\n",
              "       1392, 1392, 1504, 1512, 1545, 1548, 1548])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J1lVFHQF1qL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec7ceb83-2313-4f9f-e9b6-ec3a41b9035a"
      },
      "source": [
        "## Remove these glitchy datapoints\n",
        "cmatrices_copy = np.delete(cmatrices_copy, idx_to_remove, axis = 0); cmatrices_copy.shape"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1566, 122, 122)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN5peLjAIk2V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca33b504-4853-4278-ad61-a7ea13a0a5e7"
      },
      "source": [
        "# Check where our MM94 optimization completely failed\n",
        "# to even generate a conformer\n",
        "\n",
        "failed_indices"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[559]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZlGhDwMIn2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dropping these failed indices\n",
        "\n",
        "affinity_train_test = affinity_train_test.drop(affinity_train_test.index[failed_indices])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xZfjV80Iq28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dropping datapoints that had too large Coulomb values\n",
        "\n",
        "affinity_train_test = affinity_train_test.drop(affinity_train_test.index[idx_to_remove])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yL-vulPJha4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06773aaa-f6d2-4bd2-db75-1b549e800091"
      },
      "source": [
        "affinity_train_test.shape"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1566,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Une1ZfRQIF2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c463c92f-4a7c-4a62-bbc5-ea253f05e263"
      },
      "source": [
        "# Check shape of our data matrix\n",
        "cmatrices_copy.shape"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1566, 122, 122)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0ijz_0uuke2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3ec5017e-d0e4-4e16-dce3-c0025b37449d"
      },
      "source": [
        "# Checking that there are no 0 values in our\n",
        "# y_labels series\n",
        "\n",
        "# Need to remove this value!!\n",
        "\n",
        "affinity_train_test[affinity_train_test < 0.1]\n",
        "\n"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "885     0.000\n",
              "1831    0.023\n",
              "2209    0.080\n",
              "2511    0.040\n",
              "Name: Standard Value, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlmykAH2uki6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "afa40aa9-f65a-48dd-bdb1-d57195e2da4e"
      },
      "source": [
        "# Some (sort of) hacky code to get rid of the weird drug from our dataset\n",
        "idx_to_drop = np.where(affinity_train_test.index==885)[0]; idx_to_drop\n",
        "affinity_train_test = affinity_train_test.drop(885)\n",
        "cmatrices_copy = np.delete(cmatrices_copy, idx_to_drop, axis = 0)\n",
        "cmatrices_copy.shape"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1565, 122, 122)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6edhXmCa8T1w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ee64608-3e53-45ef-dcbc-f49ec3d88f18"
      },
      "source": [
        "affinity_train_test.shape"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1565,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnRA-GMQU-pk",
        "colab_type": "text"
      },
      "source": [
        "## Convert binding affinity to the natural logarithm of binding affinity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo0o3Xkqsc6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will try to predict the log binding affinity.\n",
        "# This is more natural to do in the context of drug binding.\n",
        "\n",
        "log_affinity_train_test = np.log(affinity_train_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiXrWllwVPnA",
        "colab_type": "text"
      },
      "source": [
        "## Have a look at our final datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbNtMB8dVQGJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9bfbf8a-2ded-455c-edf4-4acd97c05e19"
      },
      "source": [
        "cmatrices_copy.shape, log_affinity_train_test.shape"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1565, 122, 122), (1565,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLZx_BItWyQF",
        "colab_type": "text"
      },
      "source": [
        "## Let's look at the distribution of log(binding affinities) for the drugs in this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpE9nW-3ucVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "a4b8e1f1-7fcb-46d4-8087-0e76ac136ffc"
      },
      "source": [
        "# Let's take a look at our distribution of \n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(log_affinity_train_test)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  8.,  10.,  35., 135., 161., 298., 356., 355., 152.,  55.]),\n",
              " array([-3.77226106, -2.14933642, -0.52641178,  1.09651286,  2.71943751,\n",
              "         4.34236215,  5.96528679,  7.58821143,  9.21113608, 10.83406072,\n",
              "        12.45698536]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngNcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEjtl7-iYNRa",
        "colab_type": "text"
      },
      "source": [
        "## This looks like it's going to be a fairly hard dataset to separate..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTdbFa4JMXFN",
        "colab_type": "text"
      },
      "source": [
        "## Split the dataset into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V6IwdJ97YwkH",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(cmatrices_copy, np.array(log_affinity_train_test), random_state = 101)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnLTc_ytbFFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8gaCOjfx5Dw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0718cbcb-a058-4beb-ca84-1550172a663a"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1173, 122, 122), (392, 122, 122), (1173,), (392,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKmHTA7zgPKZ",
        "colab_type": "text"
      },
      "source": [
        "# Converting the Coulomb matrix into a more suitable input for a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kafb2LogbBW",
        "colab_type": "text"
      },
      "source": [
        "##Two problems with the Coulomb matrix are the follows:\n",
        "1. The size of the Coulomb matrix is equal to the number of atoms in the molecule. Therefore different molecules will have differently sized Coulomb matrices, which is a less-than-ideal data input.\n",
        "2.  For one molecule with N atoms, there exists N! possible valid Coulomb matrices, as the molecule is invariant to atom index labelling, but each different set of labels (for which there are N! of them) correspond to a different Coulomb matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyyVOrlQhgBO",
        "colab_type": "text"
      },
      "source": [
        "## The first of these issues can be sorted with ***padding***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkJzBvZzhpqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vmsj0CVBL-D0",
        "colab_type": "text"
      },
      "source": [
        "## Define some input preprocessing functions to tackle the above problems"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJQIhxEHME3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Module:\n",
        "  def update(self, lr):pass\n",
        "  def average(self,nn,a):pass\n",
        "  def backward(self,DY):pass\n",
        "  def forward(self,X):pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCR1FJCaNUfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Input(Module):\n",
        "  \n",
        "  def __init__(self,X):\n",
        "    self.step=1.0\n",
        "    self.noise=1.0\n",
        "    \n",
        "    # Boolean array to select for only the upper triangular half of the matrix.\n",
        "    # Since the matrix is symmetric, the lower triangular half is exactly the same\n",
        "    # as the upper triangular half, hence redundant information.\n",
        "    \n",
        "    self.triuind = (np.arange(122)[:,np.newaxis] <= np.arange(122)[np.newaxis,:]).flatten()\n",
        "    \n",
        "    # Get the maximum value of the X array - this will be required to generate\n",
        "    # the binary inputs to our neural network\n",
        "    self.max = 0\n",
        "    for _ in range(10): \n",
        "      self.max=np.maximum(self.max,self.realize(X).max(axis=0))\n",
        "      \n",
        "    # Obtain some normalization metrics about the dataset\n",
        "    X = self.expand(self.realize(X))\n",
        "    self.nbout = X.shape[1]\n",
        "    self.mean = X.mean(axis=0)\n",
        "    self.std = (X - self.mean).std()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  def realize(self,X):\n",
        "    '''\n",
        "    Function to sort the Coulomb matrix (with some random noise)\n",
        "    '''\n",
        "    def _realize_(x):\n",
        "      inds = np.argsort(-(x**2).sum(axis=0)**.5+np.random.normal(0,self.noise,x[0].shape))\n",
        "      x = x[inds,:][:,inds]*1\n",
        "      x = x.flatten()[self.triuind]\n",
        "      return x\n",
        "    return np.array([_realize_(z) for z in X])\n",
        "      \n",
        "    \n",
        "  def expand(self,X):\n",
        "    '''\n",
        "    Function to expand the Coulomb matrix into a set of (essentially) binary inputs\n",
        "    '''\n",
        "    Xexp = []\n",
        "    for i in range(X.shape[1]):\n",
        "      for k in np.arange(0,self.max[i]+self.step,self.step):\n",
        "        Xexp += [np.tanh((X[:,i]-k)/self.step)]\n",
        "    return np.array(Xexp).T\n",
        "  \n",
        "  \n",
        "  def normalize(self,X): return (X-self.mean)/self.std\n",
        "  \n",
        "  def forward(self,X): return self.normalize(self.expand(self.realize(X))).astype('float32')\n",
        "\n",
        "  \n",
        "  \n",
        "class Output(Module):\n",
        "  \n",
        "  def __init__(self,T):\n",
        "    self.tmean = T.mean()\n",
        "    self.tstd = T.std()\n",
        "    self.nbinp = 1\n",
        "    \n",
        "  def forward(self,X):\n",
        "    return X*self.tstd + self.tmean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPDz2q-WT4Rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck9_9EWFZAoR",
        "colab_type": "text"
      },
      "source": [
        "# Tensorflow time..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W89y1XgY_zA",
        "colab_type": "text"
      },
      "source": [
        "## Define some helper functions to shuffle the training data, as well as get the next batch of training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q8UggBbbMoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def randomize(x,y):\n",
        "  '''\n",
        "  Randomizes the order of data samples and corresponding labels\n",
        "  '''\n",
        "  perm = np.random.permutation(y.shape[0])\n",
        "  shuffled_x = x[perm]\n",
        "  shuffled_y = y[perm]\n",
        "  return shuffled_x, shuffled_y\n",
        "def get_next_batch(x,y,start,end):\n",
        "  '''\n",
        "  Gets the next batch of training data\n",
        "  '''\n",
        "  x_batch = x[start:end]\n",
        "  y_batch = y[start:end]\n",
        "  return x_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsLNeL15ZNrq",
        "colab_type": "text"
      },
      "source": [
        "## Define two helper functions to help us initalize the weights with Xavier initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2j_XSv1Y9KD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weight_variable(name, shape):\n",
        "  initer = tf.truncated_normal_initializer(stddev=(1/(shape[0])**.5))\n",
        "  return tf.get_variable('W_'+name,\n",
        "                        dtype=tf.float32,\n",
        "                        shape=shape,\n",
        "                        initializer=initer)\n",
        "\n",
        "def bias_variable(name,shape):\n",
        "  initial = tf.constant(0.,shape=shape,dtype=tf.float32)\n",
        "  return tf.get_variable('b_'+name,\n",
        "                        dtype=tf.float32,\n",
        "                        initializer=initial)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbvGFOsAZVQ1",
        "colab_type": "text"
      },
      "source": [
        "## Define a helper function to forward propogate an input through a fully connected neural network layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvGgO99hZTcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc_layer(x,num_units,name,use_sigmoid=True):\n",
        "  in_dim=x.get_shape().as_list()[1]\n",
        "  W=weight_variable(name,shape=[in_dim,num_units])\n",
        "  b=bias_variable(name,[num_units])\n",
        "  layer=tf.matmul(x,W)\n",
        "  layer+=b\n",
        "  if use_sigmoid:\n",
        "    layer=tf.nn.tanh(layer)\n",
        "  return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "652fRTAPNYWW",
        "colab_type": "text"
      },
      "source": [
        "## Apply the preprocessing to the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpii4x38Zgdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "I,O = Input(X_train),Output(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF3apttWZj-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b06085ff-0fc3-4ae9-f7ad-2f224856be6a"
      },
      "source": [
        "# The dimensionality of our flattened, binarized Coulomb matrix\n",
        "I.nbout"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95944"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtsrBFTZNlp3",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters of the NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVJ5HBl1Nnu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper-parameters\n",
        "epochs = 1000             # Total number of training epochs\n",
        "batch_size = 25        # Training batch size\n",
        "display_freq = 20      # Frequency of displaying the training results\n",
        "learning_rate = 0.0001   # The optimization initial learning rate\n",
        " \n",
        "  \n",
        "flattened_input_size = I.nbout\n",
        "n_classes = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTIflIvLNs9b",
        "colab_type": "text"
      },
      "source": [
        "## Reset the graph, define variables and the computational graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOs52gwnbMyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXLJ7YzbVRJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder(tf.float32,shape=[None,flattened_input_size],name='X')\n",
        "y = tf.placeholder(tf.float32,shape=[None,n_classes],name='Y')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_jK2USiVozy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computational Graph\n",
        "\n",
        "fc1 = fc_layer(x,400,'FC1',use_sigmoid=True)\n",
        "fc2 = fc_layer(fc1,100,'FC2',use_sigmoid=True)\n",
        "pre_preds = fc_layer(fc2,n_classes,'OUT',use_sigmoid=False)\n",
        "preds = O.forward(pre_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLWL1DlhNy2q",
        "colab_type": "text"
      },
      "source": [
        "## Define the loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0p6gvUkWfxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss function and optimizer\n",
        "\n",
        "loss = tf.losses.mean_squared_error(labels=y,predictions=preds)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,name='Adam-op').minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQELScUROBQH",
        "colab_type": "text"
      },
      "source": [
        "## Make a new folder to store the model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XBEcZPfOD76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2gQ2AxaOLrx",
        "colab_type": "text"
      },
      "source": [
        "## Initialize the variables, the saver, and GPU options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw3sdg6eXEYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init=tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlhCCsaxB-sw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKf_sudpLAzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khrs72N-OUIH",
        "colab_type": "text"
      },
      "source": [
        "# Start training the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AhwcyQ3YfSc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b25ee7d-ff1f-426e-c9b1-313e18356dc4"
      },
      "source": [
        "sess = tf.InteractiveSession(config = config)\n",
        "sess.run(init)\n",
        "global_step=0\n",
        "\n",
        "if os.path.exists('/content/tmp/checkpoint'):\n",
        "  print('\\nRestoring model...\\n')\n",
        "  saver.restore(sess, '/content/tmp/model.ckpt')\n",
        "else:\n",
        "  print('\\nCreating a saver to save the model weights\\n')\n",
        "  saver = tf.train.Saver()\n",
        "\n",
        "num_tr_iter = int(len(X_test) / batch_size)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('Training epoch:{}'.format(epoch+1))\n",
        "  \n",
        "  X_train, y_train = randomize(X_train,y_train)\n",
        "  \n",
        "  for iteration in trange(num_tr_iter):\n",
        "    global_step+=1\n",
        "    \n",
        "    start = iteration*batch_size\n",
        "    end = (iteration+1)*batch_size\n",
        "    \n",
        "    x_batch, y_batch = get_next_batch(X_train,y_train,start,end)\n",
        "    \n",
        "    x_batch = I.forward(x_batch)\n",
        "    \n",
        "    y_batch = y_batch[:,np.newaxis]\n",
        "    \n",
        "    # Run optimization operation\n",
        "    feed_dict_batch = {x:x_batch, y:y_batch}\n",
        "    sess.run(optimizer,feed_dict=feed_dict_batch)\n",
        "    \n",
        "\n",
        "  # Calc and display batch loss\n",
        "  loss_batch = sess.run(loss,feed_dict={x:x_batch,y:y_batch})\n",
        "  print(\"iter {0:3d}:\\t Training Loss={1:.2f}\".\n",
        "        format(iteration, loss_batch))\n",
        "            \n",
        "  # Calc and display validation loss \n",
        "  test_input = I.forward(X_test)\n",
        "  val_loss_batch = sess.run(loss, feed_dict={x:test_input, y:y_test[:,np.newaxis]})\n",
        "  print(\"iter {0:3d}:\\t Validation loss={1:.2f}\".format(iteration,\n",
        "                                                       val_loss_batch))\n",
        "      \n",
        "  # Save the model every epoch\n",
        "  save_path = saver.save(sess, \"/content/tmp/model.ckpt\")\n",
        "  print(\"Model saved in path: %s\" % save_path)"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Creating a saver to save the model weights\n",
            "\n",
            "Training epoch:1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:10<00:00,  1.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter  14:\t Training Loss=6.78\n",
            "iter  14:\t Validation loss=10.29\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved in path: /content/tmp/model.ckpt\n",
            "Training epoch:2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:10<00:00,  1.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter  14:\t Training Loss=7.42\n",
            "iter  14:\t Validation loss=7.08\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved in path: /content/tmp/model.ckpt\n",
            "Training epoch:3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:10<00:00,  1.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter  14:\t Training Loss=4.81\n",
            "iter  14:\t Validation loss=6.25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved in path: /content/tmp/model.ckpt\n",
            "Training epoch:4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:10<00:00,  1.44it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter  14:\t Training Loss=6.95\n",
            "iter  14:\t Validation loss=6.17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved in path: /content/tmp/model.ckpt\n",
            "Training epoch:5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:10<00:00,  1.44it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter  14:\t Training Loss=3.60\n",
            "iter  14:\t Validation loss=5.90\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved in path: /content/tmp/model.ckpt\n",
            "Training epoch:6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:10<00:00,  1.43it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter  14:\t Training Loss=2.44\n",
            "iter  14:\t Validation loss=5.89\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved in path: /content/tmp/model.ckpt\n",
            "Training epoch:7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:10<00:00,  1.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter  14:\t Training Loss=4.13\n",
            "iter  14:\t Validation loss=5.59\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved in path: /content/tmp/model.ckpt\n",
            "Training epoch:8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:10<00:00,  1.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iter  14:\t Training Loss=3.72\n",
            "iter  14:\t Validation loss=5.39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved in path: /content/tmp/model.ckpt\n",
            "Training epoch:9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 7/15 [00:04<00:05,  1.45it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-217-37c2eb52f3cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-145-963b7b198461>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-145-963b7b198461>\u001b[0m in \u001b[0;36mexpand\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mXexp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiBZgqtaSx11",
        "colab_type": "text"
      },
      "source": [
        "#See some of the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU2z0ZyAb4Bw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "00ef3b1a-326b-4885-e68c-9419546a8c2c"
      },
      "source": [
        "modxtest = I.forward(X_test)\n",
        "modytest = y_test.copy()[:,np.newaxis]\n",
        "print(modxtest.shape,modytest.shape)\n",
        "feed_dict_test = {x: modxtest, y:modytest}\n",
        "predictions = sess.run(preds, feed_dict = feed_dict_test)"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(392, 95944) (392, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMSytNAqdhPq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "03f67d58-9283-4414-e02c-bdbf9010dde1"
      },
      "source": [
        "import pandas as pd\n",
        "results_df = pd.DataFrame(np.array([predictions.reshape(-1),modytest.reshape(-1)]).T, columns = ['predictions','true']); results_df"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predictions</th>\n",
              "      <th>true</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.307949</td>\n",
              "      <td>4.065602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.199729</td>\n",
              "      <td>9.239899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.648564</td>\n",
              "      <td>5.010635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.925185</td>\n",
              "      <td>2.532903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.224054</td>\n",
              "      <td>4.143135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.786320</td>\n",
              "      <td>4.442651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>8.735395</td>\n",
              "      <td>9.392662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6.068913</td>\n",
              "      <td>3.850148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>6.468623</td>\n",
              "      <td>2.397895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5.163790</td>\n",
              "      <td>8.328934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>7.298910</td>\n",
              "      <td>7.863267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>7.084153</td>\n",
              "      <td>7.495542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>5.460505</td>\n",
              "      <td>5.799093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>7.299791</td>\n",
              "      <td>5.010635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>6.078752</td>\n",
              "      <td>7.244228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>9.640970</td>\n",
              "      <td>6.907755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>7.055809</td>\n",
              "      <td>9.210340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>8.409899</td>\n",
              "      <td>10.545341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>5.236543</td>\n",
              "      <td>6.907755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>7.853885</td>\n",
              "      <td>7.691200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>6.577185</td>\n",
              "      <td>6.593045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>7.740009</td>\n",
              "      <td>5.703782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>6.248491</td>\n",
              "      <td>5.828946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>9.216809</td>\n",
              "      <td>11.156251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>7.669116</td>\n",
              "      <td>4.234107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>6.674841</td>\n",
              "      <td>2.532903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>7.737313</td>\n",
              "      <td>7.138017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>6.932031</td>\n",
              "      <td>5.247024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>3.026953</td>\n",
              "      <td>3.258097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>7.791214</td>\n",
              "      <td>8.732305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>9.177060</td>\n",
              "      <td>9.392662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>4.990319</td>\n",
              "      <td>5.629059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>5.757910</td>\n",
              "      <td>9.903488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>4.945631</td>\n",
              "      <td>2.564949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>366</th>\n",
              "      <td>5.813056</td>\n",
              "      <td>6.907755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367</th>\n",
              "      <td>5.494741</td>\n",
              "      <td>7.438384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>6.903078</td>\n",
              "      <td>4.867534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>7.385701</td>\n",
              "      <td>6.870469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370</th>\n",
              "      <td>5.972128</td>\n",
              "      <td>6.257668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>7.879028</td>\n",
              "      <td>7.170120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>5.825750</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>8.979359</td>\n",
              "      <td>9.903488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>4.647740</td>\n",
              "      <td>1.740466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>5.590145</td>\n",
              "      <td>10.308953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>6.214162</td>\n",
              "      <td>12.400817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>3.426888</td>\n",
              "      <td>1.808289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>5.717722</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>4.866982</td>\n",
              "      <td>2.708050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>5.415040</td>\n",
              "      <td>6.907755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>6.705650</td>\n",
              "      <td>8.517193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>7.995681</td>\n",
              "      <td>8.779557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>7.355734</td>\n",
              "      <td>5.828946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>9.607982</td>\n",
              "      <td>9.047821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>6.521163</td>\n",
              "      <td>7.828791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>5.934394</td>\n",
              "      <td>6.821107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387</th>\n",
              "      <td>7.020532</td>\n",
              "      <td>3.828641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>7.882883</td>\n",
              "      <td>7.090077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>8.234948</td>\n",
              "      <td>9.903488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>8.866275</td>\n",
              "      <td>9.210340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>6.762305</td>\n",
              "      <td>3.496508</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>392 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     predictions       true\n",
              "0       7.307949   4.065602\n",
              "1       7.199729   9.239899\n",
              "2       6.648564   5.010635\n",
              "3       4.925185   2.532903\n",
              "4       6.224054   4.143135\n",
              "5       5.786320   4.442651\n",
              "6       8.735395   9.392662\n",
              "7       6.068913   3.850148\n",
              "8       6.468623   2.397895\n",
              "9       5.163790   8.328934\n",
              "10      7.298910   7.863267\n",
              "11      7.084153   7.495542\n",
              "12      5.460505   5.799093\n",
              "13      7.299791   5.010635\n",
              "14      6.078752   7.244228\n",
              "15      9.640970   6.907755\n",
              "16      7.055809   9.210340\n",
              "17      8.409899  10.545341\n",
              "18      5.236543   6.907755\n",
              "19      7.853885   7.691200\n",
              "20      6.577185   6.593045\n",
              "21      7.740009   5.703782\n",
              "22      6.248491   5.828946\n",
              "23      9.216809  11.156251\n",
              "24      7.669116   4.234107\n",
              "25      6.674841   2.532903\n",
              "26      7.737313   7.138017\n",
              "27      6.932031   5.247024\n",
              "28      3.026953   3.258097\n",
              "29      7.791214   8.732305\n",
              "..           ...        ...\n",
              "362     9.177060   9.392662\n",
              "363     4.990319   5.629059\n",
              "364     5.757910   9.903488\n",
              "365     4.945631   2.564949\n",
              "366     5.813056   6.907755\n",
              "367     5.494741   7.438384\n",
              "368     6.903078   4.867534\n",
              "369     7.385701   6.870469\n",
              "370     5.972128   6.257668\n",
              "371     7.879028   7.170120\n",
              "372     5.825750   4.605170\n",
              "373     8.979359   9.903488\n",
              "374     4.647740   1.740466\n",
              "375     5.590145  10.308953\n",
              "376     6.214162  12.400817\n",
              "377     3.426888   1.808289\n",
              "378     5.717722   4.605170\n",
              "379     4.866982   2.708050\n",
              "380     5.415040   6.907755\n",
              "381     6.705650   8.517193\n",
              "382     7.995681   8.779557\n",
              "383     7.355734   5.828946\n",
              "384     9.607982   9.047821\n",
              "385     6.521163   7.828791\n",
              "386     5.934394   6.821107\n",
              "387     7.020532   3.828641\n",
              "388     7.882883   7.090077\n",
              "389     8.234948   9.903488\n",
              "390     8.866275   9.210340\n",
              "391     6.762305   3.496508\n",
              "\n",
              "[392 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8FQFp4A3E2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_array = np.array(results_df['predictions'])\n",
        "true_array = np.array(results_df['true'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtycavI4S7kd",
        "colab_type": "text"
      },
      "source": [
        "## Convert to binary classification problem - see how it did"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk6Ke6Cb4Z6K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0e166453-782d-4749-e332-9662b2bb92b4"
      },
      "source": [
        "thresh = 5\n",
        "preds_bin = np.array([1 if x < thresh else 0 for x in preds_array])\n",
        "true_bin = np.array([1 if x < thresh else 0 for x in true_array])\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "confusion_matrix(true_bin,preds_bin)"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[256,  10],\n",
              "       [ 86,  40]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-nxX6gz4tKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "6e266655-e3ce-4969-9497-96266d0ddd7b"
      },
      "source": [
        "print(classification_report(true_bin,preds_bin))"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.96      0.84       266\n",
            "           1       0.80      0.32      0.45       126\n",
            "\n",
            "    accuracy                           0.76       392\n",
            "   macro avg       0.77      0.64      0.65       392\n",
            "weighted avg       0.77      0.76      0.72       392\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p97Ysn3ESkcv",
        "colab_type": "text"
      },
      "source": [
        "# Not bad at all, about as good as RF, and has the potential to predict out of cluster better"
      ]
    }
  ]
}