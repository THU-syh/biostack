{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepDrugDiscoveryExpt1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aced125/Coulomb_matrix_for_Drug_discovery/blob/master/CoulombNetv1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNBH2DK8HVvc",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a1b53a1-d74a-4044-995d-caa36a6c083b"
      },
      "source": [
        "#@title\n",
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!time conda install -q -y -c conda-forge rdkit"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-04 21:40:59--  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.200.79, 104.18.201.79, 2606:4700::6812:c94f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.200.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75257002 (72M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-latest-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-latest-L 100%[===================>]  71.77M   156MB/s    in 0.5s    \n",
            "\n",
            "2019-08-04 21:41:05 (156 MB/s) - ‘Miniconda3-latest-Linux-x86_64.sh’ saved [75257002/75257002]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==0.24.0=py37_0\n",
            "    - bzip2==1.0.8=h7b6447c_0\n",
            "    - ca-certificates==2019.5.15=0\n",
            "    - certifi==2019.6.16=py37_0\n",
            "    - cffi==1.12.3=py37h2e261b9_0\n",
            "    - chardet==3.0.4=py37_1\n",
            "    - conda-package-handling==1.3.11=py37_0\n",
            "    - conda==4.7.10=py37_0\n",
            "    - cryptography==2.7=py37h1ba5d50_0\n",
            "    - idna==2.8=py37_0\n",
            "    - libarchive==3.3.3=h5d8350f_5\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - libxml2==2.9.9=hea5a465_1\n",
            "    - lz4-c==1.8.1.2=h14c3975_0\n",
            "    - lzo==2.10=h49e0be7_2\n",
            "    - ncurses==6.1=he6710b0_1\n",
            "    - openssl==1.1.1c=h7b6447c_1\n",
            "    - pip==19.1.1=py37_0\n",
            "    - pycosat==0.6.3=py37h14c3975_0\n",
            "    - pycparser==2.19=py37_0\n",
            "    - pyopenssl==19.0.0=py37_0\n",
            "    - pysocks==1.7.0=py37_0\n",
            "    - python-libarchive-c==2.8=py37_11\n",
            "    - python==3.7.3=h0371630_0\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py37_0\n",
            "    - ruamel_yaml==0.15.46=py37h14c3975_0\n",
            "    - setuptools==41.0.1=py37_0\n",
            "    - six==1.12.0=py37_0\n",
            "    - sqlite==3.29.0=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.32.1=py_0\n",
            "    - urllib3==1.24.2=py37_0\n",
            "    - wheel==0.33.4=py37_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "    - zstd==1.3.7=h0b5b093_0\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-0.24.0-py37_0\n",
            "  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h7b6447c_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2019.5.15-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.6.16-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.12.3-py37h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1\n",
            "  conda              pkgs/main/linux-64::conda-4.7.10-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.3.11-py37_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.7-py37h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py37_0\n",
            "  libarchive         pkgs/main/linux-64::libarchive-3.3.3-h5d8350f_5\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  libxml2            pkgs/main/linux-64::libxml2-2.9.9-hea5a465_1\n",
            "  lz4-c              pkgs/main/linux-64::lz4-c-1.8.1.2-h14c3975_0\n",
            "  lzo                pkgs/main/linux-64::lzo-2.10-h49e0be7_2\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.1-he6710b0_1\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1c-h7b6447c_1\n",
            "  pip                pkgs/main/linux-64::pip-19.1.1-py37_0\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h14c3975_0\n",
            "  pycparser          pkgs/main/linux-64::pycparser-2.19-py37_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.0.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.0-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.3-h0371630_0\n",
            "  python-libarchive~ pkgs/main/linux-64::python-libarchive-c-2.8-py37_11\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py37_0\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.46-py37h14c3975_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-41.0.1-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.12.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.29.0-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.32.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.24.2-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.33.4-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "  zstd               pkgs/main/linux-64::zstd-1.3.7-h0b5b093_0\n",
            "\n",
            "\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ WARNING conda.core.envs_manager:register_env(46): Unable to register environment. Path not writable or missing.\n",
            "  environment location: /usr/local\n",
            "  registry file: /root/.conda/environments.txt\n",
            "\b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "\n",
            "real\t0m17.996s\n",
            "user\t0m10.462s\n",
            "sys\t0m5.175s\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - rdkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    boost-1.70.0               |   py37h9de70de_1         337 KB  conda-forge\n",
            "    boost-cpp-1.70.0           |       h8e57a91_2        21.1 MB  conda-forge\n",
            "    ca-certificates-2019.6.16  |       hecc5488_0         145 KB  conda-forge\n",
            "    cairo-1.16.0               |    hfb77d84_1002         1.5 MB  conda-forge\n",
            "    certifi-2019.6.16          |           py37_1         149 KB  conda-forge\n",
            "    conda-4.7.10               |           py37_0         3.0 MB  conda-forge\n",
            "    fontconfig-2.13.1          |    h86ecdb6_1001         340 KB  conda-forge\n",
            "    freetype-2.10.0            |       he983fc9_0         885 KB  conda-forge\n",
            "    gettext-0.19.8.1           |    hc5be6a0_1002         3.6 MB  conda-forge\n",
            "    glib-2.58.3                |    h6f030ca_1002         3.3 MB  conda-forge\n",
            "    icu-64.2                   |       he1b5a44_0        12.6 MB  conda-forge\n",
            "    jpeg-9c                    |    h14c3975_1001         251 KB  conda-forge\n",
            "    libblas-3.8.0              |      11_openblas          10 KB  conda-forge\n",
            "    libcblas-3.8.0             |      11_openblas          10 KB  conda-forge\n",
            "    libgfortran-ng-7.3.0       |       hdf63c60_0        1006 KB\n",
            "    libiconv-1.15              |    h516909a_1005         2.0 MB  conda-forge\n",
            "    liblapack-3.8.0            |      11_openblas          10 KB  conda-forge\n",
            "    libopenblas-0.3.6          |       h6e990d7_6         7.7 MB  conda-forge\n",
            "    libpng-1.6.37              |       hed695b0_0         343 KB  conda-forge\n",
            "    libtiff-4.0.10             |       h2733197_2         435 KB\n",
            "    libuuid-2.32.1             |    h14c3975_1000          26 KB  conda-forge\n",
            "    libxcb-1.13                |    h14c3975_1002         396 KB  conda-forge\n",
            "    lz4-c-1.8.3                |    he1b5a44_1001         187 KB  conda-forge\n",
            "    numpy-1.17.0               |   py37h95a1406_0         5.2 MB  conda-forge\n",
            "    olefile-0.46               |             py_0          31 KB  conda-forge\n",
            "    openssl-1.1.1c             |       h516909a_0         2.1 MB  conda-forge\n",
            "    pandas-0.25.0              |   py37hb3f55d8_0        11.4 MB  conda-forge\n",
            "    pcre-8.41                  |    hf484d3e_1003         249 KB  conda-forge\n",
            "    pillow-6.1.0               |   py37h34e0f95_0         635 KB\n",
            "    pixman-0.38.0              |    h516909a_1003         594 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h14c3975_1001           5 KB  conda-forge\n",
            "    pycairo-1.18.1             |   py37h438ddbb_0          77 KB  conda-forge\n",
            "    python-dateutil-2.8.0      |             py_0         219 KB  conda-forge\n",
            "    pytz-2019.2                |             py_0         228 KB  conda-forge\n",
            "    rdkit-2019.03.2            |   py37hb31dc5d_1        23.3 MB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h14c3975_1002          26 KB  conda-forge\n",
            "    xorg-libice-1.0.10         |       h516909a_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.3           |    h84519dc_1000          25 KB  conda-forge\n",
            "    xorg-libx11-1.6.8          |       h516909a_0         907 KB  conda-forge\n",
            "    xorg-libxau-1.0.9          |       h14c3975_0          13 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h516909a_0          18 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h516909a_0          51 KB  conda-forge\n",
            "    xorg-libxrender-0.9.10     |    h516909a_1002          31 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h14c3975_1002           8 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h14c3975_1002          27 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h14c3975_1007          72 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       104.4 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  boost              conda-forge/linux-64::boost-1.70.0-py37h9de70de_1\n",
            "  boost-cpp          conda-forge/linux-64::boost-cpp-1.70.0-h8e57a91_2\n",
            "  cairo              conda-forge/linux-64::cairo-1.16.0-hfb77d84_1002\n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.13.1-h86ecdb6_1001\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.0-he983fc9_0\n",
            "  gettext            conda-forge/linux-64::gettext-0.19.8.1-hc5be6a0_1002\n",
            "  glib               conda-forge/linux-64::glib-2.58.3-h6f030ca_1002\n",
            "  icu                conda-forge/linux-64::icu-64.2-he1b5a44_0\n",
            "  jpeg               conda-forge/linux-64::jpeg-9c-h14c3975_1001\n",
            "  libblas            conda-forge/linux-64::libblas-3.8.0-11_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.8.0-11_openblas\n",
            "  libgfortran-ng     pkgs/main/linux-64::libgfortran-ng-7.3.0-hdf63c60_0\n",
            "  libiconv           conda-forge/linux-64::libiconv-1.15-h516909a_1005\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.8.0-11_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.6-h6e990d7_6\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-hed695b0_0\n",
            "  libtiff            pkgs/main/linux-64::libtiff-4.0.10-h2733197_2\n",
            "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000\n",
            "  libxcb             conda-forge/linux-64::libxcb-1.13-h14c3975_1002\n",
            "  numpy              conda-forge/linux-64::numpy-1.17.0-py37h95a1406_0\n",
            "  olefile            conda-forge/noarch::olefile-0.46-py_0\n",
            "  pandas             conda-forge/linux-64::pandas-0.25.0-py37hb3f55d8_0\n",
            "  pcre               conda-forge/linux-64::pcre-8.41-hf484d3e_1003\n",
            "  pillow             pkgs/main/linux-64::pillow-6.1.0-py37h34e0f95_0\n",
            "  pixman             conda-forge/linux-64::pixman-0.38.0-h516909a_1003\n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h14c3975_1001\n",
            "  pycairo            conda-forge/linux-64::pycairo-1.18.1-py37h438ddbb_0\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.0-py_0\n",
            "  pytz               conda-forge/noarch::pytz-2019.2-py_0\n",
            "  rdkit              conda-forge/linux-64::rdkit-2019.03.2-py37hb31dc5d_1\n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.0.10-h516909a_0\n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.3-h84519dc_1000\n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.6.8-h516909a_0\n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h14c3975_0\n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h516909a_0\n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h516909a_0\n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h516909a_1002\n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h14c3975_1007\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2019.5.15-0 --> conda-forge::ca-certificates-2019.6.16-hecc5488_0\n",
            "  certifi               pkgs/main::certifi-2019.6.16-py37_0 --> conda-forge::certifi-2019.6.16-py37_1\n",
            "  lz4-c                 pkgs/main::lz4-c-1.8.1.2-h14c3975_0 --> conda-forge::lz4-c-1.8.3-he1b5a44_1001\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  conda                                           pkgs/main --> conda-forge\n",
            "  openssl              pkgs/main::openssl-1.1.1c-h7b6447c_1 --> conda-forge::openssl-1.1.1c-h516909a_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "\n",
            "real\t0m45.451s\n",
            "user\t0m39.301s\n",
            "sys\t0m5.920s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmlZYL-9pDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5q0WtZuHfJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJj7YpJFHp7y",
        "colab_type": "code",
        "outputId": "e1186da1-1607-4c37-f9ca-5b51e1d31be5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/aced125/RandomMatrixDiscriminant"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'RandomMatrixDiscriminant'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (5/5), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfHHnIi7ISD0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9caadf9f-b74f-4fef-a7f3-8e88b2cea235"
      },
      "source": [
        "# Imports\n",
        "import os\n",
        "\n",
        "# RDkit, a chemoinformatics library\n",
        "import rdkit\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import PandasTools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import functools\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import keras\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTpHv-DsJJ3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_morgan_matrix(smiles):\n",
        "    \n",
        "    # Function that generates the Morgan matrix - can take a couple of minutes\n",
        "    # for 1000s of molecules\n",
        "    \n",
        "    # Input is a pandas Series of SMILES representation of molecules\n",
        "    \n",
        "    morgan_matrix = np.zeros((1,2048))\n",
        "    l = len(smiles)\n",
        "    \n",
        "    # Iterate through the compounds\n",
        "    for i in range(l):\n",
        "        \n",
        "        # For each compound, get the structure, convert to Morgan fingerprint,\n",
        "        # and add to the data matrix\n",
        "        \n",
        "        try:\n",
        "            compound = Chem.MolFromSmiles(smiles[i])\n",
        "            fp = Chem.AllChem.GetMorganFingerprintAsBitVect(compound, 2, nBits = 2048)\n",
        "            fp = fp.ToBitString()\n",
        "            matrix_row = np.array([int(x) for x in list(fp)])\n",
        "            morgan_matrix = np.row_stack((morgan_matrix, matrix_row))\n",
        "            \n",
        "            # Progress checker\n",
        "            if i%500==0:\n",
        "                percentage = np.round(100*(i/l),1)\n",
        "                print(f'{percentage}% done')\n",
        "        except:\n",
        "            print(f'problem index:{i}')\n",
        "    \n",
        "    # Deleting first row of zeros\n",
        "    morgan_matrix = np.delete(morgan_matrix, 0, axis = 0)\n",
        "    \n",
        "    print('\\n')\n",
        "    print(f'Morgan Matrix dimensions:{morgan_matrix.shape}')\n",
        "    return morgan_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTcuH0oPuujj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset loading functions for ChemBL datasets\n",
        "def load_dataset(file_name, drop_non_binders = True):\n",
        "    df = pd.read_csv(file_name, encoding = \"ISO-8859-1\")\n",
        "    \n",
        "    # Converting strings to floats, also set all non-numbers to NaN\n",
        "    df['Standard Vaue'] = pd.to_numeric(df['Standard Value'],errors = 'coerce')\n",
        "\n",
        "    # Drop Nans in affinity column\n",
        "    df.dropna(subset = ['Standard Value'], inplace = True)\n",
        "    df.reset_index(inplace = True)\n",
        "    df = df.drop('index',axis = 1)\n",
        "    \n",
        "    # Filtering for only activities recorded in nanomolar affinity\n",
        "    df = df[df['Standard Units'] == 'nM']    \n",
        "\n",
        "    # Dropping any molecules that don't have a SMILES\n",
        "    df = df.dropna(subset = ['Canonical Smiles'])\n",
        "    \n",
        "    # Considering only the binders (compounds with affinities of less than 1000nM)\n",
        "    if drop_non_binders:\n",
        "        df = df[df['Standard Value'] < 1000]\n",
        "    \n",
        "    # Dropping duplicate molecules\n",
        "    df = df.drop_duplicates(subset = 'Canonical Smiles', keep = 'first')\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK62T7ubs00G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train_test = load_dataset('/content/RandomMatrixDiscriminant/adr1b_chembl.csv', False)\n",
        "df_decoy = load_dataset('/content/RandomMatrixDiscriminant/5ht1a_chembl.csv', drop_non_binders = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mkItiJBJW4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smiles_train_test = df_train_test['Canonical Smiles']\n",
        "smiles_train_test = smiles_train_test.reset_index()['Canonical Smiles']\n",
        "affinity_train_test = df_train_test['Standard Value']\n",
        "\n",
        "# Making \n",
        "binding_threshold = 1000  # units of nM\n",
        "affinity_train_test_binary = affinity_train_test.apply(lambda x: 1 if x<1000 else 0)\n",
        "\n",
        "smiles_decoy = df_decoy['Canonical Smiles']\n",
        "smiles_decoy = smiles_decoy.reset_index()['Canonical Smiles']\n",
        "affinity_decoy = pd.Series([0 for i in range(len(smiles_decoy))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3wdpfPywm-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "affinity_train_test = df_train_test['Standard Value']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_-Xc9kDva5l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "98603fb1-2c5d-4432-a5c1-bb89b0ad3f3a"
      },
      "source": [
        "Morgan_matrix_train_test = generate_morgan_matrix(smiles_train_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0% done\n",
            "30.9% done\n",
            "61.8% done\n",
            "92.8% done\n",
            "\n",
            "\n",
            "Morgan Matrix dimensions:(1617, 2048)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MfXBzUXvhh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(Morgan_matrix_train_test, affinity_train_test, stratify = affinity_train_test_binary, random_state = 3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pep2gUGovhlK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "405b09b9-e1e8-4557-a0b5-3cc5e07a6f9e"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(405,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njfmfoF2vhoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RMTClassifier(object):\n",
        "    \n",
        "    def __init__(self,threshold_multiple = 1, cutoff = 0.95):\n",
        "        self.cutoff = cutoff\n",
        "        self.thresh_multiple = threshold_multiple\n",
        "        \n",
        "\n",
        "    def _RMT(self, matrix):\n",
        "        N, p = matrix.shape\n",
        "        \n",
        "        gamma = p/N\n",
        "        thresh = ((1 + np.sqrt(gamma))**2)*self.thresh_multiple\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        matrix = scaler.fit_transform(matrix)\n",
        "        pca = PCA()\n",
        "        pca.fit_transform(matrix)\n",
        "        \n",
        "        # Find significant vector space V\n",
        "        dim_V = pca.explained_variance_[pca.explained_variance_>thresh].shape[0]\n",
        "        \n",
        "        return scaler, pca, dim_V\n",
        "    \n",
        "    def _distance_to_projection(self, data, vector_space):\n",
        "        # Given a vector space, V, we project the data onto that vector space, and then return distance to\n",
        "        # the original data\n",
        "        \n",
        "        # V is formatted such that different vectors are along different columns\n",
        "        \n",
        "        # Data matrix is formatted such that rows correspond to different data points\n",
        "        # and columns correspond to different features\n",
        "        \n",
        "        # projected_matrix = D V V.T\n",
        "        #\n",
        "        # finally, subtract original data (projectect_matrix - D) take euclidean norm along columns \n",
        "        \n",
        "        # Also note that the vector space can be as large/small as required\n",
        "        \n",
        "        return np.linalg.norm( np.dot(data, \n",
        "                                      np.dot( vector_space, vector_space.T ) )  - data,\n",
        "                             axis = 1)\n",
        "    \n",
        "    def _distance(self, data, vector_space):\n",
        "        return np.dot(data, np.dot(vector_space,vector_space.T)) - data\n",
        "    \n",
        "    def fit(self, X,y):\n",
        "        \n",
        "        # This code is optimized for speed using numpy matrix operations\n",
        "        X,y = np.array(X), np.array(y)\n",
        "        actives = X[np.where(y==1)[0],:]\n",
        "        inactives = X[np.where(y==0)[0],:]\n",
        "        \n",
        "        self.scaler_actives, self.pca_actives, self.dim_V_actives = self._RMT(actives)\n",
        "        self.scaler_inactives, self.pca_inactives, self.dim_V_inactives = self._RMT(inactives)\n",
        "        \n",
        "        metric = self.predict_scores(actives)\n",
        "        \n",
        "        idx = np.argsort(metric) #sorts in ascending order\n",
        "        metric = metric[idx]\n",
        "        cutoff_idx = int(self.cutoff * len(metric))\n",
        "        self.epsilon = metric[cutoff_idx]\n",
        "\n",
        "    \n",
        "    def predict_scores(self, X_test):\n",
        "        # As described in https://www.pnas.org/content/116/9/3373\n",
        "        self.scores = (self._distance_to_projection(self.scaler_actives.transform(X_test), \n",
        "                                                 self.pca_actives.components_.T[:, :self.dim_V_actives])\n",
        "                  - \n",
        "                  \n",
        "                 self._distance_to_projection(self.scaler_inactives.transform(X_test), \n",
        "                                                 self.pca_inactives.components_.T[:, :self.dim_V_inactives]))\n",
        "        return self.scores\n",
        "    \n",
        "    def projection(self, X):\n",
        "        return self._distance(self.scaler_actives.transform(X), \n",
        "                                                 self.pca_actives.components_.T[:, :self.dim_V_actives])\n",
        "    \n",
        "    def predict(self, X_test, epsilon_multiple = 1):\n",
        "        \n",
        "        scores = self.predict_scores(X_test)\n",
        "        predictions = np.array([1 if x<self.epsilon * epsilon_multiple else 0 for x in scores])\n",
        "        \n",
        "        return predictions\n",
        "    \n",
        "    def return_indices_of_common_molecules_of_active_eig(self, matrix, n=5,eigenvector_index=0):\n",
        "        \n",
        "        # returns molecules that lie closest to the selected eigenvector\n",
        "        # default number of molecules returned = 5\n",
        "        \n",
        "        # Pick out the (best) eigenvector\n",
        "        eig = self.feature_vecs[:,eigenvector_index].reshape(self.p,1)\n",
        "        \n",
        "        # Project molecules onto the one-dimensional vector space and get the indices of the top 5 molecules\n",
        "        # https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array\n",
        "        return np.argpartition(    np.dot(matrix, eig).reshape(matrix.shape[0])   , -n)[-n:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn4KFc8Svh0Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "3b3db67c-25c6-4a34-9495-ae4f83552768"
      },
      "source": [
        "clf = RMTClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "test_preds = clf.predict(X_test, epsilon_multiple = 1)\n",
        "print('confusion_matrix \\n',confusion_matrix(y_test, test_preds))\n",
        "print(classification_report(y_test,test_preds))"
      ],
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-347-59fb9ceda4b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMTClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_multiple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'confusion_matrix \\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-346-0e5b007c987d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler_actives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpca_actives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim_V_actives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RMT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler_inactives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpca_inactives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim_V_inactives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RMT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minactives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-346-0e5b007c987d>\u001b[0m in \u001b[0;36m_RMT\u001b[0;34m(self, matrix)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mthresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthresh_multiple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apmayx8oKnVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_rmt_train = clf.projection(X_train)\n",
        "X_rmt_test = clf.projection(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMryyISgLcDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FKlUl14Ll_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 2\n",
        "epochs = 20\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf7hjoFhLo4k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0c3b406-6dbc-4bdb-f6f4-36be9cbd7132"
      },
      "source": [
        "X_rmt_train = X_rmt_train.astype('float32')\n",
        "X_rmt_test = X_rmt_test.astype('float32')\n",
        "X_rmt_test.shape, y_test.shape"
      ],
      "execution_count": 764,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((405, 2048), (405,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 764
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJdS4cGgL9cO",
        "colab_type": "code",
        "outputId": "535019f1-a0de-46bf-ad19-60a69229a592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "y_train[:5]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2016     4329.0\n",
              "762     10000.0\n",
              "395       300.0\n",
              "168       250.0\n",
              "416       160.0\n",
              "Name: Standard Value, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK5qUKHqMBXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = keras.utils.to_categorical(np.array(y_train), num_classes)\n",
        "y_test = keras.utils.to_categorical(np.array(y_test), num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSl896mSxbd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_coulomb_matrix(numbers, coords, alpha=1, use_decay=False):\n",
        "    \"\"\"\n",
        "    Generates the unsorted Coulomb-matrix, given that\n",
        "    \"\"\"\n",
        "    top = np.outer(numbers, numbers).astype(np.float64)\n",
        "    r = cdist(coords, coords)\n",
        "    if use_decay:\n",
        "        other = cdist([coords[0]], coords).reshape(-1)\n",
        "        r += numpy.add.outer(other, other)\n",
        "\n",
        "    r **= alpha\n",
        "\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        np.divide(top, r, top)\n",
        "    np.fill_diagonal(top, 0.5 * np.array(numbers) ** 2.4)\n",
        "    top[top == np.Infinity] = 0\n",
        "    top[np.isnan(top)] = 0\n",
        "    return top"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFdY2mKRx2BP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31a6f27d-768d-4ffa-f339-a8633a2eb221"
      },
      "source": [
        "mol = Chem.MolFromSmiles(smiles_train_test[0])\n",
        "mol = Chem.AddHs(mol)\n",
        "Chem.EmbedMolecule(mol, Chem.ETKDG())"
      ],
      "execution_count": 353,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 353
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGk3TcoCjnZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "}from rdkit.Chem import AllChem as Chem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOhQ3xFgx2H3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf = mol.GetConformer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zzNyHJcx2Lk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8d605e6-7859-4df4-d3ea-ca695dd4a015"
      },
      "source": [
        "z = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
        "m = np.zeros((mol.GetNumAtoms(),mol.GetNumAtoms()))\n",
        "z = np.outer(z,z)\n",
        "z.shape"
      ],
      "execution_count": 355,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(61, 61)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 355
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RI570hqptO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smi = smiles_train_test[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsH2jsDApwvt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "c4729e11-264d-4fdd-9e80-4690a40a2a53"
      },
      "source": [
        "mol = Chem.MolFromSmiles(smi)\n",
        "mol = Chem.AddHs(mol)\n",
        "mol"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcIAAACWCAIAAADCEh9HAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\nZJRAIBAY8f/w4qMvaagNDgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<rdkit.Chem.rdchem.Mol at 0x7fd5ca2705d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NlaYBydpw1O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "800f1e8b-397a-47a4-bc38-2c1d8711c351"
      },
      "source": [
        "Chem.EmbedMolecule(mol, Chem.ETKDG())"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PWEeXqqqF42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf = molGetConformer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGza2XuQqGEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "363819b3-6ffe-4bdd-d3d7-ede69e07e47c"
      },
      "source": [
        "conf.GetPositions().shape"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(61, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZWuf72lqUIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiP83vkUqUL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "7f86819b-0a34-41aa-b588-501ef1f0b8de"
      },
      "source": [
        "mol.GetAtoms()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-7b09439d7a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetAtoms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetSymbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: '_ROAtomSeq' object has no attribute 'GetSymbol'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QffZpytppV4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rdkit.Chem import AllChem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3F6792pHYsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def coulomb_generator(smiles):\n",
        "    matrices = []\n",
        "    failed_indices = []\n",
        "    for idx,smile in enumerate(smiles):\n",
        "        print(idx)\n",
        "        mol = Chem.MolFromSmiles(smile)\n",
        "        mol = Chem.AddHs(mol)\n",
        "        try:\n",
        "          Chem.EmbedMolecule(mol, Chem.ETKDG())\n",
        "          conf = mol.GetConformer()\n",
        "          n_atoms = mol.GetNumAtoms()\n",
        "          z = np.array([atom.GetAtomicNum() for atom in mol.GetAtoms()])\n",
        "          xyz = conf.GetPositions()\n",
        "          m = get_coulomb_matrix(z,xyz)\n",
        "          \n",
        "          matrices.append(m)\n",
        "        \n",
        "        except:\n",
        "          print('failed_idx:',idx)\n",
        "          failed_indices.append(idx)\n",
        "    \n",
        "    max_atoms = max([m[0].shape[0] for m in matrices])\n",
        "    for index, matrix in enumerate(matrices):\n",
        "        n_atoms = matrix[0].shape[0]\n",
        "        m = np.zeros((max_atoms, max_atoms))\n",
        "        m[:n_atoms, :n_atoms] = matrix\n",
        "        matrices[index] = m\n",
        "        \n",
        "    return matrices, failed_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CYDhIUHkWfO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8d7f3355-d89c-40ba-cc38-164e7b9606bd"
      },
      "source": [
        "cmatrices, failed_indices = coulomb_generator(np.array(smiles_train_test))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "failed_idx: 559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "RDKit ERROR: [22:10:12] UFFTYPER: Unrecognized charge state for atom: 6\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n",
            "1141\n",
            "1142\n",
            "1143\n",
            "1144\n",
            "1145\n",
            "1146\n",
            "1147\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1151\n",
            "1152\n",
            "1153\n",
            "1154\n",
            "1155\n",
            "1156\n",
            "1157\n",
            "1158\n",
            "1159\n",
            "1160\n",
            "1161\n",
            "1162\n",
            "1163\n",
            "1164\n",
            "1165\n",
            "1166\n",
            "1167\n",
            "1168\n",
            "1169\n",
            "1170\n",
            "1171\n",
            "1172\n",
            "1173\n",
            "1174\n",
            "1175\n",
            "1176\n",
            "1177\n",
            "1178\n",
            "1179\n",
            "1180\n",
            "1181\n",
            "1182\n",
            "1183\n",
            "1184\n",
            "1185\n",
            "1186\n",
            "1187\n",
            "1188\n",
            "1189\n",
            "1190\n",
            "1191\n",
            "1192\n",
            "1193\n",
            "1194\n",
            "1195\n",
            "1196\n",
            "1197\n",
            "1198\n",
            "1199\n",
            "1200\n",
            "1201\n",
            "1202\n",
            "1203\n",
            "1204\n",
            "1205\n",
            "1206\n",
            "1207\n",
            "1208\n",
            "1209\n",
            "1210\n",
            "1211\n",
            "1212\n",
            "1213\n",
            "1214\n",
            "1215\n",
            "1216\n",
            "1217\n",
            "1218\n",
            "1219\n",
            "1220\n",
            "1221\n",
            "1222\n",
            "1223\n",
            "1224\n",
            "1225\n",
            "1226\n",
            "1227\n",
            "1228\n",
            "1229\n",
            "1230\n",
            "1231\n",
            "1232\n",
            "1233\n",
            "1234\n",
            "1235\n",
            "1236\n",
            "1237\n",
            "1238\n",
            "1239\n",
            "1240\n",
            "1241\n",
            "1242\n",
            "1243\n",
            "1244\n",
            "1245\n",
            "1246\n",
            "1247\n",
            "1248\n",
            "1249\n",
            "1250\n",
            "1251\n",
            "1252\n",
            "1253\n",
            "1254\n",
            "1255\n",
            "1256\n",
            "1257\n",
            "1258\n",
            "1259\n",
            "1260\n",
            "1261\n",
            "1262\n",
            "1263\n",
            "1264\n",
            "1265\n",
            "1266\n",
            "1267\n",
            "1268\n",
            "1269\n",
            "1270\n",
            "1271\n",
            "1272\n",
            "1273\n",
            "1274\n",
            "1275\n",
            "1276\n",
            "1277\n",
            "1278\n",
            "1279\n",
            "1280\n",
            "1281\n",
            "1282\n",
            "1283\n",
            "1284\n",
            "1285\n",
            "1286\n",
            "1287\n",
            "1288\n",
            "1289\n",
            "1290\n",
            "1291\n",
            "1292\n",
            "1293\n",
            "1294\n",
            "1295\n",
            "1296\n",
            "1297\n",
            "1298\n",
            "1299\n",
            "1300\n",
            "1301\n",
            "1302\n",
            "1303\n",
            "1304\n",
            "1305\n",
            "1306\n",
            "1307\n",
            "1308\n",
            "1309\n",
            "1310\n",
            "1311\n",
            "1312\n",
            "1313\n",
            "1314\n",
            "1315\n",
            "1316\n",
            "1317\n",
            "1318\n",
            "1319\n",
            "1320\n",
            "1321\n",
            "1322\n",
            "1323\n",
            "1324\n",
            "1325\n",
            "1326\n",
            "1327\n",
            "1328\n",
            "1329\n",
            "1330\n",
            "1331\n",
            "1332\n",
            "1333\n",
            "1334\n",
            "1335\n",
            "1336\n",
            "1337\n",
            "1338\n",
            "1339\n",
            "1340\n",
            "1341\n",
            "1342\n",
            "1343\n",
            "1344\n",
            "1345\n",
            "1346\n",
            "1347\n",
            "1348\n",
            "1349\n",
            "1350\n",
            "1351\n",
            "1352\n",
            "1353\n",
            "1354\n",
            "1355\n",
            "1356\n",
            "1357\n",
            "1358\n",
            "1359\n",
            "1360\n",
            "1361\n",
            "1362\n",
            "1363\n",
            "1364\n",
            "1365\n",
            "1366\n",
            "1367\n",
            "1368\n",
            "1369\n",
            "1370\n",
            "1371\n",
            "1372\n",
            "1373\n",
            "1374\n",
            "1375\n",
            "1376\n",
            "1377\n",
            "1378\n",
            "1379\n",
            "1380\n",
            "1381\n",
            "1382\n",
            "1383\n",
            "1384\n",
            "1385\n",
            "1386\n",
            "1387\n",
            "1388\n",
            "1389\n",
            "1390\n",
            "1391\n",
            "1392\n",
            "1393\n",
            "1394\n",
            "1395\n",
            "1396\n",
            "1397\n",
            "1398\n",
            "1399\n",
            "1400\n",
            "1401\n",
            "1402\n",
            "1403\n",
            "1404\n",
            "1405\n",
            "1406\n",
            "1407\n",
            "1408\n",
            "1409\n",
            "1410\n",
            "1411\n",
            "1412\n",
            "1413\n",
            "1414\n",
            "1415\n",
            "1416\n",
            "1417\n",
            "1418\n",
            "1419\n",
            "1420\n",
            "1421\n",
            "1422\n",
            "1423\n",
            "1424\n",
            "1425\n",
            "1426\n",
            "1427\n",
            "1428\n",
            "1429\n",
            "1430\n",
            "1431\n",
            "1432\n",
            "1433\n",
            "1434\n",
            "1435\n",
            "1436\n",
            "1437\n",
            "1438\n",
            "1439\n",
            "1440\n",
            "1441\n",
            "1442\n",
            "1443\n",
            "1444\n",
            "1445\n",
            "1446\n",
            "1447\n",
            "1448\n",
            "1449\n",
            "1450\n",
            "1451\n",
            "1452\n",
            "1453\n",
            "1454\n",
            "1455\n",
            "1456\n",
            "1457\n",
            "1458\n",
            "1459\n",
            "1460\n",
            "1461\n",
            "1462\n",
            "1463\n",
            "1464\n",
            "1465\n",
            "1466\n",
            "1467\n",
            "1468\n",
            "1469\n",
            "1470\n",
            "1471\n",
            "1472\n",
            "1473\n",
            "1474\n",
            "1475\n",
            "1476\n",
            "1477\n",
            "1478\n",
            "1479\n",
            "1480\n",
            "1481\n",
            "1482\n",
            "1483\n",
            "1484\n",
            "1485\n",
            "1486\n",
            "1487\n",
            "1488\n",
            "1489\n",
            "1490\n",
            "1491\n",
            "1492\n",
            "1493\n",
            "1494\n",
            "1495\n",
            "1496\n",
            "1497\n",
            "1498\n",
            "1499\n",
            "1500\n",
            "1501\n",
            "1502\n",
            "1503\n",
            "1504\n",
            "1505\n",
            "1506\n",
            "1507\n",
            "1508\n",
            "1509\n",
            "1510\n",
            "1511\n",
            "1512\n",
            "1513\n",
            "1514\n",
            "1515\n",
            "1516\n",
            "1517\n",
            "1518\n",
            "1519\n",
            "1520\n",
            "1521\n",
            "1522\n",
            "1523\n",
            "1524\n",
            "1525\n",
            "1526\n",
            "1527\n",
            "1528\n",
            "1529\n",
            "1530\n",
            "1531\n",
            "1532\n",
            "1533\n",
            "1534\n",
            "1535\n",
            "1536\n",
            "1537\n",
            "1538\n",
            "1539\n",
            "1540\n",
            "1541\n",
            "1542\n",
            "1543\n",
            "1544\n",
            "1545\n",
            "1546\n",
            "1547\n",
            "1548\n",
            "1549\n",
            "1550\n",
            "1551\n",
            "1552\n",
            "1553\n",
            "1554\n",
            "1555\n",
            "1556\n",
            "1557\n",
            "1558\n",
            "1559\n",
            "1560\n",
            "1561\n",
            "1562\n",
            "1563\n",
            "1564\n",
            "1565\n",
            "1566\n",
            "1567\n",
            "1568\n",
            "1569\n",
            "1570\n",
            "1571\n",
            "1572\n",
            "1573\n",
            "1574\n",
            "1575\n",
            "1576\n",
            "1577\n",
            "1578\n",
            "1579\n",
            "1580\n",
            "1581\n",
            "1582\n",
            "1583\n",
            "1584\n",
            "1585\n",
            "1586\n",
            "1587\n",
            "1588\n",
            "1589\n",
            "1590\n",
            "1591\n",
            "1592\n",
            "1593\n",
            "1594\n",
            "1595\n",
            "1596\n",
            "1597\n",
            "1598\n",
            "1599\n",
            "1600\n",
            "1601\n",
            "1602\n",
            "1603\n",
            "1604\n",
            "1605\n",
            "1606\n",
            "1607\n",
            "1608\n",
            "1609\n",
            "1610\n",
            "1611\n",
            "1612\n",
            "1613\n",
            "1614\n",
            "1615\n",
            "1616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JivWKbnZHY1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmatrices = np.array(cmatrices)\n",
        "cmatrices_copy = cmatrices.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72YZhg0p_HC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savez_compressed('ADR1B_unsorted_coulomb_matrix',cmatrices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Une1ZfRQIF2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d6e949e-82a7-43a1-986c-326f3fe61089"
      },
      "source": [
        "cmatrices_copy.shape"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1616, 122, 122)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8l1huYfsc0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Failed index = 559\n",
        "\n",
        "# Dropping failed index\n",
        "\n",
        "affinity_train_test = affinity_train_test.drop(affinity_train_test.index[failed_indices])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5xQr7Ztv8dF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27a235ba-a7fc-49e7-b674-b9411f83b7d3"
      },
      "source": [
        "affinity_train_test.shape"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1616,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0ijz_0uuke2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "827f1b30-15ba-475a-a0ab-367a8aecf239"
      },
      "source": [
        "# Need to drop\n",
        "\n",
        "affinity_train_test[affinity_train_test < 0.1]"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "885     0.000\n",
              "1831    0.023\n",
              "2076    0.050\n",
              "2081    0.050\n",
              "2209    0.080\n",
              "2511    0.040\n",
              "Name: Standard Value, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlmykAH2uki6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "541efcff-2d32-4c00-b0ec-d513acfae923"
      },
      "source": [
        "idx_to_drop = np.where(affinity_train_test.index==885)[0]; idx_to_drop"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([616])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLqVagENwRQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "affinity_train_test = affinity_train_test.drop(885)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nNN1zhwxD1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmatrices_copy = np.delete(cmatrices_copy, idx_to_drop, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4F73H3BxNfo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0490cfc-f013-4433-de52-fa0c5e3200c3"
      },
      "source": [
        "cmatrices_copy.shape"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1615, 122, 122)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo0o3Xkqsc6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will try to predict the log binding affinity\n",
        "\n",
        "log_affinity_train_test = np.log(affinity_train_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpE9nW-3ucVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "5b061f38-d176-4b2e-9f90-4ae22762bb85"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(log_affinity_train_test)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 10.,  11.,  37., 136., 165., 311., 370., 364., 154.,  57.]),\n",
              " array([-3.77226106, -2.14933642, -0.52641178,  1.09651286,  2.71943751,\n",
              "         4.34236215,  5.96528679,  7.58821143,  9.21113608, 10.83406072,\n",
              "        12.45698536]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxv0/9BDUXmejDxsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpQ7XNOqucaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86IrpmcQt4N4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_coulomb_matrices(cmatrices_copy):\n",
        "  ## Function to sort the training matrices\n",
        "  \n",
        "  # Get the number of training examples\n",
        "  n_training_ex = cmatrices_copy.shape[0]\n",
        "  \n",
        "  # Get the sorted indices\n",
        "  idxs = np.linalg.norm(cmatrices_copy, axis = 1).argsort()\n",
        "  \n",
        "  # Sort the matrix\n",
        "  for ex in range(n_training_ex):\n",
        "    cmatrices_copy[ex] = cmatrices_copy[ex][:,idxs[ex][::-1]][idxs[ex][::-1],:]\n",
        "    \n",
        "  return cmatrices_copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghGTwVcQt48S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cmatrices_copy = sort_coulomb_matrices(cmatrices_copy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA3mDzmzgl4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_flattened_NN_tanh_input(cmatrices_copy, num_thetas=1, theta=1):\n",
        "  ## As described in \n",
        "  ## https://pdfs.semanticscholar.org/5761/d22bb67798167a832879a473e0ece867df04.pdf\n",
        "  ## pp5.\n",
        "  \n",
        "  # Get batch size, and dimensions of matrix (rows should == cols)\n",
        "  batch_size, dim_rows, dim_cols = cmatrices_copy.shape\n",
        "  \n",
        "  # Assert nrows == ncols\n",
        "  assert dim_rows == dim_cols, 'Input matrix must be square'\n",
        "  \n",
        "  # Get size of new mini-vector\n",
        "  size = 2*num_thetas + 1\n",
        "  \n",
        "  # Generate new vector\n",
        "  arr = np.array([np.tanh(  (cmatrices_copy + theta * v)/theta  ) for v in range(-num_thetas, num_thetas + 1)  ])\n",
        "  \n",
        "  # Swap back the batches axis to the first axis\n",
        "  arr = np.transpose(arr, axes = (1,0,2,3))\n",
        "  \n",
        "  # Finally, flatten the array\n",
        "  arr = arr.reshape(batch_size, size*dim_rows*dim_cols)\n",
        "  \n",
        "  return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVHV621Zgl9R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92a667c5-d443-450d-d550-77e4629695b2"
      },
      "source": [
        "generate_flattened_NN_tanh_input(cmatrices_copy).shape"
      ],
      "execution_count": 862,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1616, 44652)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 862
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a78cg8jar5Mu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9689420d-6423-4ce3-95d1-0a720201e529"
      },
      "source": [
        "## Delete the label for the compounds that failed to get a coulomb matrix\n",
        "failed_indices\n",
        "modded_affinity_train_test = affinity_train_test.drop(labels = affinity_train_test.index[failed_indices])"
      ],
      "execution_count": 866,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[559]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 866
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_Ypx-0fr7j_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(cmatrices_copy,modded_affinity_train_test,random_state = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUu8CP75tn8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = keras.utils.to_categorical(np.array(y_train), num_classes)\n",
        "y_test = keras.utils.to_categorical(np.array(y_test), num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhY9HXUEPfMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_noisy_batch_of_coulomb_matrices(cmatrices_copy, sig=1):\n",
        "  ## Function to return a random batch of coulomb matrices, \n",
        "  ## given a batch of sorted/unsorted coulomb matrices.\n",
        "  \n",
        "  ## #As described in https://pdfs.semanticscholar.org/5761/d22bb67798167a832879a473e0ece867df04.pdf\n",
        "  \n",
        "  # Get the batch size\n",
        "  batch_size = cmatrices_copy.shape[0]\n",
        "  \n",
        "  # Get the dimension of the padded coulomb matrices\n",
        "  dim_C = cmatrices_copy.shape[1]\n",
        "  \n",
        "  # Get the rownorm of the tensor\n",
        "  rownorm = np.linalg.norm(cmatrices_copy,axis=1)\n",
        "  \n",
        "  # Generate some noise\n",
        "  noise = np.random.normal(0, sig, (batch_size,dim_C))\n",
        "  \n",
        "  # Add noise to row norm vector and sort\n",
        "  noisy_row_norm = rownorm + noise\n",
        "  noisy_idxs = noisy_row_norm.argsort()\n",
        "  \n",
        "  # Reshape the tensor according to the new indices\n",
        "  for ex in range(batch_size):\n",
        "    cmatrices_copy[ex] = cmatrices_copy[ex][:,noisy_idxs[ex][::-1]][noisy_idxs[ex][::-1],:]\n",
        "  \n",
        "  return cmatrices_copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzUwHUbjymxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, pickle, sys, copy, scipy, scipy.io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AHMy2mmym2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76R-LKrLyvem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 3453\n",
        "mb = 25\n",
        "hist = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aQYC8ijyviH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWN0wJhdyvko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = scipy.io.loadmat('/content/qm7.mat')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv5ZTJoZyvzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_coulomb_matrix(numbers, coords, alpha=1, use_decay=False):\n",
        "    \"\"\"\n",
        "    Generates the unsorted Coulomb-matrix, given that\n",
        "    \"\"\"\n",
        "    top = np.outer(numbers, numbers).astype(np.float64)\n",
        "    r = cdist(coords, coords)\n",
        "    if use_decay:\n",
        "        other = cdist([coords[0]], coords).reshape(-1)\n",
        "        r += numpy.add.outer(other, other)\n",
        "\n",
        "    r **= alpha\n",
        "\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        np.divide(top, r, top)\n",
        "    np.fill_diagonal(top, 0.5 * np.array(numbers) ** 2.4)\n",
        "    top[top == np.Infinity] = 0\n",
        "    top[np.isnan(top)] = 0\n",
        "    return top"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN-8z4bCyv2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "312b1529-44f5-4f26-bb99-7afb1ef77cc4"
      },
      "source": [
        "numbers = dataset['Z'];numbers.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7165, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDtL5Vn1ym6M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2018402e-14b1-42ec-be2c-3ed3486956c5"
      },
      "source": [
        "coords = dataset['R']; coords.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7165, 23, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYZzVsUe5D91",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "ec1af809-1156-4196-c137-bef0c748d51e"
      },
      "source": [
        "matlist = []\n",
        "for idx in range(7165):\n",
        "  if idx%500==0:\n",
        "    print(idx)\n",
        "  mat = get_coulomb_matrix(numbers[idx],coords[idx])\n",
        "  matlist.append(mat)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n",
            "3000\n",
            "3500\n",
            "4000\n",
            "4500\n",
            "5000\n",
            "5500\n",
            "6000\n",
            "6500\n",
            "7000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW82ZCIu5EO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matlist = np.array(matlist, dtype = np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxoBT2ax5ESX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cadd94c-78b0-4754-f7c9-7aee1fd28d55"
      },
      "source": [
        "matlist.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7165, 23, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICWCEpK9-OQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matlist = sort_coulomb_matrices(matlist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMQ4jrs28Fyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matlist = sort_coulomb_matrices(matlist)\n",
        "dataset['X'] = sort_coulomb_matrices(dataset['X'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjlEcvLeoqf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYRrv80M7vXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_coulomb_matrices(cmatrices_copy):\n",
        "  ## Function to sort the training matrices\n",
        "  \n",
        "  # Get the number of training examples\n",
        "  n_training_ex = cmatrices_copy.shape[0]\n",
        "  \n",
        "  # Get the sorted indices\n",
        "  idxs = np.linalg.norm(cmatrices_copy, axis = 2).argsort()\n",
        "  \n",
        "  # Sort the matrix\n",
        "  for ex in range(n_training_ex):\n",
        "    cmatrices_copy[ex] = cmatrices_copy[ex][:,idxs[ex][::-1]][idxs[ex][::-1],:]\n",
        "    \n",
        "  return cmatrices_copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LohvGK-g8oHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sortmat = dataset['X'].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRndmDJI8qiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sortmat = sort_coulomb_matrices(sortmat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCUFHted-u3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "82fa01ed-7ac1-49e3-f066-dc5518e24416"
      },
      "source": [
        "dataset['T'][0]"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -417.96,  -712.42,  -564.21, ..., -1662.1 , -1782.01, -1919.  ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7RGA_wh-u-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4kY_bMN-vBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Module:\n",
        "  def update(self, lr):pass\n",
        "  def average(self,nn,a):pass\n",
        "  def backward(self,DY):pass\n",
        "  def forward(self,X):pass\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm4AkqLs-vEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sequential(Module):\n",
        "  def __init__(self,modules):\n",
        "    self.modules = modules\n",
        "  \n",
        "  def forward(self,X):\n",
        "    for m in self.modules: X=m.forward(X)\n",
        "    return X\n",
        "  \n",
        "  def backward(self,DY):\n",
        "    for m in self.modules[::-1]: DY = m.backward(DY)\n",
        "    return DY\n",
        "  \n",
        "  def update(self,lr):\n",
        "    for m in self.modules: X = m.update(lr)\n",
        "  \n",
        "  def average(self,nn,a):\n",
        "    for m,n in zip(self.modules,nn.modules): m.average(n,a)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGGU21YUAxSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Input(Module):\n",
        "  \n",
        "  def __init__(self,X):\n",
        "    self.step=1.0\n",
        "    self.noise=1.0\n",
        "    self.triuind = (np.arange(122)[:,np.newaxis] <= np.arange(122)[np.newaxis,:]).flatten()\n",
        "    self.max = 0\n",
        "    for _ in range(10): \n",
        "      self.max=np.maximum(self.max,self.realize(X).max(axis=0))\n",
        "    X = self.expand(self.realize(X))\n",
        "    self.nbout = X.shape[1]\n",
        "    self.mean = X.mean(axis=0)\n",
        "    self.std = (X - self.mean).std()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "  def realize(self,X):\n",
        "    def _realize_(x):\n",
        "      inds = np.argsort(-(x**2).sum(axis=0)**.5+np.random.normal(0,self.noise,x[0].shape))\n",
        "      x = x[inds,:][:,inds]*1\n",
        "      x = x.flatten()[self.triuind]\n",
        "      return x\n",
        "    return np.array([_realize_(z) for z in X])\n",
        "      \n",
        "  def expand(self,X):\n",
        "    Xexp = []\n",
        "    for i in range(X.shape[1]):\n",
        "      for k in np.arange(0,self.max[i]+self.step,self.step):\n",
        "        Xexp += [np.tanh((X[:,i]-k)/self.step)]\n",
        "    return np.array(Xexp).T\n",
        "  \n",
        "  def normalize(self,X): return (X-self.mean)/self.std\n",
        "  \n",
        "  def forward(self,X): return self.normalize(self.expand(self.realize(X))).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJUOnu9n72U_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Output(Module):\n",
        "  \n",
        "  def __init__(self,T):\n",
        "    self.tmean = T.mean()\n",
        "    self.tstd = T.std()\n",
        "    self.nbinp = 1\n",
        "    \n",
        "  def forward(self,X):\n",
        "    return X*self.tstd + self.tmean\n",
        "  \n",
        "  def backward(self,DY):\n",
        "    return (DY/self.tstd).astype('float32')[:,np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIXtflp8FFnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(Module):\n",
        "  \n",
        "  def __init__(self,m,n):\n",
        "    \n",
        "    self.tr = m**.5 / n**.5\n",
        "    self.lr = 1/m**.5\n",
        "    \n",
        "    self.W = np.random.normal(0,1/m**.5,[m,n]).astype('float32')\n",
        "    self.A = np.zeros([m]).astype('float32')\n",
        "    self.B = np.zeros([n]).astype('float32')\n",
        "    \n",
        "  def forward(self,X):\n",
        "    self.X = X\n",
        "    Y = np.dot(X-self.A,self.W) + self.B\n",
        "    return Y\n",
        "  \n",
        "  def backward(self,DY):\n",
        "    self.DW = np.dot((self.X-self.A).T,DY)\n",
        "    self.DA = -(self.X-self.A).sum(axis=0)\n",
        "    self.DB = DY.sum(axis=0)+np.dot(self.DA,self.W)\n",
        "    DX = self.tr * np.dot(DY,self.W.T)\n",
        "    return DX\n",
        "    \n",
        "  def update(self,lr):\n",
        "    self.W -= lr*self.lr*self.DW\n",
        "    self.B -= lr*self.lr*self.DB\n",
        "    self.A -= lr*self.lr*self.DA\n",
        "  \n",
        "  def average(self,nn,a):\n",
        "    self.W = a*nn.W + (1-a)*self.W\n",
        "    self.B = a*nn.B + (1-a)*self.B\n",
        "    self.A = a*nn.A + (1-a)*self.A\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5_7MPsQ50WN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Linear without A\n",
        "class Linear(Module):\n",
        "  \n",
        "  def __init__(self,m,n):\n",
        "    \n",
        "    self.tr = m**.5 / n**.5\n",
        "    self.lr = 1/m**.5\n",
        "    \n",
        "    self.W = np.random.normal(0,1/m**.5,[m,n]).astype('float32')\n",
        "    self.B = np.zeros([n]).astype('float32')\n",
        "    \n",
        "  def forward(self,X):\n",
        "    self.X = X\n",
        "    Y = np.dot(X,self.W) + self.B\n",
        "    return Y\n",
        "  \n",
        "  def backward(self,DY):\n",
        "    self.DW = np.dot((self.X).T,DY)\n",
        "    self.DB = DY.sum(axis=0)\n",
        "    DX = self.tr * np.dot(DY,self.W.T)\n",
        "    return DX\n",
        "    \n",
        "  def update(self,lr):\n",
        "    self.W -= lr*self.lr*self.DW\n",
        "    self.B -= lr*self.lr*self.DB\n",
        "\n",
        "  \n",
        "  def average(self,nn,a):\n",
        "    self.W = a*nn.W + (1-a)*self.W\n",
        "    self.B = a*nn.B + (1-a)*self.B"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9wasSE1FFqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sigmoid(Module):\n",
        "  \n",
        "  def forward(self,X):\n",
        "    self.Y = np.tanh(X/1.5)\n",
        "    return 1.5*self.Y\n",
        "  \n",
        "  def backward(self,DY):\n",
        "    return DY*(1-self.Y**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1myknGe4I24",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52609024-d1cc-47ba-ed7a-cc4f85ba6983"
      },
      "source": [
        "P = dataset['P'][list(range(0,1)) + list(range(1+1,5))].flatten(); P.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5732,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8G9f2C-4I88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7d885cc-1c99-4383-b71a-dd26057dd482"
      },
      "source": [
        "X = matlist[P]; X.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5732, 23, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB78TO5H_F8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e4efffd-946d-4789-fc51-bb365d9b2b9f"
      },
      "source": [
        "Ptest = dataset['P'][1]; Ptest.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1433,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_psjnVZ-mmn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba694e18-c47f-47f6-fc69-d3df6e8d33d6"
      },
      "source": [
        "Xtest = matlist[Ptest]; Xtest.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1433, 23, 23)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPa7uZEU4JCK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e50edfe-767f-4c77-c322-16e93ae1d53f"
      },
      "source": [
        "T = dataset['T'][0,P]; T.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5732,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDpS4p2g_TQa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "55c64333-2ec8-440c-a3c4-5f8d32ae5bc0"
      },
      "source": [
        "Ttest = dataset['T'][0,Ptest]; Ttest.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1433,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjYmHjTk4I_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "I,O = Input(X),Output(T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbZhZtSR8fSl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f56d080c-1d6c-49c5-9b54-5fc2499135d8"
      },
      "source": [
        "O.nbinp"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5w4GDIbCI0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_trakn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRjM7HZYCI4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaoMdBHgCI8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ltp7re_7CJAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1MV7fa08EvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nnsgd = Sequential([\n",
        "    I,\n",
        "    Linear(I.nbout,400),\n",
        "    Sigmoid(),\n",
        "    Linear(400,100),\n",
        "    Sigmoid(),\n",
        "    Linear(100,O.nbinp),\n",
        "    O\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIpW--jj8Eyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Set the weights to the output initialized at 0\n",
        "nnsgd.modules[-2].W *= 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XvQundc8E4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nnavg = copy.deepcopy(nnsgd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtwardEs8E7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a17dd79-536e-42d9-9586-8b8ae6df54bc"
      },
      "source": [
        "for i in range(1,1000001):\n",
        "  \n",
        "  # LR schedule\n",
        "  if i>0:     lr = 0.001\n",
        "  if i>500:   lr = 0.0025\n",
        "  if i>2500:  lr = 0.005\n",
        "  if i>12500: lr = 0.01\n",
        "    \n",
        "  r = np.random.randint(0,len(X),[mb])\n",
        "  Y = nnsgd.forward(X[r])\n",
        "  nnsgd.backward(Y - T[r])\n",
        "  nnsgd.update(lr)\n",
        "  nnavg.average(nnsgd,(1/hist)/((1/hist)+i))\n",
        "  \n",
        "  if i%100==0:\n",
        "    print(i)\n",
        "    Y = np.array([nnsgd.forward(Xtest) for _ in range(10)]).mean(axis=0)\n",
        "    print('MAE: %5.2f kcal/mol'%np.abs(Y-Ttest).mean(axis=0))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "MAE: 101.90 kcal/mol\n",
            "200\n",
            "MAE: 67.95 kcal/mol\n",
            "300\n",
            "MAE: 55.52 kcal/mol\n",
            "400\n",
            "MAE: 46.36 kcal/mol\n",
            "500\n",
            "MAE: 41.40 kcal/mol\n",
            "600\n",
            "MAE: 34.63 kcal/mol\n",
            "700\n",
            "MAE: 31.57 kcal/mol\n",
            "800\n",
            "MAE: 29.74 kcal/mol\n",
            "900\n",
            "MAE: 29.05 kcal/mol\n",
            "1000\n",
            "MAE: 27.11 kcal/mol\n",
            "1100\n",
            "MAE: 26.23 kcal/mol\n",
            "1200\n",
            "MAE: 24.98 kcal/mol\n",
            "1300\n",
            "MAE: 24.89 kcal/mol\n",
            "1400\n",
            "MAE: 23.38 kcal/mol\n",
            "1500\n",
            "MAE: 22.88 kcal/mol\n",
            "1600\n",
            "MAE: 22.18 kcal/mol\n",
            "1700\n",
            "MAE: 22.37 kcal/mol\n",
            "1800\n",
            "MAE: 21.57 kcal/mol\n",
            "1900\n",
            "MAE: 21.08 kcal/mol\n",
            "2000\n",
            "MAE: 20.45 kcal/mol\n",
            "2100\n",
            "MAE: 20.80 kcal/mol\n",
            "2200\n",
            "MAE: 21.07 kcal/mol\n",
            "2300\n",
            "MAE: 19.98 kcal/mol\n",
            "2400\n",
            "MAE: 19.58 kcal/mol\n",
            "2500\n",
            "MAE: 19.60 kcal/mol\n",
            "2600\n",
            "MAE: 19.37 kcal/mol\n",
            "2700\n",
            "MAE: 19.26 kcal/mol\n",
            "2800\n",
            "MAE: 18.81 kcal/mol\n",
            "2900\n",
            "MAE: 18.39 kcal/mol\n",
            "3000\n",
            "MAE: 17.62 kcal/mol\n",
            "3100\n",
            "MAE: 17.69 kcal/mol\n",
            "3200\n",
            "MAE: 18.02 kcal/mol\n",
            "3300\n",
            "MAE: 17.27 kcal/mol\n",
            "3400\n",
            "MAE: 17.13 kcal/mol\n",
            "3500\n",
            "MAE: 17.04 kcal/mol\n",
            "3600\n",
            "MAE: 16.62 kcal/mol\n",
            "3700\n",
            "MAE: 16.54 kcal/mol\n",
            "3800\n",
            "MAE: 16.41 kcal/mol\n",
            "3900\n",
            "MAE: 16.48 kcal/mol\n",
            "4000\n",
            "MAE: 15.94 kcal/mol\n",
            "4100\n",
            "MAE: 16.02 kcal/mol\n",
            "4200\n",
            "MAE: 15.83 kcal/mol\n",
            "4300\n",
            "MAE: 15.42 kcal/mol\n",
            "4400\n",
            "MAE: 15.65 kcal/mol\n",
            "4500\n",
            "MAE: 15.56 kcal/mol\n",
            "4600\n",
            "MAE: 15.15 kcal/mol\n",
            "4700\n",
            "MAE: 15.39 kcal/mol\n",
            "4800\n",
            "MAE: 15.03 kcal/mol\n",
            "4900\n",
            "MAE: 14.69 kcal/mol\n",
            "5000\n",
            "MAE: 14.63 kcal/mol\n",
            "5100\n",
            "MAE: 15.35 kcal/mol\n",
            "5200\n",
            "MAE: 14.11 kcal/mol\n",
            "5300\n",
            "MAE: 14.15 kcal/mol\n",
            "5400\n",
            "MAE: 14.29 kcal/mol\n",
            "5500\n",
            "MAE: 14.66 kcal/mol\n",
            "5600\n",
            "MAE: 13.96 kcal/mol\n",
            "5700\n",
            "MAE: 14.19 kcal/mol\n",
            "5800\n",
            "MAE: 14.09 kcal/mol\n",
            "5900\n",
            "MAE: 14.15 kcal/mol\n",
            "6000\n",
            "MAE: 14.37 kcal/mol\n",
            "6100\n",
            "MAE: 13.90 kcal/mol\n",
            "6200\n",
            "MAE: 13.29 kcal/mol\n",
            "6300\n",
            "MAE: 13.62 kcal/mol\n",
            "6400\n",
            "MAE: 13.69 kcal/mol\n",
            "6500\n",
            "MAE: 13.50 kcal/mol\n",
            "6600\n",
            "MAE: 13.59 kcal/mol\n",
            "6700\n",
            "MAE: 13.55 kcal/mol\n",
            "6800\n",
            "MAE: 13.77 kcal/mol\n",
            "6900\n",
            "MAE: 13.05 kcal/mol\n",
            "7000\n",
            "MAE: 12.74 kcal/mol\n",
            "7100\n",
            "MAE: 12.93 kcal/mol\n",
            "7200\n",
            "MAE: 12.89 kcal/mol\n",
            "7300\n",
            "MAE: 13.05 kcal/mol\n",
            "7400\n",
            "MAE: 12.78 kcal/mol\n",
            "7500\n",
            "MAE: 12.83 kcal/mol\n",
            "7600\n",
            "MAE: 12.91 kcal/mol\n",
            "7700\n",
            "MAE: 12.94 kcal/mol\n",
            "7800\n",
            "MAE: 13.09 kcal/mol\n",
            "7900\n",
            "MAE: 12.54 kcal/mol\n",
            "8000\n",
            "MAE: 12.54 kcal/mol\n",
            "8100\n",
            "MAE: 12.53 kcal/mol\n",
            "8200\n",
            "MAE: 12.84 kcal/mol\n",
            "8300\n",
            "MAE: 12.49 kcal/mol\n",
            "8400\n",
            "MAE: 12.66 kcal/mol\n",
            "8500\n",
            "MAE: 12.52 kcal/mol\n",
            "8600\n",
            "MAE: 12.27 kcal/mol\n",
            "8700\n",
            "MAE: 12.25 kcal/mol\n",
            "8800\n",
            "MAE: 12.49 kcal/mol\n",
            "8900\n",
            "MAE: 12.23 kcal/mol\n",
            "9000\n",
            "MAE: 12.12 kcal/mol\n",
            "9100\n",
            "MAE: 11.97 kcal/mol\n",
            "9200\n",
            "MAE: 12.11 kcal/mol\n",
            "9300\n",
            "MAE: 11.94 kcal/mol\n",
            "9400\n",
            "MAE: 12.04 kcal/mol\n",
            "9500\n",
            "MAE: 12.16 kcal/mol\n",
            "9600\n",
            "MAE: 11.93 kcal/mol\n",
            "9700\n",
            "MAE: 11.75 kcal/mol\n",
            "9800\n",
            "MAE: 11.60 kcal/mol\n",
            "9900\n",
            "MAE: 12.03 kcal/mol\n",
            "10000\n",
            "MAE: 11.93 kcal/mol\n",
            "10100\n",
            "MAE: 11.94 kcal/mol\n",
            "10200\n",
            "MAE: 11.57 kcal/mol\n",
            "10300\n",
            "MAE: 11.58 kcal/mol\n",
            "10400\n",
            "MAE: 11.61 kcal/mol\n",
            "10500\n",
            "MAE: 11.69 kcal/mol\n",
            "10600\n",
            "MAE: 11.51 kcal/mol\n",
            "10700\n",
            "MAE: 11.79 kcal/mol\n",
            "10800\n",
            "MAE: 11.54 kcal/mol\n",
            "10900\n",
            "MAE: 11.53 kcal/mol\n",
            "11000\n",
            "MAE: 11.53 kcal/mol\n",
            "11100\n",
            "MAE: 11.55 kcal/mol\n",
            "11200\n",
            "MAE: 11.73 kcal/mol\n",
            "11300\n",
            "MAE: 11.42 kcal/mol\n",
            "11400\n",
            "MAE: 11.19 kcal/mol\n",
            "11500\n",
            "MAE: 11.26 kcal/mol\n",
            "11600\n",
            "MAE: 11.48 kcal/mol\n",
            "11700\n",
            "MAE: 11.11 kcal/mol\n",
            "11800\n",
            "MAE: 11.29 kcal/mol\n",
            "11900\n",
            "MAE: 11.18 kcal/mol\n",
            "12000\n",
            "MAE: 11.25 kcal/mol\n",
            "12100\n",
            "MAE: 11.27 kcal/mol\n",
            "12200\n",
            "MAE: 11.24 kcal/mol\n",
            "12300\n",
            "MAE: 11.07 kcal/mol\n",
            "12400\n",
            "MAE: 11.27 kcal/mol\n",
            "12500\n",
            "MAE: 10.98 kcal/mol\n",
            "12600\n",
            "MAE: 11.34 kcal/mol\n",
            "12700\n",
            "MAE: 11.13 kcal/mol\n",
            "12800\n",
            "MAE: 11.15 kcal/mol\n",
            "12900\n",
            "MAE: 11.25 kcal/mol\n",
            "13000\n",
            "MAE: 10.85 kcal/mol\n",
            "13100\n",
            "MAE: 11.21 kcal/mol\n",
            "13200\n",
            "MAE: 10.95 kcal/mol\n",
            "13300\n",
            "MAE: 10.97 kcal/mol\n",
            "13400\n",
            "MAE: 10.97 kcal/mol\n",
            "13500\n",
            "MAE: 10.89 kcal/mol\n",
            "13600\n",
            "MAE: 10.90 kcal/mol\n",
            "13700\n",
            "MAE: 11.05 kcal/mol\n",
            "13800\n",
            "MAE: 10.68 kcal/mol\n",
            "13900\n",
            "MAE: 10.81 kcal/mol\n",
            "14000\n",
            "MAE: 10.77 kcal/mol\n",
            "14100\n",
            "MAE: 10.53 kcal/mol\n",
            "14200\n",
            "MAE: 10.52 kcal/mol\n",
            "14300\n",
            "MAE: 10.69 kcal/mol\n",
            "14400\n",
            "MAE: 10.65 kcal/mol\n",
            "14500\n",
            "MAE: 10.52 kcal/mol\n",
            "14600\n",
            "MAE: 10.40 kcal/mol\n",
            "14700\n",
            "MAE: 10.67 kcal/mol\n",
            "14800\n",
            "MAE: 10.53 kcal/mol\n",
            "14900\n",
            "MAE: 10.56 kcal/mol\n",
            "15000\n",
            "MAE: 10.25 kcal/mol\n",
            "15100\n",
            "MAE: 10.63 kcal/mol\n",
            "15200\n",
            "MAE: 10.80 kcal/mol\n",
            "15300\n",
            "MAE: 10.83 kcal/mol\n",
            "15400\n",
            "MAE: 10.76 kcal/mol\n",
            "15500\n",
            "MAE: 10.75 kcal/mol\n",
            "15600\n",
            "MAE: 10.59 kcal/mol\n",
            "15700\n",
            "MAE: 10.10 kcal/mol\n",
            "15800\n",
            "MAE: 10.01 kcal/mol\n",
            "15900\n",
            "MAE: 10.63 kcal/mol\n",
            "16000\n",
            "MAE:  9.96 kcal/mol\n",
            "16100\n",
            "MAE: 10.73 kcal/mol\n",
            "16200\n",
            "MAE: 10.04 kcal/mol\n",
            "16300\n",
            "MAE: 10.09 kcal/mol\n",
            "16400\n",
            "MAE:  9.94 kcal/mol\n",
            "16500\n",
            "MAE:  9.97 kcal/mol\n",
            "16600\n",
            "MAE: 10.19 kcal/mol\n",
            "16700\n",
            "MAE: 10.41 kcal/mol\n",
            "16800\n",
            "MAE: 10.07 kcal/mol\n",
            "16900\n",
            "MAE:  9.83 kcal/mol\n",
            "17000\n",
            "MAE:  9.83 kcal/mol\n",
            "17100\n",
            "MAE: 10.19 kcal/mol\n",
            "17200\n",
            "MAE: 10.00 kcal/mol\n",
            "17300\n",
            "MAE:  9.81 kcal/mol\n",
            "17400\n",
            "MAE:  9.67 kcal/mol\n",
            "17500\n",
            "MAE:  9.79 kcal/mol\n",
            "17600\n",
            "MAE:  9.36 kcal/mol\n",
            "17700\n",
            "MAE:  9.63 kcal/mol\n",
            "17800\n",
            "MAE:  9.99 kcal/mol\n",
            "17900\n",
            "MAE:  9.77 kcal/mol\n",
            "18000\n",
            "MAE:  9.76 kcal/mol\n",
            "18100\n",
            "MAE:  9.56 kcal/mol\n",
            "18200\n",
            "MAE:  9.95 kcal/mol\n",
            "18300\n",
            "MAE:  9.59 kcal/mol\n",
            "18400\n",
            "MAE:  9.48 kcal/mol\n",
            "18500\n",
            "MAE:  9.55 kcal/mol\n",
            "18600\n",
            "MAE:  9.51 kcal/mol\n",
            "18700\n",
            "MAE:  9.51 kcal/mol\n",
            "18800\n",
            "MAE:  9.75 kcal/mol\n",
            "18900\n",
            "MAE: 10.02 kcal/mol\n",
            "19000\n",
            "MAE:  9.79 kcal/mol\n",
            "19100\n",
            "MAE:  9.43 kcal/mol\n",
            "19200\n",
            "MAE:  9.56 kcal/mol\n",
            "19300\n",
            "MAE:  9.33 kcal/mol\n",
            "19400\n",
            "MAE:  9.41 kcal/mol\n",
            "19500\n",
            "MAE:  9.45 kcal/mol\n",
            "19600\n",
            "MAE:  9.50 kcal/mol\n",
            "19700\n",
            "MAE:  9.43 kcal/mol\n",
            "19800\n",
            "MAE:  9.44 kcal/mol\n",
            "19900\n",
            "MAE:  9.58 kcal/mol\n",
            "20000\n",
            "MAE:  9.08 kcal/mol\n",
            "20100\n",
            "MAE:  9.58 kcal/mol\n",
            "20200\n",
            "MAE:  9.06 kcal/mol\n",
            "20300\n",
            "MAE:  9.87 kcal/mol\n",
            "20400\n",
            "MAE:  9.45 kcal/mol\n",
            "20500\n",
            "MAE:  9.48 kcal/mol\n",
            "20600\n",
            "MAE:  9.25 kcal/mol\n",
            "20700\n",
            "MAE:  9.47 kcal/mol\n",
            "20800\n",
            "MAE:  9.30 kcal/mol\n",
            "20900\n",
            "MAE:  9.19 kcal/mol\n",
            "21000\n",
            "MAE:  9.36 kcal/mol\n",
            "21100\n",
            "MAE:  9.20 kcal/mol\n",
            "21200\n",
            "MAE:  9.59 kcal/mol\n",
            "21300\n",
            "MAE:  9.48 kcal/mol\n",
            "21400\n",
            "MAE:  9.15 kcal/mol\n",
            "21500\n",
            "MAE:  9.26 kcal/mol\n",
            "21600\n",
            "MAE:  9.47 kcal/mol\n",
            "21700\n",
            "MAE:  9.09 kcal/mol\n",
            "21800\n",
            "MAE:  9.09 kcal/mol\n",
            "21900\n",
            "MAE:  9.22 kcal/mol\n",
            "22000\n",
            "MAE:  9.07 kcal/mol\n",
            "22100\n",
            "MAE:  9.18 kcal/mol\n",
            "22200\n",
            "MAE:  8.89 kcal/mol\n",
            "22300\n",
            "MAE:  9.13 kcal/mol\n",
            "22400\n",
            "MAE:  8.83 kcal/mol\n",
            "22500\n",
            "MAE:  8.84 kcal/mol\n",
            "22600\n",
            "MAE:  9.18 kcal/mol\n",
            "22700\n",
            "MAE:  9.24 kcal/mol\n",
            "22800\n",
            "MAE:  9.07 kcal/mol\n",
            "22900\n",
            "MAE:  9.36 kcal/mol\n",
            "23000\n",
            "MAE:  9.22 kcal/mol\n",
            "23100\n",
            "MAE:  9.12 kcal/mol\n",
            "23200\n",
            "MAE:  8.74 kcal/mol\n",
            "23300\n",
            "MAE:  9.10 kcal/mol\n",
            "23400\n",
            "MAE:  9.11 kcal/mol\n",
            "23500\n",
            "MAE:  8.73 kcal/mol\n",
            "23600\n",
            "MAE:  8.78 kcal/mol\n",
            "23700\n",
            "MAE:  9.06 kcal/mol\n",
            "23800\n",
            "MAE:  8.94 kcal/mol\n",
            "23900\n",
            "MAE:  9.50 kcal/mol\n",
            "24000\n",
            "MAE:  8.88 kcal/mol\n",
            "24100\n",
            "MAE:  8.81 kcal/mol\n",
            "24200\n",
            "MAE:  8.82 kcal/mol\n",
            "24300\n",
            "MAE:  8.86 kcal/mol\n",
            "24400\n",
            "MAE:  8.82 kcal/mol\n",
            "24500\n",
            "MAE:  8.90 kcal/mol\n",
            "24600\n",
            "MAE:  8.72 kcal/mol\n",
            "24700\n",
            "MAE:  8.75 kcal/mol\n",
            "24800\n",
            "MAE:  9.13 kcal/mol\n",
            "24900\n",
            "MAE:  8.97 kcal/mol\n",
            "25000\n",
            "MAE:  8.79 kcal/mol\n",
            "25100\n",
            "MAE:  8.79 kcal/mol\n",
            "25200\n",
            "MAE:  8.81 kcal/mol\n",
            "25300\n",
            "MAE:  8.59 kcal/mol\n",
            "25400\n",
            "MAE:  8.73 kcal/mol\n",
            "25500\n",
            "MAE:  8.70 kcal/mol\n",
            "25600\n",
            "MAE:  8.69 kcal/mol\n",
            "25700\n",
            "MAE:  8.73 kcal/mol\n",
            "25800\n",
            "MAE:  8.83 kcal/mol\n",
            "25900\n",
            "MAE:  8.50 kcal/mol\n",
            "26000\n",
            "MAE:  8.66 kcal/mol\n",
            "26100\n",
            "MAE:  8.88 kcal/mol\n",
            "26200\n",
            "MAE:  8.47 kcal/mol\n",
            "26300\n",
            "MAE:  8.52 kcal/mol\n",
            "26400\n",
            "MAE:  8.58 kcal/mol\n",
            "26500\n",
            "MAE:  8.75 kcal/mol\n",
            "26600\n",
            "MAE:  8.58 kcal/mol\n",
            "26700\n",
            "MAE:  8.77 kcal/mol\n",
            "26800\n",
            "MAE:  8.65 kcal/mol\n",
            "26900\n",
            "MAE:  8.48 kcal/mol\n",
            "27000\n",
            "MAE:  8.55 kcal/mol\n",
            "27100\n",
            "MAE:  8.42 kcal/mol\n",
            "27200\n",
            "MAE:  8.66 kcal/mol\n",
            "27300\n",
            "MAE:  8.52 kcal/mol\n",
            "27400\n",
            "MAE:  8.66 kcal/mol\n",
            "27500\n",
            "MAE:  8.40 kcal/mol\n",
            "27600\n",
            "MAE:  8.55 kcal/mol\n",
            "27700\n",
            "MAE:  8.38 kcal/mol\n",
            "27800\n",
            "MAE:  8.67 kcal/mol\n",
            "27900\n",
            "MAE:  8.50 kcal/mol\n",
            "28000\n",
            "MAE:  8.16 kcal/mol\n",
            "28100\n",
            "MAE:  8.36 kcal/mol\n",
            "28200\n",
            "MAE:  8.88 kcal/mol\n",
            "28300\n",
            "MAE:  8.44 kcal/mol\n",
            "28400\n",
            "MAE:  8.35 kcal/mol\n",
            "28500\n",
            "MAE:  8.38 kcal/mol\n",
            "28600\n",
            "MAE:  8.41 kcal/mol\n",
            "28700\n",
            "MAE:  8.73 kcal/mol\n",
            "28800\n",
            "MAE:  8.51 kcal/mol\n",
            "28900\n",
            "MAE:  8.45 kcal/mol\n",
            "29000\n",
            "MAE:  8.34 kcal/mol\n",
            "29100\n",
            "MAE:  8.28 kcal/mol\n",
            "29200\n",
            "MAE:  8.71 kcal/mol\n",
            "29300\n",
            "MAE:  8.30 kcal/mol\n",
            "29400\n",
            "MAE:  8.57 kcal/mol\n",
            "29500\n",
            "MAE:  8.33 kcal/mol\n",
            "29600\n",
            "MAE:  8.51 kcal/mol\n",
            "29700\n",
            "MAE:  8.18 kcal/mol\n",
            "29800\n",
            "MAE:  8.12 kcal/mol\n",
            "29900\n",
            "MAE:  8.32 kcal/mol\n",
            "30000\n",
            "MAE:  8.45 kcal/mol\n",
            "30100\n",
            "MAE:  8.27 kcal/mol\n",
            "30200\n",
            "MAE:  8.57 kcal/mol\n",
            "30300\n",
            "MAE:  8.25 kcal/mol\n",
            "30400\n",
            "MAE:  8.39 kcal/mol\n",
            "30500\n",
            "MAE:  8.66 kcal/mol\n",
            "30600\n",
            "MAE:  8.31 kcal/mol\n",
            "30700\n",
            "MAE:  8.90 kcal/mol\n",
            "30800\n",
            "MAE:  8.31 kcal/mol\n",
            "30900\n",
            "MAE:  8.88 kcal/mol\n",
            "31000\n",
            "MAE:  8.30 kcal/mol\n",
            "31100\n",
            "MAE:  8.02 kcal/mol\n",
            "31200\n",
            "MAE:  9.01 kcal/mol\n",
            "31300\n",
            "MAE:  8.21 kcal/mol\n",
            "31400\n",
            "MAE:  8.29 kcal/mol\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-39335c365f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnsgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mnnsgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mnnsgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-8e2b45184506>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-542978d81edf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-542978d81edf>\u001b[0m in \u001b[0;36mexpand\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mXexp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b647O8w_xwsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(cmatrices_copy, np.array(log_affinity_train_test), random_state = 101)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8gaCOjfx5Dw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4e0cb72-7fc0-48c8-e184-b25910d5f217"
      },
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1211, 122, 122), (404, 122, 122), (1211,), (404,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KigMvz9xx5Ht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPDz2q-WT4Rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q8UggBbbMoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def randomize(x,y):\n",
        "  '''\n",
        "  Randomizes the order of data samples and corresponding labels\n",
        "  '''\n",
        "  perm = np.random.permutation(y.shape[0])\n",
        "  shuffled_x = x[perm]\n",
        "  shuffled_y = y[perm]\n",
        "  return shuffled_x, shuffled_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsKHvrOwbMrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_next_batch(x,y,start,end):\n",
        "  x_batch = x[start:end]\n",
        "  y_batch = y[start:end]\n",
        "  return x_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOs52gwnbMyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "init=tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJYeB_CaYQBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper-parameters\n",
        "epochs = 1000             # Total number of training epochs\n",
        "batch_size = 25        # Training batch size\n",
        "display_freq = 20      # Frequency of displaying the training results\n",
        "learning_rate = 0.0001   # The optimization initial learning rate\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If8YuKVU8R1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weight_variable(name, shape):\n",
        "  initer = tf.truncated_normal_initializer(stddev=(1/(shape[0])**.5))\n",
        "  return tf.get_variable('W_'+name,\n",
        "                        dtype=tf.float32,\n",
        "                        shape=shape,\n",
        "                        initializer=initer)\n",
        "\n",
        "def bias_variable(name,shape):\n",
        "  initial = tf.constant(0.,shape=shape,dtype=tf.float32)\n",
        "  return tf.get_variable('b_'+name,\n",
        "                        dtype=tf.float32,\n",
        "                        initializer=initial)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXgvSjARU8As",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc_layer(x,num_units,name,use_sigmoid=True):\n",
        "  in_dim=x.get_shape().as_list()[1]\n",
        "  W=weight_variable(name,shape=[in_dim,num_units])\n",
        "  b=bias_variable(name,[num_units])\n",
        "  layer=tf.matmul(x,W)\n",
        "  layer+=b\n",
        "  if use_sigmoid:\n",
        "    layer=tf.nn.tanh(layer)\n",
        "  return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_R91RE7Kxud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "I,O = Input(X_train),Output(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCYDVKzmyVpL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33d8eaf6-c6ee-41c9-ead8-d5da51f65008"
      },
      "source": [
        "I.nbout"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "391629"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ltNIIFWV0zD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flattened_input_size = I.nbout\n",
        "n_classes = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXLJ7YzbVRJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder(tf.float32,shape=[None,flattened_input_size],name='X')\n",
        "y = tf.placeholder(tf.float32,shape=[None,n_classes],name='Y')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_jK2USiVozy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computational Graph\n",
        "\n",
        "fc1 = fc_layer(x,400,'FC1',use_sigmoid=True)\n",
        "fc2 = fc_layer(fc1,100,'FC2',use_sigmoid=True)\n",
        "pre_preds = fc_layer(fc2,n_classes,'OUT',use_sigmoid=False)\n",
        "preds = O.forward(pre_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0p6gvUkWfxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Network predictions\n",
        "#cls_predictions = tf.argmax(output_logits, axis=1, name='predictions')\n",
        "\n",
        "regression_predictions = preds\n",
        "\n",
        "loss = tf.losses.mean_squared_error(labels=y,predictions=preds)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,name='Adam-op').minimize(loss)\n",
        "#correct_prediction = tf.equal(tf.argmax(output_logits,1),tf.argmax(y,1),name='correct_pred')\n",
        "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw3sdg6eXEYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init=tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsv6oyrtGGwO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6fa3c2d7-39cd-49f0-9653-82e176bfc692"
      },
      "source": [
        "I.forward(X_train).shape"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1211, 391629)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VNaUQQH_QoX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9db7b424-5906-409d-abf5-4c158ee5b2f2"
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(init)"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[391629,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node W_FC1/Adam-op/Initializer/zeros}}, {{node W_FC1/Adam-op_1/Initializer/zeros}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-252-60fd287262b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[391629,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node W_FC1/Adam-op/Initializer/zeros (defined at <ipython-input-247-a9867891e253>:6) , node W_FC1/Adam-op_1/Initializer/zeros (defined at <ipython-input-247-a9867891e253>:6) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nOriginal stack trace for 'W_FC1/Adam-op/Initializer/zeros':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-247-a9867891e253>\", line 6, in <module>\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,name='Adam-op').minimize(loss)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 413, in minimize\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 597, in apply_gradients\n    self._create_slots(var_list)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adam.py\", line 131, in _create_slots\n    self._zeros_slot(v, \"m\", self._name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\", line 1155, in _zeros_slot\n    new_slot_variable = slot_creator.create_zeros_slot(var, op_name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/slot_creator.py\", line 190, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/slot_creator.py\", line 164, in create_slot_with_initializer\n    dtype)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/slot_creator.py\", line 74, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1496, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1239, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 562, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 514, in _true_getter\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 929, in _get_single_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2511, in default_variable_creator\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1568, in __init__\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1698, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 901, in <lambda>\n    partition_info=partition_info)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py\", line 114, in __call__\n    return array_ops.zeros(shape, dtype)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 1883, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3613, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AhwcyQ3YfSc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb151629-3892-4395-a0aa-b55ea6b7e022"
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(init)\n",
        "global_step=0\n",
        "\n",
        "\n",
        "num_tr_iter = int(len(X_test) / batch_size)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('Training epoch:{}'.format(epoch+1))\n",
        "  #X_train, y_train = randomize(X_train, y_train)\n",
        "  \n",
        "  X_train, y_train = randomize(X_train,y_train)\n",
        "  \n",
        "  for iteration in range(num_tr_iter):\n",
        "    global_step+=1\n",
        "    \n",
        "    start = iteration*batch_size\n",
        "    end = (iteration+1)*batch_size\n",
        "    \n",
        "    \n",
        "    \n",
        "    x_batch, y_batch = get_next_batch(X_train,y_train,start,end)\n",
        "    \n",
        "    \n",
        "    print(x_batch.shape)\n",
        "    \n",
        "    x_batch = I.forward(x_batch)\n",
        "    \n",
        "    y_batch = y_batch[:,np.newaxis]\n",
        "    \n",
        "    \n",
        "    # Generate noisy coulomb batch\n",
        "    #x_batch = return_noisy_batch_of_coulomb_matrices(x_batch,sig = 10)\n",
        "    \n",
        "    # Convert into the binary-like flattened input\n",
        "    #x_batch = generate_flattened_NN_tanh_input(x_batch,theta = 5)\n",
        "    \n",
        "    \n",
        "    # Run optimization operation\n",
        "    feed_dict_batch = {x:x_batch, y:y_batch}\n",
        "    sess.run(optimizer,feed_dict=feed_dict_batch)\n",
        "    \n",
        "    if iteration%display_freq==0:\n",
        "      # Calculate and display batch loss and accuracy\n",
        "      loss_batch = sess.run(loss,feed_dict={x:x_batch,y:y_batch})\n",
        "      \n",
        "      print(\"iter {0:3d}:\\t Loss={1:.2f}\".\n",
        "                  format(iteration, loss_batch))\n",
        "  \n",
        "  save_path = saver.save(sess, \"model.ckpt\")\n",
        "  print(\"Model saved in path: %s\" % save_path)\n",
        "  #X_test_validation = generate_flattened_NN_tanh_input(X_test, theta=5)\n",
        "  \n",
        "  #feed_dict_valid = {x:X_test_validation[:1000],y:y_test[:1000]}\n",
        "  #loss_valid, acc_valid = sess.run([loss,accuracy],feed_dict=feed_dict_valid)\n",
        "  #print('------------------------------')\n",
        "  #print('Epoch:{0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}'.\n",
        "       #format(epoch+1, loss_valid, acc_valid))\n",
        "  #print('------------------------------')"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[391629,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node W_FC1/Initializer/truncated_normal/TruncatedNormal}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-250-82e8c9bf3473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[391629,400] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node W_FC1/Initializer/truncated_normal/TruncatedNormal (defined at <ipython-input-240-1f37471b3e57>:6) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nOriginal stack trace for 'W_FC1/Initializer/truncated_normal/TruncatedNormal':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-246-e4448353eba4>\", line 2, in <module>\n    fc1 = fc_layer(x,400,'FC1',use_sigmoid=True)\n  File \"<ipython-input-241-a1858f06b38c>\", line 3, in fc_layer\n    W=weight_variable(name,shape=[in_dim,num_units])\n  File \"<ipython-input-240-1f37471b3e57>\", line 6, in weight_variable\n    initializer=initer)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1496, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 1239, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 562, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 514, in _true_getter\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 929, in _get_single_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 259, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 220, in _variable_v1_call\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 198, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2511, in default_variable_creator\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 263, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1568, in __init__\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1698, in _init_from_args\n    initial_value(), name=\"initial_value\", dtype=dtype)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 901, in <lambda>\n    partition_info=partition_info)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py\", line 369, in __call__\n    shape, self.mean, self.stddev, dtype, seed=self.seed)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\", line 178, in truncated_normal\n    shape_tensor, dtype, seed=seed1, seed2=seed2)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_random_ops.py\", line 1013, in truncated_normal\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU2z0ZyAb4Bw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "outputId": "c79f69eb-a0db-4d64-f7e8-94e0a5a9bf6c"
      },
      "source": [
        "modxtest = I.forward(X_test)\n",
        "modytest = y_test.copy()[:,np.newaxis]\n",
        "print(modxtest.shape,modytest.shape)\n",
        "feed_dict_test = {x: modxtest, y:modytest}\n",
        "preds = sess.run(preds, feed_dict = feed_dict_test)"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(404, 391638) (404, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: 2 root error(s) found.\n  (0) Internal: Dst tensor is not initialized.\n\t [[{{node _arg_X_0_0}}]]\n\t [[add_3/_3]]\n  (1) Internal: Dst tensor is not initialized.\n\t [[{{node _arg_X_0_0}}]]\n0 successful operations.\n0 derived errors ignored.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-234-ae767e5d943c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodxtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeed_dict_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmodytest\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: 2 root error(s) found.\n  (0) Internal: Dst tensor is not initialized.\n\t [[{{node _arg_X_0_0}}]]\n\t [[add_3/_3]]\n  (1) Internal: Dst tensor is not initialized.\n\t [[{{node _arg_X_0_0}}]]\n0 successful operations.\n0 derived errors ignored."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2ngybXQ7sgy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d17a178-2c1f-4aaa-81f2-16455e4215b0"
      },
      "source": [
        "np.exp(5)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "148.4131591025766"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5il8WxSEe52z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f037bb9a-e44a-42b9-fb04-5ee113e4d581"
      },
      "source": [
        "preds.reshape(-1).shape"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(404,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2On-Shu2gQQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fbd55e1-b6cf-4dac-dc83-4e1175a75a03"
      },
      "source": [
        "np.array([preds.reshape(-1),modytest.reshape(-1)]).T.shape"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(404, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMSytNAqdhPq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "011ef0dd-2fcd-49ce-ac0e-cf12f72d56c4"
      },
      "source": [
        "import pandas as pd\n",
        "results_df = pd.DataFrame(np.array([preds.reshape(-1),modytest.reshape(-1)]).T, columns = ['preds','true']); results_df"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>true</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.372783</td>\n",
              "      <td>3.784190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.101027</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.732010</td>\n",
              "      <td>4.941642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.804065</td>\n",
              "      <td>6.363028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.511439</td>\n",
              "      <td>2.532903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.655371</td>\n",
              "      <td>4.143135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6.132413</td>\n",
              "      <td>6.677499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>9.044476</td>\n",
              "      <td>6.286054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7.479804</td>\n",
              "      <td>8.612503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4.035995</td>\n",
              "      <td>1.098612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>5.196323</td>\n",
              "      <td>6.907755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>6.345572</td>\n",
              "      <td>2.532903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>8.093293</td>\n",
              "      <td>6.907755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>4.442065</td>\n",
              "      <td>6.907755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>7.740282</td>\n",
              "      <td>7.863267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>5.970960</td>\n",
              "      <td>6.345636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>7.474404</td>\n",
              "      <td>10.341742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2.486828</td>\n",
              "      <td>2.993229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3.649376</td>\n",
              "      <td>3.737670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>8.155022</td>\n",
              "      <td>6.907755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5.909678</td>\n",
              "      <td>8.006368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>6.858993</td>\n",
              "      <td>6.887553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>5.858840</td>\n",
              "      <td>6.907755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8.711058</td>\n",
              "      <td>7.377759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>6.153400</td>\n",
              "      <td>5.590987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>7.335690</td>\n",
              "      <td>9.903488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>7.128744</td>\n",
              "      <td>7.090077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>4.775855</td>\n",
              "      <td>3.401197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>7.505648</td>\n",
              "      <td>6.917706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>5.168908</td>\n",
              "      <td>5.799093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>5.381680</td>\n",
              "      <td>4.382027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>6.746229</td>\n",
              "      <td>3.433987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>9.040214</td>\n",
              "      <td>9.210340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>8.824965</td>\n",
              "      <td>7.244228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>4.625823</td>\n",
              "      <td>0.405465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>6.003041</td>\n",
              "      <td>5.560682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>5.365905</td>\n",
              "      <td>3.610918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>5.641319</td>\n",
              "      <td>6.907755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>8.291870</td>\n",
              "      <td>9.903488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>8.205085</td>\n",
              "      <td>5.010635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>8.319072</td>\n",
              "      <td>8.411833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>6.198665</td>\n",
              "      <td>-0.023064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>6.493032</td>\n",
              "      <td>5.480639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387</th>\n",
              "      <td>3.388750</td>\n",
              "      <td>1.458615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>4.545517</td>\n",
              "      <td>1.609438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>6.678812</td>\n",
              "      <td>9.210340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>6.796292</td>\n",
              "      <td>11.512925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>5.070835</td>\n",
              "      <td>0.483660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>8.428443</td>\n",
              "      <td>2.072291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>4.156914</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>6.399063</td>\n",
              "      <td>9.024011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>6.371811</td>\n",
              "      <td>-1.386294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>5.236112</td>\n",
              "      <td>6.677499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>4.500448</td>\n",
              "      <td>4.941642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>8.594692</td>\n",
              "      <td>2.261763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>4.342445</td>\n",
              "      <td>8.418477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>6.533992</td>\n",
              "      <td>4.605170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>6.461045</td>\n",
              "      <td>9.903488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>3.586594</td>\n",
              "      <td>1.504077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>4.215933</td>\n",
              "      <td>4.490096</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>404 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        preds       true\n",
              "0    4.372783   3.784190\n",
              "1    3.101027   4.605170\n",
              "2    5.732010   4.941642\n",
              "3    5.804065   6.363028\n",
              "4    6.511439   2.532903\n",
              "5    5.655371   4.143135\n",
              "6    6.132413   6.677499\n",
              "7    9.044476   6.286054\n",
              "8    7.479804   8.612503\n",
              "9    4.035995   1.098612\n",
              "10   5.196323   6.907755\n",
              "11   6.345572   2.532903\n",
              "12   8.093293   6.907755\n",
              "13   4.442065   6.907755\n",
              "14   7.740282   7.863267\n",
              "15   5.970960   6.345636\n",
              "16   7.474404  10.341742\n",
              "17   2.486828   2.993229\n",
              "18   3.649376   3.737670\n",
              "19   8.155022   6.907755\n",
              "20   5.909678   8.006368\n",
              "21   6.858993   6.887553\n",
              "22   5.858840   6.907755\n",
              "23   8.711058   7.377759\n",
              "24   6.153400   5.590987\n",
              "25   7.335690   9.903488\n",
              "26   7.128744   7.090077\n",
              "27   4.775855   3.401197\n",
              "28   7.505648   6.917706\n",
              "29   5.168908   5.799093\n",
              "..        ...        ...\n",
              "374  5.381680   4.382027\n",
              "375  6.746229   3.433987\n",
              "376  9.040214   9.210340\n",
              "377  8.824965   7.244228\n",
              "378  4.625823   0.405465\n",
              "379  6.003041   5.560682\n",
              "380  5.365905   3.610918\n",
              "381  5.641319   6.907755\n",
              "382  8.291870   9.903488\n",
              "383  8.205085   5.010635\n",
              "384  8.319072   8.411833\n",
              "385  6.198665  -0.023064\n",
              "386  6.493032   5.480639\n",
              "387  3.388750   1.458615\n",
              "388  4.545517   1.609438\n",
              "389  6.678812   9.210340\n",
              "390  6.796292  11.512925\n",
              "391  5.070835   0.483660\n",
              "392  8.428443   2.072291\n",
              "393  4.156914   4.605170\n",
              "394  6.399063   9.024011\n",
              "395  6.371811  -1.386294\n",
              "396  5.236112   6.677499\n",
              "397  4.500448   4.941642\n",
              "398  8.594692   2.261763\n",
              "399  4.342445   8.418477\n",
              "400  6.533992   4.605170\n",
              "401  6.461045   9.903488\n",
              "402  3.586594   1.504077\n",
              "403  4.215933   4.490096\n",
              "\n",
              "[404 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8FQFp4A3E2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = np.array(results_df['preds'])\n",
        "true = np.array(results_df['true'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk6Ke6Cb4Z6K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "73451df5-f19b-41f7-d710-f95a823e2d24"
      },
      "source": [
        "thresh = 7\n",
        "preds_bin = np.array([1 if x < thresh else 0 for x in preds])\n",
        "true_bin = np.array([1 if x < thresh else 0 for x in true])\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "confusion_matrix(true_bin,preds_bin)"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 87,  68],\n",
              "       [ 30, 219]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-nxX6gz4tKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "8b35d0d0-5aea-4177-b966-2ecdd9819ee4"
      },
      "source": [
        "print(classification_report(true_bin,preds_bin))"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.56      0.64       155\n",
            "           1       0.76      0.88      0.82       249\n",
            "\n",
            "    accuracy                           0.76       404\n",
            "   macro avg       0.75      0.72      0.73       404\n",
            "weighted avg       0.76      0.76      0.75       404\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0LnAvqh4-rL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "34183a50-3b32-4b18-ad2e-12a00526fb7e"
      },
      "source": [
        ""
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-200-f73ee080cfb5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ~\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWAUVjYR5ZHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}