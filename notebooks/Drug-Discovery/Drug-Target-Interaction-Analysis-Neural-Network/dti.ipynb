{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, Draw\n",
    "\n",
    "from chembl_webresource_client.new_client import new_client\n",
    "from chembl_webresource_client.utils import utils\n",
    "\n",
    "import numpy as np\n",
    "import json as json\n",
    "import csv\n",
    "import requests\n",
    "import xml.etree.cElementTree as et\n",
    "\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "import random\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request Raw Data From ChEMBL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You may need to wait for more than an hour, please be paitent :-)\n",
      "Thanks for the waiting, now you can access the activity data in the dataframe named 'frame'\n"
     ]
    }
   ],
   "source": [
    "activities = new_client.activity\n",
    "drugs = new_client.drug\n",
    "targets = new_client.target\n",
    "FOLDER = 'data\\\\'\n",
    "\n",
    "dic = activities[:500]\n",
    "print('You may need to wait for more than an hour, please be paitent :-)')\n",
    "frame = pd.DataFrame(dic)\n",
    "print('Thanks for the waiting, now you can access the activity data in the dataframe named \\'frame\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invaild comment data removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\28297\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "D:\\Users\\28297\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invaild uniprot data removed\n",
      "The positive data set includes 237 pieces of data\n"
     ]
    }
   ],
   "source": [
    "def remove_invalid_comment(data):\n",
    "    valid_rows = data['data_validity_comment'].isnull()\n",
    "    df = data[valid_rows]\n",
    "    return df\n",
    "\n",
    "def remove_non_uniprots(data):\n",
    "    data['mark_to_remove'] = False\n",
    "    for i in range(data.shape[0]):\n",
    "        target_id = data['target_chembl_id'][i]\n",
    "        targets1 = new_client.target.filter(target_chembl_id__in=target_id) \n",
    "        uniprots = []\n",
    "        uniprots = sum([[comp['accession'] for comp in t['target_components']] for t in targets1],[])\n",
    "        if(len(uniprots)==0 or uniprots[0] is None):\n",
    "            data['mark_to_remove'][i] = True\n",
    "    valid_rows = data['mark_to_remove'] == False\n",
    "    df = data[valid_rows]\n",
    "    return df\n",
    "\n",
    "def remove_non_smile(data):\n",
    "    data['mark_to_remove'] = False\n",
    "    for i in range(data.shape[0]):\n",
    "        if(data['canonical_smiles'][i] is None):\n",
    "            data['mark_to_remove'][i] = True\n",
    "    valid_rows = data['mark_to_remove'] == False\n",
    "    df = data[valid_rows]\n",
    "    return df\n",
    "\n",
    "def clean_dataset(data):\n",
    "    data = remove_invalid_comment(data)\n",
    "    print('invaild comment data removed')\n",
    "    data = data.set_index(np.arange(0,data.shape[0]))\n",
    "    data = remove_non_smile(data)\n",
    "    data = data.set_index(np.arange(0,data.shape[0]))\n",
    "    data = remove_non_uniprots(data)\n",
    "    print('Invaild uniprot data removed')\n",
    "    return data\n",
    "\n",
    "df_clean = clean_dataset(frame)\n",
    "df_clean = df_clean.set_index(np.arange(0,df_clean.shape[0]))\n",
    "print('The positive data set includes %s pieces of data' %(df_clean.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly select target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\28297\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 negative target data has been selected\n",
      "20 negative target data has been selected\n",
      "30 negative target data has been selected\n",
      "40 negative target data has been selected\n",
      "50 negative target data has been selected\n",
      "60 negative target data has been selected\n",
      "70 negative target data has been selected\n",
      "80 negative target data has been selected\n",
      "90 negative target data has been selected\n",
      "100 negative target data has been selected\n",
      "110 negative target data has been selected\n",
      "120 negative target data has been selected\n",
      "130 negative target data has been selected\n",
      "140 negative target data has been selected\n",
      "150 negative target data has been selected\n",
      "160 negative target data has been selected\n",
      "170 negative target data has been selected\n",
      "180 negative target data has been selected\n",
      "190 negative target data has been selected\n",
      "200 negative target data has been selected\n",
      "210 negative target data has been selected\n",
      "220 negative target data has been selected\n",
      "230 negative target data has been selected\n",
      "240 negative target data has been selected\n",
      "250 negative target data has been selected\n",
      "260 negative target data has been selected\n",
      "270 negative target data has been selected\n",
      "280 negative target data has been selected\n",
      "290 negative target data has been selected\n",
      "300 negative target data has been selected\n",
      "310 negative target data has been selected\n",
      "320 negative target data has been selected\n",
      "330 negative target data has been selected\n",
      "340 negative target data has been selected\n",
      "350 negative target data has been selected\n",
      "The negative target data has been saved in the dataframe named'negative_target_df'\n"
     ]
    }
   ],
   "source": [
    "# get random target data\n",
    "target_list = []\n",
    "for i in range(500):\n",
    "    num = random.randint(0,len(targets))\n",
    "    # get a random target id\n",
    "    ctid = targets[num]['target_chembl_id']\n",
    "    target_list.append(ctid)\n",
    "    negative_target_df = pd.DataFrame(target_list, columns=['target_chembl_id'])\n",
    "\n",
    "    negative_target_df = remove_non_uniprots(negative_target_df)\n",
    "    negative_target_df = negative_target_df.set_index(np.arange(0,negative_target_df.shape[0]))\n",
    "    if((i+1)%10 == 0):\n",
    "        print('%s negative target data has been selected' %((i+1)))\n",
    "    if(negative_target_df.shape[0] == df_clean.shape[0]):\n",
    "        break\n",
    "\n",
    "negative_target_df = negative_target_df.drop(['mark_to_remove'],axis = 1)\n",
    "print('The negative target data has been saved in the dataframe named\\'negative_target_df\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Selection for molecule data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 negative molecule data has been selected\n",
      "20 negative molecule data has been selected\n",
      "30 negative molecule data has been selected\n",
      "40 negative molecule data has been selected\n",
      "50 negative molecule data has been selected\n",
      "60 negative molecule data has been selected\n",
      "70 negative molecule data has been selected\n",
      "80 negative molecule data has been selected\n",
      "90 negative molecule data has been selected\n",
      "100 negative molecule data has been selected\n",
      "110 negative molecule data has been selected\n",
      "120 negative molecule data has been selected\n",
      "130 negative molecule data has been selected\n",
      "140 negative molecule data has been selected\n",
      "150 negative molecule data has been selected\n",
      "160 negative molecule data has been selected\n",
      "170 negative molecule data has been selected\n",
      "180 negative molecule data has been selected\n",
      "190 negative molecule data has been selected\n",
      "200 negative molecule data has been selected\n",
      "210 negative molecule data has been selected\n",
      "220 negative molecule data has been selected\n",
      "230 negative molecule data has been selected\n",
      "The negative target data has been saved in the dataframe named'negative_molecule_df'\n"
     ]
    }
   ],
   "source": [
    "# molecule data\n",
    "from chembl_webresource_client.new_client import new_client\n",
    "molecules = new_client.molecule\n",
    "\n",
    "df_molecules = pd.DataFrame(molecules[:500])\n",
    "\n",
    "# make sure the data is negative\n",
    "def data_is_negative(molecule_id,target_id):\n",
    "    activities = new_client.activity.filter(molecule_chembl_id__in=molecule_id)\n",
    "    data_negativity = True\n",
    "    for dic in activities:\n",
    "        if(dic['target_chembl_id'] == target_id):\n",
    "            data_negativity = False\n",
    "            break\n",
    "    return data_negativity\n",
    "\n",
    "# # see if the selected molecules has a SMILE\n",
    "def smile_exist(mole):\n",
    "    smile_existence = None\n",
    "    \n",
    "    if(mole['molecule_structures'] == None):\n",
    "        smile_existence = False\n",
    "    elif('canonical_smiles' in mole['molecule_structures'].keys()):\n",
    "        smile_existence = True\n",
    "    else:\n",
    "        smile_existence = False\n",
    "    return smile_existence\n",
    "\n",
    "def structure_exist(mole):\n",
    "    existence = None\n",
    "    if('molecule_structures' in mole.keys()):\n",
    "        existence = smile_exist(mole)\n",
    "    else:\n",
    "        existence = False\n",
    "    \n",
    "    return existence\n",
    "\n",
    "def get_mol_data(num):\n",
    "    cdid = molecules[num]['molecule_chembl_id']\n",
    "    ctid = negative_target_df['target_chembl_id'][i]\n",
    "    data_negativity = data_is_negative(cdid,ctid)\n",
    "    existence = structure_exist(molecules[num])\n",
    "    return cdid,ctid,data_negativity,existence\n",
    "\n",
    "# get random drug data\n",
    "drug_list = []\n",
    "for i in range(500):\n",
    "    num = random.randint(0,len(molecules))\n",
    "    if(molecules[num] is None):\n",
    "        num = 0\n",
    "    cdid,ctid,data_negativity,existence = get_mol_data(num)\n",
    "    \n",
    "    while((data_negativity == False) or (existence == False)):\n",
    "        new_num = random.randint(0,len(molecules))\n",
    "        cdid,ctid,data_negativity,existence = get_mol_data(new_num)\n",
    "        num = new_num\n",
    "        \n",
    "    structure = molecules[num]['molecule_structures']\n",
    "    smile = structure['canonical_smiles']\n",
    "    dic = {'molecule_chembl_id':cdid,'smile':smile}\n",
    "    drug_list.append(dic)\n",
    "    if((i+1)%10 == 0):\n",
    "        print('%s negative molecule data has been selected' %((i+1)))\n",
    "    if(len(drug_list) == df_clean.shape[0]):\n",
    "        break\n",
    "    \n",
    "negative_molecule_df = pd.DataFrame(drug_list)\n",
    "print('The negative target data has been saved in the dataframe named\\'negative_molecule_df\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request Uniprot ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 pieces of data have been retrieved\n",
      "20 pieces of data have been retrieved\n",
      "30 pieces of data have been retrieved\n",
      "40 pieces of data have been retrieved\n",
      "50 pieces of data have been retrieved\n",
      "60 pieces of data have been retrieved\n",
      "70 pieces of data have been retrieved\n",
      "80 pieces of data have been retrieved\n",
      "90 pieces of data have been retrieved\n",
      "100 pieces of data have been retrieved\n",
      "110 pieces of data have been retrieved\n",
      "120 pieces of data have been retrieved\n",
      "130 pieces of data have been retrieved\n",
      "140 pieces of data have been retrieved\n",
      "150 pieces of data have been retrieved\n",
      "160 pieces of data have been retrieved\n",
      "170 pieces of data have been retrieved\n",
      "180 pieces of data have been retrieved\n",
      "190 pieces of data have been retrieved\n",
      "200 pieces of data have been retrieved\n",
      "210 pieces of data have been retrieved\n",
      "220 pieces of data have been retrieved\n",
      "230 pieces of data have been retrieved\n",
      "The uniprot id file can be found as 'uniprot_id.txt' in the folder 'data'\n"
     ]
    }
   ],
   "source": [
    "def get_uniprot_id(target_id):\n",
    "    targets = new_client.target.filter(target_chembl_id__in=target_id) \n",
    "    uniprots = []\n",
    "    uniprots = sum([[comp['accession'] for comp in t['target_components']] for t in targets],[])\n",
    "    uniprot_id = uniprots[0]\n",
    "    return uniprot_id\n",
    "\n",
    "def get_all_uniprot_id(data,file_name = 'uniprot_id.txt'):\n",
    "    targets = new_client.target\n",
    "    target_list = []\n",
    "    dumped_id = []\n",
    "    for i in range(0, data.shape[0]):\n",
    "        if((i+1)%10 == 0):\n",
    "            print('%s pieces of data have been retrieved' %((i+1)))\n",
    "        target_id = data['target_chembl_id'][i]\n",
    "        uniprot_id = get_uniprot_id(target_id)\n",
    "        target_list.append(uniprot_id)\n",
    "    df_tmp = pd.DataFrame(target_list, columns = ['uniprot_id'])\n",
    "    # write the IDs into a txt file\n",
    "    df_tmp.to_csv(FOLDER + file_name, sep='\\t', index=False, header = None)\n",
    "    print('The uniprot id file can be found as \\'%s\\' in the folder \\'data\\'' %(file_name))\n",
    "get_all_uniprot_id(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target chembl id file can be found as 'target_chembl_id.txt' in the folder 'data'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\28297\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# retrieve the target data\n",
    "df_tmp = df_clean['target_chembl_id']\n",
    "df_tmp.to_csv(FOLDER + 'target_chembl_id.txt', sep='\\t', index=False, header = None)\n",
    "print('The target chembl id file can be found as \\'%s\\' in the folder \\'data\\'' %('target_chembl_id.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access protein sequences from Uniprot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 sequences have been extracted\n",
      "20 sequences have been extracted\n",
      "30 sequences have been extracted\n",
      "40 sequences have been extracted\n",
      "50 sequences have been extracted\n",
      "60 sequences have been extracted\n",
      "70 sequences have been extracted\n",
      "80 sequences have been extracted\n",
      "90 sequences have been extracted\n",
      "100 sequences have been extracted\n",
      "110 sequences have been extracted\n",
      "120 sequences have been extracted\n",
      "130 sequences have been extracted\n",
      "140 sequences have been extracted\n",
      "150 sequences have been extracted\n",
      "160 sequences have been extracted\n",
      "170 sequences have been extracted\n",
      "180 sequences have been extracted\n",
      "190 sequences have been extracted\n",
      "200 sequences have been extracted\n",
      "210 sequences have been extracted\n",
      "220 sequences have been extracted\n",
      "230 sequences have been extracted\n",
      "The protein information is saved in the dataframe named 'protein_df'\n"
     ]
    }
   ],
   "source": [
    "# get file from uniprot\n",
    "def get_files(resource):\n",
    "    # Uniprot webservice\n",
    "    sequence_file = requests.get('http://www.uniprot.org/uniprot/' + resource)\n",
    "\n",
    "    # If response is empty\n",
    "    if len(sequence_file.text) == 0:\n",
    "        print 'not available in .xml or does not exist'\n",
    "        sequence_file = requests.get('http://www.uniprot.org/uniprot/' + 'A0ZX81.xml')\n",
    "        with open(FOLDER + resource, \"wb\") as file_name:\n",
    "            [file_name.write(line + '\\n') for line in sequence_file.iter_lines()]\n",
    "        \n",
    "\n",
    "    # http not 200\n",
    "    elif sequence_file.status_code != 200:\n",
    "        print('http error ' + str(sequence_file.status_code))\n",
    "        sequence_file = requests.get('http://www.uniprot.org/uniprot/' + 'A0ZX81.xml')\n",
    "        with open(FOLDER + resource, \"wb\") as file_name:\n",
    "            [file_name.write(line + '\\n') for line in sequence_file.iter_lines()]\n",
    "\n",
    "    # If response is html, then it's invalid\n",
    "    else:\n",
    "        html = False\n",
    "        for line in sequence_file.iter_lines():\n",
    "            if '<!DOCTYPE html' in line:\n",
    "                print 'not available in .' + output_format + ' or does not exist'\n",
    "                html = True\n",
    "\n",
    "        with open(FOLDER + resource, \"wb\") as file_name:\n",
    "            [file_name.write(line + '\\n') for line in sequence_file.iter_lines()]\n",
    "\n",
    "# get one protein sequence\n",
    "def get_protein_seq(file_name):\n",
    "    tree = et.ElementTree(file = FOLDER + file_name)\n",
    "    root = tree.getroot()\n",
    "    for child in root:\n",
    "        for grchild in child:\n",
    "            if grchild.tag == '{http://uniprot.org/uniprot}sequence':\n",
    "                seq = grchild.text.strip()\n",
    "                return seq\n",
    "\n",
    "# get all protein sequences\n",
    "def get_all_protseq(file_name,target_chembl_id = 'target_chembl_id.txt'):\n",
    "    id_list = []\n",
    "    with open(FOLDER + file_name, 'r') as f:\n",
    "        data = csv.reader(f)\n",
    "        for row in f:\n",
    "            id_list.append(row.strip())\n",
    "        \n",
    "    seq_list = []\n",
    "    count = 1\n",
    "    for i in id_list:\n",
    "        resource = i + '.xml'\n",
    "        get_files(resource)\n",
    "        seq = get_protein_seq(resource)\n",
    "        seq_list.append(seq.replace('\\n',''))\n",
    "        if(count%10 == 0):\n",
    "            print('%s sequences have been extracted' %(count))\n",
    "        count+=1\n",
    "    chembl_id_list = []\n",
    "    with open(FOLDER + target_chembl_id,'r') as f1:\n",
    "        data1 = csv.reader(f1)\n",
    "        for row in f1:\n",
    "            chembl_id_list.append(row.strip())\n",
    "    tmp = {'uniprot_id':id_list,\n",
    "          'sequence':seq_list,\n",
    "          'target_id':chembl_id_list}\n",
    "    df = pd.DataFrame(tmp)\n",
    "    return df\n",
    "\n",
    "protein_df = get_all_protseq('uniprot_id.txt')\n",
    "print('The protein information is saved in the dataframe named \\'protein_df\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Morgan Fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fingerprints have been calculated\n",
      "20 fingerprints have been calculated\n",
      "30 fingerprints have been calculated\n",
      "40 fingerprints have been calculated\n",
      "50 fingerprints have been calculated\n",
      "60 fingerprints have been calculated\n",
      "70 fingerprints have been calculated\n",
      "80 fingerprints have been calculated\n",
      "90 fingerprints have been calculated\n",
      "100 fingerprints have been calculated\n",
      "110 fingerprints have been calculated\n",
      "120 fingerprints have been calculated\n",
      "130 fingerprints have been calculated\n",
      "140 fingerprints have been calculated\n",
      "150 fingerprints have been calculated\n",
      "160 fingerprints have been calculated\n",
      "170 fingerprints have been calculated\n",
      "180 fingerprints have been calculated\n",
      "190 fingerprints have been calculated\n",
      "200 fingerprints have been calculated\n",
      "210 fingerprints have been calculated\n",
      "220 fingerprints have been calculated\n",
      "230 fingerprints have been calculated\n",
      "The computation of fingerprints is done\n"
     ]
    }
   ],
   "source": [
    "# get molecule id and smiles\n",
    "data = {'molecule_chembl_id':df_clean['molecule_chembl_id'],\n",
    "       'smile':df_clean['canonical_smiles']}\n",
    "molecule_id_df = pd.DataFrame(data)\n",
    "\n",
    "# compute morgan fingerprint\n",
    "def morgan_fingerprint(smile,radius, n_bits):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    vect = AllChem.GetHashedMorganFingerprint(mol = mol, radius = radius, nBits = n_bits)\n",
    "    vect = vect.GetNonzeroElements()\n",
    "    vect_keys = list(vect.keys()) # get the non-zero keys\n",
    "    vect_values = list(vect.values()) # get the non-zero values\n",
    "    \n",
    "    vect_dense = np.zeros(shape = (n_bits,))\n",
    "    vect_dense[vect_keys] = vect_values\n",
    "    fingerprint = ''\n",
    "    for i in vect_dense:\n",
    "        fingerprint += str(int(i))\n",
    "    return fingerprint\n",
    "\n",
    "# compute all morgan fingerprint\n",
    "def all_morgan_fingerprint(data):\n",
    "    df_list = []\n",
    "    for i in range(data.shape[0]):\n",
    "        fingerprint = morgan_fingerprint(data['smile'][i],2,300)\n",
    "        df_list.append(fingerprint)\n",
    "        if((i+1)%10 == 0):\n",
    "            print('%s fingerprints have been calculated' %((i+1)))\n",
    "    df = pd.DataFrame(df_list, columns = ['morgan_fingerprint'])\n",
    "    df_new = pd.concat([data,df],axis = 1)\n",
    "    return df_new \n",
    "\n",
    "molecule_df = all_morgan_fingerprint(molecule_id_df)\n",
    "print('The computation of fingerprints is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The positive features are saved in the file 'positive_features.csv'\n"
     ]
    }
   ],
   "source": [
    "positive_data = pd.concat([molecule_df,protein_df],axis = 1)\n",
    "positive_data.to_csv(FOLDER+'positive_features.csv')\n",
    "print('The positive features are saved in the file \\'positive_features.csv\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 pieces of data have been retrieved\n",
      "20 pieces of data have been retrieved\n",
      "30 pieces of data have been retrieved\n",
      "40 pieces of data have been retrieved\n",
      "50 pieces of data have been retrieved\n",
      "60 pieces of data have been retrieved\n",
      "70 pieces of data have been retrieved\n",
      "80 pieces of data have been retrieved\n",
      "90 pieces of data have been retrieved\n",
      "100 pieces of data have been retrieved\n",
      "110 pieces of data have been retrieved\n",
      "120 pieces of data have been retrieved\n",
      "130 pieces of data have been retrieved\n",
      "140 pieces of data have been retrieved\n",
      "150 pieces of data have been retrieved\n",
      "160 pieces of data have been retrieved\n",
      "170 pieces of data have been retrieved\n",
      "180 pieces of data have been retrieved\n",
      "190 pieces of data have been retrieved\n",
      "200 pieces of data have been retrieved\n",
      "210 pieces of data have been retrieved\n",
      "220 pieces of data have been retrieved\n",
      "230 pieces of data have been retrieved\n",
      "The uniprot id file can be found as 'negative_uniprot_id.txt' in the folder 'data'\n",
      "The negative target data has been save as 'negative_target_chembl.txt'in the folder 'data'\n",
      "10 sequences have been extracted\n",
      "20 sequences have been extracted\n",
      "30 sequences have been extracted\n",
      "40 sequences have been extracted\n",
      "50 sequences have been extracted\n",
      "60 sequences have been extracted\n",
      "70 sequences have been extracted\n",
      "80 sequences have been extracted\n",
      "http error 404\n",
      "90 sequences have been extracted\n",
      "100 sequences have been extracted\n",
      "110 sequences have been extracted\n",
      "120 sequences have been extracted\n",
      "130 sequences have been extracted\n",
      "140 sequences have been extracted\n",
      "150 sequences have been extracted\n",
      "160 sequences have been extracted\n",
      "not available in .xml or does not exist\n",
      "170 sequences have been extracted\n",
      "180 sequences have been extracted\n",
      "190 sequences have been extracted\n",
      "200 sequences have been extracted\n",
      "210 sequences have been extracted\n",
      "220 sequences have been extracted\n",
      "230 sequences have been extracted\n"
     ]
    }
   ],
   "source": [
    "negative_target_df = negative_target_df.set_index(np.arange(0,negative_target_df.shape[0]))\n",
    "negative_target_df.to_csv(FOLDER + 'negative_target_chembl_id.txt', sep='\\t', index=False, header = None)\n",
    "get_all_uniprot_id(negative_target_df,'negative_uniprot_id.txt')\n",
    "negative_target_df.to_csv(FOLDER + 'negative_target_chembl_id.txt', sep='\\t', index=False, header = None)\n",
    "print('The negative target data has been save as \\'negative_target_chembl.txt\\'in the folder \\'data\\'')\n",
    "# get protein sequence dataframe\n",
    "negative_protseq_df = get_all_protseq('negative_uniprot_id.txt','negative_target_chembl_id.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Molecule Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fingerprints have been calculated\n",
      "20 fingerprints have been calculated\n",
      "30 fingerprints have been calculated\n",
      "40 fingerprints have been calculated\n",
      "50 fingerprints have been calculated\n",
      "60 fingerprints have been calculated\n",
      "70 fingerprints have been calculated\n",
      "80 fingerprints have been calculated\n",
      "90 fingerprints have been calculated\n",
      "100 fingerprints have been calculated\n",
      "110 fingerprints have been calculated\n",
      "120 fingerprints have been calculated\n",
      "130 fingerprints have been calculated\n",
      "140 fingerprints have been calculated\n",
      "150 fingerprints have been calculated\n",
      "160 fingerprints have been calculated\n",
      "170 fingerprints have been calculated\n",
      "180 fingerprints have been calculated\n",
      "190 fingerprints have been calculated\n",
      "200 fingerprints have been calculated\n",
      "210 fingerprints have been calculated\n",
      "220 fingerprints have been calculated\n",
      "230 fingerprints have been calculated\n",
      "The positive features are saved in the file 'negative_features.csv'\n"
     ]
    }
   ],
   "source": [
    "# compute molecule morgan fingerprint\n",
    "negative_drug_df = all_morgan_fingerprint(negative_molecule_df)\n",
    "negative_data = pd.concat([negative_drug_df,negative_protseq_df],axis = 1)\n",
    "negative_data.to_csv(FOLDER + 'negative_features.csv')\n",
    "print('The positive features are saved in the file \\'negative_features.csv\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amino Acids Compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file\n",
    "positive_d = pd.read_csv(FOLDER+'positive_features.csv')\n",
    "negative_d = pd.read_csv(FOLDER+'negative_features.csv')\n",
    "\n",
    "positive_d['y'] = 1\n",
    "negative_d['y'] = 0\n",
    "\n",
    "data = pd.concat([positive_d,negative_d],axis = 0)\n",
    "data = data.drop(['Unnamed: 0'], axis = 1)\n",
    "data = data.set_index(np.arange(0,data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct bigram\n",
    "def construct_bigram(seq): # amino acids composition\n",
    "    a='ACDEFGHIKLMNPQRSTVWY'\n",
    "    b=[]\n",
    "    for i in range(0,len(a)):\n",
    "        for j in range(0,len(a)):\n",
    "            b.append(a[i]+a[j])\n",
    "\n",
    "    bi_dict=dict((letter,seq.count(letter)) for letter in set(b))\n",
    "    bi=list(bi_dict.values())\n",
    "    return bi\n",
    "\n",
    "def protein_features(seq):\n",
    "    sequence = []\n",
    "    analysed_seq = ProteinAnalysis(seq)\n",
    "    sequence.append(seq)\n",
    "    \n",
    "    unigram = analysed_seq.count_amino_acids() # count single amino acid\n",
    "    uni = list(unigram.values())\n",
    "    \n",
    "    # construct bigram\n",
    "    bi = construct_bigram(seq) \n",
    "    \n",
    "    iso=analysed_seq.isoelectric_point() # count iso\n",
    "\n",
    "    X_list=[0]*421\n",
    "    X_list[0] = iso\n",
    "    X_list[1:21]=uni[:]\n",
    "    X_list[21:421]=bi[:]\n",
    "\n",
    "    return X_list\n",
    "\n",
    "def compute_all_features(data,features):\n",
    "    data_list = []\n",
    "    for i in range(data.shape[0]):\n",
    "        seq = data['sequence'][i]\n",
    "        protf = protein_features(seq)\n",
    "        data_list.append(protf)\n",
    "        if((i+1)%10 == 0):\n",
    "            print('%s protein features have been calculated' %((i+1)))\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df_new = pd.concat([features,df],axis = 1)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 protein features have been calculated\n",
      "20 protein features have been calculated\n",
      "30 protein features have been calculated\n",
      "40 protein features have been calculated\n",
      "50 protein features have been calculated\n",
      "60 protein features have been calculated\n",
      "70 protein features have been calculated\n",
      "80 protein features have been calculated\n",
      "90 protein features have been calculated\n",
      "100 protein features have been calculated\n",
      "110 protein features have been calculated\n",
      "120 protein features have been calculated\n",
      "130 protein features have been calculated\n",
      "140 protein features have been calculated\n",
      "150 protein features have been calculated\n",
      "160 protein features have been calculated\n",
      "170 protein features have been calculated\n",
      "180 protein features have been calculated\n",
      "190 protein features have been calculated\n",
      "200 protein features have been calculated\n",
      "210 protein features have been calculated\n",
      "220 protein features have been calculated\n",
      "230 protein features have been calculated\n",
      "240 protein features have been calculated\n",
      "250 protein features have been calculated\n",
      "260 protein features have been calculated\n",
      "270 protein features have been calculated\n",
      "280 protein features have been calculated\n",
      "290 protein features have been calculated\n",
      "300 protein features have been calculated\n",
      "310 protein features have been calculated\n",
      "320 protein features have been calculated\n",
      "330 protein features have been calculated\n",
      "340 protein features have been calculated\n",
      "350 protein features have been calculated\n",
      "360 protein features have been calculated\n",
      "370 protein features have been calculated\n",
      "380 protein features have been calculated\n",
      "390 protein features have been calculated\n",
      "400 protein features have been calculated\n",
      "410 protein features have been calculated\n",
      "420 protein features have been calculated\n",
      "430 protein features have been calculated\n",
      "440 protein features have been calculated\n",
      "450 protein features have been calculated\n",
      "460 protein features have been calculated\n",
      "470 protein features have been calculated\n",
      "The feature data has been saved into file named 'feature_data.csv'\n"
     ]
    }
   ],
   "source": [
    "feature_data = pd.DataFrame()\n",
    "feature_data = compute_all_features(data,feature_data)\n",
    "feature_data['y'] = data['y']\n",
    "feature_data = feature_data.dropna(axis=1,how='any')\n",
    "\n",
    "feature_data.to_csv(FOLDER + 'feature_data.csv')\n",
    "print('The feature data has been saved into file named \\'feature_data.csv\\'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = pd.read_csv(FOLDER+'feature_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(feature_data, test_size=0.2, random_state=0)\n",
    "X_train = trainset.drop(['y'], axis = 1)\n",
    "y_train = trainset['y']\n",
    "X_test = testset.drop(['y'], axis = 1)\n",
    "y_test = testset['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(500, 20), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=236, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(500,20), max_iter=i)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1\n",
      " 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1\n",
      " 0 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
