{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "11_Learning_Unsupervised_Embeddings_for_Molecules.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzpae9-r2aoK",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial Part 11: Learning Unsupervised Embeddings for Molecules\n",
        "\n",
        "\n",
        "In this example, we will use a `SeqToSeq` model to generate fingerprints for classifying molecules.  This is based on the following paper, although some of the implementation details are different: Xu et al., \"Seq2seq Fingerprint: An Unsupervised Deep Molecular Embedding for Drug Discovery\" (https://doi.org/10.1145/3107411.3107424).\n",
        "\n",
        "Many types of models require their inputs to have a fixed shape.  Since molecules can vary widely in the numbers of atoms and bonds they contain, this makes it hard to apply those models to them.  We need a way of generating a fixed length \"fingerprint\" for each molecule.  Various ways of doing this have been designed, such as Extended-Connectivity Fingerprints (ECFPs).  But in this example, instead of designing a fingerprint by hand, we will let a `SeqToSeq` model learn its own method of creating fingerprints.\n",
        "\n",
        "A `SeqToSeq` model performs sequence to sequence translation.  For example, they are often used to translate text from one language to another.  It consists of two parts called the \"encoder\" and \"decoder\".  The encoder is a stack of recurrent layers.  The input sequence is fed into it, one token at a time, and it generates a fixed length vector called the \"embedding vector\".  The decoder is another stack of recurrent layers that performs the inverse operation: it takes the embedding vector as input, and generates the output sequence.  By training it on appropriately chosen input/output pairs, you can create a model that performs many sorts of transformations.\n",
        "\n",
        "In this case, we will use SMILES strings describing molecules as the input sequences.  We will train the model as an autoencoder, so it tries to make the output sequences identical to the input sequences.  For that to work, the encoder must create embedding vectors that contain all information from the original sequence.  That's exactly what we want in a fingerprint, so perhaps those embedding vectors will then be useful as a way to represent molecules in other models!\n",
        "\n",
        "\n",
        "## Colab\n",
        "\n",
        "This tutorial and the rest in this sequence are designed to be done in Google colab. If you'd like to open this notebook in colab, you can use the following link.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepchem/deepchem/blob/master/examples/tutorials/11_Learning_Unsupervised_Embeddings_for_Molecules.ipynb)\n",
        "\n",
        "## Setup\n",
        "\n",
        "To run DeepChem within Colab, you'll need to run the following cell of installation commands. This will take about 5 minutes to run to completion and install your environment. This notebook will take a few hours to run on a GPU machine, so we encourage you to run it on Google colab unless you have a good GPU machine available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci69aRSm2aoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!curl -Lo deepchem_installer.py https://raw.githubusercontent.com/deepchem/deepchem/master/scripts/colab_install.py\n",
        "import deepchem_installer\n",
        "%time deepchem_installer.install(version='2.3.0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bm1iYbw2aoT",
        "colab_type": "text"
      },
      "source": [
        "Let's start by loading the data.  We will use the MUV dataset.  It includes 74,501 molecules in the training set, and 9313 molecules in the validation set, so it gives us plenty of SMILES strings to work with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnAnjl9d2aoU",
        "colab_type": "code",
        "colab": {},
        "outputId": "672ec5a4-9d90-44f1-d503-98e9d9fbb40d"
      },
      "source": [
        "import deepchem as dc\n",
        "tasks, datasets, transformers = dc.molnet.load_muv()\n",
        "train_dataset, valid_dataset, test_dataset = datasets\n",
        "train_smiles = train_dataset.ids\n",
        "valid_smiles = valid_dataset.ids"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "RDKit WARNING: [15:40:18] Enabling RDKit 2019.09.3 jupyter extensions\n",
            "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading raw samples now.\n",
            "shard_size: 8192\n",
            "About to start loading CSV from /var/folders/st/ds45jcqj2232lvhr0y9qt5sc0000gn/T/muv.csv.gz\n",
            "Loading shard 1 of size 8192.\n",
            "Featurizing sample 0\n",
            "Featurizing sample 1000\n",
            "Featurizing sample 2000\n",
            "Featurizing sample 3000\n",
            "Featurizing sample 4000\n",
            "Featurizing sample 5000\n",
            "Featurizing sample 6000\n",
            "Featurizing sample 7000\n",
            "Featurizing sample 8000\n",
            "TIMING: featurizing shard 0 took 10.486 s\n",
            "Loading shard 2 of size 8192.\n",
            "Featurizing sample 0\n",
            "Featurizing sample 1000\n",
            "Featurizing sample 2000\n",
            "Featurizing sample 3000\n",
            "Featurizing sample 4000\n",
            "Featurizing sample 5000\n",
            "Featurizing sample 6000\n",
            "Featurizing sample 7000\n",
            "Featurizing sample 8000\n",
            "TIMING: featurizing shard 1 took 10.458 s\n",
            "Loading shard 3 of size 8192.\n",
            "Featurizing sample 0\n",
            "Featurizing sample 1000\n",
            "Featurizing sample 2000\n",
            "Featurizing sample 3000\n",
            "Featurizing sample 4000\n",
            "Featurizing sample 5000\n",
            "Featurizing sample 6000\n",
            "Featurizing sample 7000\n",
            "Featurizing sample 8000\n",
            "TIMING: featurizing shard 2 took 10.235 s\n",
            "Loading shard 4 of size 8192.\n",
            "Featurizing sample 0\n",
            "Featurizing sample 1000\n",
            "Featurizing sample 2000\n",
            "Featurizing sample 3000\n",
            "Featurizing sample 4000\n",
            "Featurizing sample 5000\n",
            "Featurizing sample 6000\n",
            "Featurizing sample 7000\n",
            "Featurizing sample 8000\n",
            "TIMING: featurizing shard 3 took 10.636 s\n",
            "Loading shard 5 of size 8192.\n",
            "Featurizing sample 0\n",
            "Featurizing sample 1000\n",
            "Featurizing sample 2000\n",
            "Featurizing sample 3000\n",
            "Featurizing sample 4000\n",
            "Featurizing sample 5000\n",
            "Featurizing sample 6000\n",
            "Featurizing sample 7000\n",
            "Featurizing sample 8000\n",
            "TIMING: featurizing shard 4 took 10.483 s\n",
            "Loading shard 6 of size 8192.\n",
            "Featurizing sample 0\n",
            "Featurizing sample 1000\n",
            "Featurizing sample 2000\n",
            "Featurizing sample 3000\n",
            "Featurizing sample 4000\n",
            "Featurizing sample 5000\n",
            "Featurizing sample 6000\n",
            "Featurizing sample 7000\n",
            "Featurizing sample 8000\n",
            "TIMING: featurizing shard 5 took 10.145 s\n",
            "Loading shard 7 of size 8192.\n",
            "Featurizing sample 0\n",
            "Featurizing sample 1000\n",
            "Featurizing sample 2000\n",
            "Featurizing sample 3000\n",
            "Featurizing sample 4000\n",
            "Featurizing sample 5000\n",
            "Featurizing sample 6000\n",
            "Featurizing sample 7000\n",
            "Featurizing sample 8000\n",
            "TIMING: featurizing shard 6 took 9.811 s\n",
            "Loading shard 8 of size 8192.\n",
            "Featurizing sample 0\n",
            "Featurizing sample 1000\n",
            "Featurizing sample 2000\n",
            "Featurizing sample 3000\n",
            "Featurizing sample 4000\n",
            "Featurizing sample 5000\n",
            "Featurizing sample 6000\n",
            "Featurizing sample 7000\n",
            "Featurizing sample 8000\n",
            "TIMING: featurizing shard 7 took 10.585 s\n",
            "Loading shard 9 of size 8192.\n",
            "Featurizing sample 0\n",
            "Featurizing sample 1000\n",
            "Featurizing sample 2000\n",
            "Featurizing sample 3000\n",
            "Featurizing sample 4000\n",
            "Featurizing sample 5000\n",
            "Featurizing sample 6000\n",
            "Featurizing sample 7000\n",
            "Featurizing sample 8000\n",
            "TIMING: featurizing shard 8 took 10.481 s\n",
            "Loading shard 10 of size 8192.\n",
            "Featurizing sample 0\n",
            "Featurizing sample 1000\n",
            "Featurizing sample 2000\n",
            "Featurizing sample 3000\n",
            "Featurizing sample 4000\n",
            "Featurizing sample 5000\n",
            "Featurizing sample 6000\n",
            "Featurizing sample 7000\n",
            "Featurizing sample 8000\n",
            "TIMING: featurizing shard 9 took 11.081 s\n",
            "Loading shard 11 of size 8192.\n",
            "Featurizing sample 0\n",
            "Featurizing sample 1000\n",
            "Featurizing sample 2000\n",
            "Featurizing sample 3000\n",
            "Featurizing sample 4000\n",
            "Featurizing sample 5000\n",
            "Featurizing sample 6000\n",
            "Featurizing sample 7000\n",
            "Featurizing sample 8000\n",
            "TIMING: featurizing shard 10 took 10.569 s\n",
            "Loading shard 12 of size 8192.\n",
            "Featurizing sample 0\n",
            "Featurizing sample 1000\n",
            "Featurizing sample 2000\n",
            "TIMING: featurizing shard 11 took 3.824 s\n",
            "TIMING: dataset construction took 121.359 s\n",
            "Loading dataset from disk.\n",
            "TIMING: dataset construction took 3.393 s\n",
            "Loading dataset from disk.\n",
            "TIMING: dataset construction took 1.770 s\n",
            "Loading dataset from disk.\n",
            "TIMING: dataset construction took 1.871 s\n",
            "Loading dataset from disk.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EslVHE2m2aoY",
        "colab_type": "text"
      },
      "source": [
        "We need to define the \"alphabet\" for our `SeqToSeq` model, the list of all tokens that can appear in sequences.  (It's also possible for input and output sequences to have different alphabets, but since we're training it as an autoencoder, they're identical in this case.)  Make a list of every character that appears in any training sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsE8e9xn2aoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = set()\n",
        "for s in train_smiles:\n",
        "  tokens = tokens.union(set(c for c in s))\n",
        "tokens = sorted(list(tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgzyJ1-42aog",
        "colab_type": "text"
      },
      "source": [
        "Create the model and define the optimization method to use.  In this case, learning works much better if we gradually decrease the learning rate.  We use an `ExponentialDecay` to multiply the learning rate by 0.9 after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHKrymnM2aoh",
        "colab_type": "code",
        "colab": {},
        "outputId": "fe3a80bd-9432-469c-d1ef-7bf0c39e42eb"
      },
      "source": [
        "from deepchem.models.optimizers import Adam, ExponentialDecay\n",
        "max_length = max(len(s) for s in train_smiles)\n",
        "batch_size = 100\n",
        "batches_per_epoch = len(train_smiles)/batch_size\n",
        "model = dc.models.SeqToSeq(tokens,\n",
        "                           tokens,\n",
        "                           max_length,\n",
        "                           encoder_layers=2,\n",
        "                           decoder_layers=2,\n",
        "                           embedding_dimension=256,\n",
        "                           model_dir='fingerprint',\n",
        "                           batch_size=batch_size,\n",
        "                           learning_rate=ExponentialDecay(0.004, 0.9, batches_per_epoch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /Users/bharath/opt/anaconda3/envs/deepchem/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method Stack.call of <deepchem.models.layers.Stack object at 0x1a3cc7b0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Stack.call of <deepchem.models.layers.Stack object at 0x1a3cc7b0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Stack.call of <deepchem.models.layers.Stack object at 0x1a3cc7b0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Stack.call of <deepchem.models.layers.Stack object at 0x1a3cc7b0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Stack.call of <deepchem.models.layers.Stack object at 0x1a3cc7b0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Stack.call of <deepchem.models.layers.Stack object at 0x1a3cc7b0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Stack.call of <deepchem.models.layers.Stack object at 0x1a3cc7b0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Stack.call of <deepchem.models.layers.Stack object at 0x1a3cc7b0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSr7FkSW2aok",
        "colab_type": "text"
      },
      "source": [
        "Let's train it!  The input to `fit_sequences()` is a generator that produces input/output pairs.  On a good GPU, this should take a few hours or less."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ5l_g1E2aok",
        "colab_type": "code",
        "colab": {},
        "outputId": "8db60a71-2724-4342-d513-13d7bcbad3f9"
      },
      "source": [
        "def generate_sequences(epochs):\n",
        "  for i in range(epochs):\n",
        "    for s in train_smiles:\n",
        "      yield (s, s)\n",
        "\n",
        "model.fit_sequences(generate_sequences(40))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ending global_step 999: Average loss 72.0029\n",
            "Ending global_step 1999: Average loss 40.7221\n",
            "Ending global_step 2999: Average loss 31.5364\n",
            "Ending global_step 3999: Average loss 26.4576\n",
            "Ending global_step 4999: Average loss 22.814\n",
            "Ending global_step 5999: Average loss 19.5248\n",
            "Ending global_step 6999: Average loss 16.4594\n",
            "Ending global_step 7999: Average loss 18.8898\n",
            "Ending global_step 8999: Average loss 13.476\n",
            "Ending global_step 9999: Average loss 11.5528\n",
            "Ending global_step 10999: Average loss 10.1594\n",
            "Ending global_step 11999: Average loss 10.6434\n",
            "Ending global_step 12999: Average loss 6.57057\n",
            "Ending global_step 13999: Average loss 6.46177\n",
            "Ending global_step 14999: Average loss 7.53559\n",
            "Ending global_step 15999: Average loss 4.95809\n",
            "Ending global_step 16999: Average loss 4.35039\n",
            "Ending global_step 17999: Average loss 3.39137\n",
            "Ending global_step 18999: Average loss 3.5216\n",
            "Ending global_step 19999: Average loss 3.08579\n",
            "Ending global_step 20999: Average loss 2.80738\n",
            "Ending global_step 21999: Average loss 2.92217\n",
            "Ending global_step 22999: Average loss 2.51032\n",
            "Ending global_step 23999: Average loss 1.86265\n",
            "Ending global_step 24999: Average loss 1.67088\n",
            "Ending global_step 25999: Average loss 1.87016\n",
            "Ending global_step 26999: Average loss 1.61166\n",
            "Ending global_step 27999: Average loss 1.40708\n",
            "Ending global_step 28999: Average loss 1.4488\n",
            "Ending global_step 29801: Average loss 1.33917\n",
            "TIMING: model fitting took 5619.924 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lxf1lmX2aoo",
        "colab_type": "text"
      },
      "source": [
        "Let's see how well it works as an autoencoder.  We'll run the first 500 molecules from the validation set through it, and see how many of them are exactly reproduced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXDBtIvn2aop",
        "colab_type": "code",
        "colab": {},
        "outputId": "59d18b07-0945-4bbb-ecf0-9860ed140e62"
      },
      "source": [
        "predicted = model.predict_from_sequences(valid_smiles[:500])\n",
        "count = 0\n",
        "for s,p in zip(valid_smiles[:500], predicted):\n",
        "  if ''.join(p) == s:\n",
        "    count += 1\n",
        "print('reproduced', count, 'of 500 validation SMILES strings')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reproduced 363 of 500 validation SMILES strings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt9GLy502aou",
        "colab_type": "text"
      },
      "source": [
        "Now we'll trying using the encoder as a way to generate molecular fingerprints.  We compute the embedding vectors for all molecules in the training and validation datasets, and create new datasets that have those as their feature vectors.  The amount of data is small enough that we can just store everything in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdUfsbtZ2aov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_embeddings = model.predict_embeddings(train_smiles)\n",
        "train_embeddings_dataset = dc.data.NumpyDataset(train_embeddings,\n",
        "                                                train_dataset.y,\n",
        "                                                train_dataset.w,\n",
        "                                                train_dataset.ids)\n",
        "\n",
        "valid_embeddings = model.predict_embeddings(valid_smiles)\n",
        "valid_embeddings_dataset = dc.data.NumpyDataset(valid_embeddings,\n",
        "                                                valid_dataset.y,\n",
        "                                                valid_dataset.w,\n",
        "                                                valid_dataset.ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVvfGr562aoz",
        "colab_type": "text"
      },
      "source": [
        "For classification, we'll use a simple fully connected network with one hidden layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFmnnVNm2aoz",
        "colab_type": "code",
        "colab": {},
        "outputId": "e4efa887-24ac-4fab-e17b-fe27fc905a2b"
      },
      "source": [
        "classifier = dc.models.MultitaskClassifier(n_tasks=len(tasks),\n",
        "                                                      n_features=256,\n",
        "                                                      layer_sizes=[512])\n",
        "classifier.fit(train_embeddings_dataset, nb_epoch=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ending global_step 999: Average loss 829.805\n",
            "Ending global_step 1999: Average loss 450.42\n",
            "Ending global_step 2999: Average loss 326.079\n",
            "Ending global_step 3999: Average loss 265.199\n",
            "Ending global_step 4999: Average loss 246.724\n",
            "Ending global_step 5999: Average loss 224.64\n",
            "Ending global_step 6999: Average loss 202.624\n",
            "Ending global_step 7460: Average loss 213.885\n",
            "TIMING: model fitting took 19.780 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khdB2v7R2ao2",
        "colab_type": "text"
      },
      "source": [
        "Find out how well it worked.  Compute the ROC AUC for the training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlilhPvm2ao2",
        "colab_type": "code",
        "colab": {},
        "outputId": "7ee4c5d3-2647-401a-ce5f-65ded20daaee"
      },
      "source": [
        "import numpy as np\n",
        "metric = dc.metrics.Metric(dc.metrics.roc_auc_score, np.mean, mode=\"classification\")\n",
        "train_score = classifier.evaluate(train_embeddings_dataset, [metric], transformers)\n",
        "valid_score = classifier.evaluate(valid_embeddings_dataset, [metric], transformers)\n",
        "print('Training set ROC AUC:', train_score)\n",
        "print('Validation set ROC AUC:', valid_score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "computed_metrics: [0.97828427249789751, 0.98705973960125326, 0.966007068438685, 0.9874401066031584, 0.97794394675150698, 0.98021719680962449, 0.95318452689781941, 0.97185747562764213, 0.96389538770053473, 0.96798988621997473, 0.9690779239145807, 0.98544402211472004, 0.97762497271338133, 0.96843239633294886, 0.97753648081489997, 0.96504683675485614, 0.93547151958366914]\n",
            "computed_metrics: [0.90790686952512678, 0.79891461649782913, 0.61900937081659968, 0.75241212956581671, 0.58678903240426017, 0.72765072765072758, 0.34929006085192693, 0.83986814712005553, 0.82379943502824859, 0.61844636844636847, 0.863620199146515, 0.68106930272108857, 0.98020477815699669, 0.85073580939032944, 0.781015678254942, 0.75399733510992673, nan]\n",
            "Training set ROC AUC: {'mean-roc_auc_score': 0.97132433878689139}\n",
            "Validation set ROC AUC: {'mean-roc_auc_score': 0.74592061629292239}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixqbRXnW2ao6",
        "colab_type": "text"
      },
      "source": [
        "# Congratulations! Time to join the Community!\n",
        "\n",
        "Congratulations on completing this tutorial notebook! If you enjoyed working through the tutorial, and want to continue working with DeepChem, we encourage you to finish the rest of the tutorials in this series. You can also help the DeepChem community in the following ways:\n",
        "\n",
        "## Star DeepChem on [GitHub](https://github.com/deepchem/deepchem)\n",
        "This helps build awareness of the DeepChem project and the tools for open source drug discovery that we're trying to build.\n",
        "\n",
        "## Join the DeepChem Gitter\n",
        "The DeepChem [Gitter](https://gitter.im/deepchem/Lobby) hosts a number of scientists, developers, and enthusiasts interested in deep learning for the life sciences. Join the conversation!"
      ]
    }
  ]
}